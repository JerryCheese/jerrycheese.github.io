<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Week9:Anomaly Detection & Recommender Systems]]></title>
    <url>%2F%2Fp%2F30591.html</url>
    <content type="text"><![CDATA[序Coursera的课程还有2周就要结束了，我这周基本上是每天学一周的内容，而下周就要开始正式接触NLP了。 身边也有一起学习Ng这套课程的人，但是学到一半没到就放弃了，我在学习过程中也问过自己，Ng这套11年的课程放在现在还有价值么？这是多虑的表现。到了Week9我慢慢感觉到了这套课程的魅力所在，首先它可以在很短时间内让你了解机器学习这门学科，我大概也就用了一个月吧；其次它总能用一些简单的例子来让你快速理解问题所在，我觉得机器学习是偏应用。 异常检测这是一个偏斜问题，传统行业像零件生产商，他们在出货前都会做这个工作，也许10000个成品里只有40个是不合格的。假如一家引擎生产商，正在做异常检测的工作，他们规定了两个重要指标（两个特征）来衡量产品的好坏，画在坐标系中可以很清楚的发现所有的点都集中在一个区域，只有一少部分数据会落在边缘地带。 对数学笔记敏感的同学也许很快反应过来，这有点像正态分布，是的。 高斯分布又名正态分布，它的方程长这个样子： 如果变量x服从正态分布，则可以记作 exp就是e为底的对数 $\mu$ 是平均数，又叫做位置参数 $\sigma $ 是标准差，所以 $\sigma^2$ 是方差 画成图就是这样： $\mu$ 和 $\sigma$ 对图像的影响是： $\mu$ 是对称轴 $\sigma$ 大则矮胖；小则廋高 图像的面积为概率，例如$ 8 &lt;= x &lt;= 10$ 的概率，就是正态函数在$x ∈ [8, 10]$区间上的积分（面积） 。 所以可以认为，如果数据x越靠近中心，那么它为正品的概率$p(x)$越高，反之如果x落在了边缘，那么它很可能是异常数据，为了更加精确的描述异常数据，我们可以取一个比较小的值 $\epsilon$ ，当 $p(x) &lt; \epsilon$ 的时候，我们就认为该 $x$ 是异常数据，否则就是正常数据。 异常检测算法既然高斯分布可以用来检测异常，那太好了，我们得到了一个有力的工具。我们假设每一个特征是服从正态分布的，那么$x_1, x_2, …, x_n$ 服从各自的正态分布，他们有各自的 $\mu_i, \sigma_i^2$ 。那么给定的数据集 { $x^{(1)},x^{(2)},…x^{(n)}$ } 中，数据$x^{(i)}$ 的概率 $ p(x^{(i)}) = p(x_1^{(i)})p(x_2^{(i)}) … p(x_n^{(i)})$ ，这个值如果小于 $\epsilon$ ，则可以认为数据是异常数据。那么现在要做的就是计算出每一个特征的$\mu_i, \sigma_i^2$ 值，而其实就是计算特征的平均数和方差。 那么如何衡量一个异常检测算法的好坏呢？当然还是使用查准率和召回率，而不是单纯的使用它的预测准确度，因为异常检测通常是偏斜问题。 异常检测和监督学习它们之间的一些区别 异常检测(0 异常 1正常) 监督学习 (0 反例 1正例) 极少异常，大部分是正常 正、反的比例相当 异常情况复杂，很难从正常数据中学习到异常的特征 足够的正例数据来进行学习 异常数据可能是我们从未出现过的数据 它们不同的应用场景 异常检测 监督学习 诈骗检测 垃圾邮件分类 制造业（如零件生产商） 天气预报 服务器故障监控 癌症预测 推荐系统评分是一个收集用户兴趣的好方法，我们假设已经观察到下面一些用户的评分行为： 电影名 用户1 用户2 用户3 用户4 何以笙箫默 5 5 0 0 栀子花开 5 0 激战 0 5 叶问2 0 4 战狼 0 0 5 5 评分范围是0-5的整数，留空表示用户没有对其评分，可能是没看过。 基于内容的推荐如果独立的看一个用户i，把电影看成是输入值 $X$，评分看成输出 $Y$，这就是一个线性回归问题，通过学习用户1已经对电影的评分，我们可以预测出用户1对“激战”这部电影的评分。 我们可以把电影分成两类：爱情片、动作片，所以特征数量是2，那么预测函数为： $h(X) = \theta_0 + \theta_1x_1 + \theta_2x_2$ ，其中 $x_1$ 理解为表示是这部电影是爱情片的程度，$\theta_1$ 是用户对于爱情片的喜爱程度，可以记用户i的$\theta$ 值为 $\theta^{(i)}$ 。 所以一个用户评价的越多，他的$\theta$就会越精确，我们就可以更加精确的为其推荐内容。 协同过滤随着电影数量越来越多，我们不好好确定一部电影的“爱情片程度”。协同过滤算法的一个特点是，它可以自动的学习出电影的特征值。怎么学的？通过询问用户，来确定他的 $\theta$ 值，然后再根据用户对电影的评分来求得这部电影的特征值。 而回想发现，$\theta$ 应该才是要被优化的参数啊！！所以协同过滤算法的另一个特点是它在递归下降时同时优化$x$ 和 $\theta$ 。 它的Cost Function： 日中 $r(i, j) = 1$ 表示用户 $j$ 对电影 $i$ 评过分（从表格看来，电影的特征是行，用户是列） 分别对 $x$ 、$\theta$ 进行梯度下降，他们的偏导数为：]]></content>
      <categories>
        <category>研</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Week8-2:Dimensionality Reduction]]></title>
    <url>%2F%2Fp%2F26479.html</url>
    <content type="text"><![CDATA[维数约简假如你负责一项机器学习项目，最开始你需要收集数据集，也许把收集任务下发给各部门后，收集上来的数据特征有上千个，你是全部用呢？还是只用一部分呢？如果只用一部分，应该如何选择呢？ 首先我们肯定是不能全用的，因为特征太多会拖慢训练速度。 我们需要选择其中的一部分特征，但是这些特征又要能保证一些特性，使他能够较好的代表所有特征。 二维转一维假设现在有2个特征 $x_1, x_2$，我们需要将其转成一维数据 $y$ ，我们将所有的点 $(x_1, x_2)$ 投影到一条直线上去，那么最后会得到一条数轴上的一些点集，也就是把二维转成一维了。 三维转二维我们将所有的三维上的点投影到一个平面上，这个平面用两个向量$z_1, z_2$来表示，那么最后我们将得到一个平面上的点集，这样也就将三维转成二维了。 主要成分分析又叫PCA，它寻找一条低维的面，使得所有点到该面的距离的平方和最小。如下图中的这条直线，所有点到该直线的距离的平方和最短。 那么其实不用找一条直线，可以发现，只要方向与直线一样，投影的结果是一样的，所以只需要找到一个向量即可。 处理过程预处理如果给定m个数据 $x^{(1)}, x^{(2)}, .. , x^{(m)}$ ，我们首先对数据进行缩放，像第二周讲的那样，我们先求得每一个特征的平均数$\mu_j = \frac 1 m (x_j^{(1)} + x_j^{(2)} … +x_j^{(m)})$ ，用 $x_j^{(i)} - \mu_j$ 替换 $x_j^{(i)}$ 。 计算协方差矩阵计算出每一个数据的协方差矩阵 计算特征向量再计算协方差矩阵的特征向量，在matlab中，可以使用[U, S, V] = svd(Sigma)（Sigma是协方差矩阵）来计算特征向量U，那么这时我们会得到一个 $n \times n$ 的特征向量。 取前k列作为$U_{reduce}$ 值，用来做降维映射。 降维令向量$z^{(i)} = U_{reduce}^T * x^{(i)}$ 就得到了n维的 $x^{(i)}$ 降成k维后的$z^{(i)}$ k的选取现在你有1000个特征，那么到底降成多少个比较合适呢？也许可以是500个，也有可能100个就够了。 这里有一个方法可以检测降维后，新数据对原始数据特性的保留程度，计算下面公式，你将得到新数据与原始数据之间的差异度。假设计算出来的结果是0.05，那么新数据保留了原始数据95%的特性，一般来说，我们只要保持计算结果小于0.01即可。 所以最好的方式是，尝试不同的k值，找到一个比较小的k值，同时新数据保留原始数据99%~90%的特性均可。 PCA的应用PCA常用于： 压缩数据，节约空间 加速机器学习过程 数据可视化（如取k=2、k=3） 一种不好的用法是，用PCA来避免过拟合，我们前几周知道，过拟合问题可以通过用正规化参数 $\lambda$ 来处理，虽然PCA可以减少特征数量，但其实它保持了原数据的特性，也就是方差，所以效果不尽人意。 同时在进行一次机器学习训练过程中，我们也许根本用不到PCA算法，最好的方式还是先不用PCA降维，得到训练结果后，再使用PCA降维重新训练一次，看哪种方式带来的结果比较合乎人意。]]></content>
      <categories>
        <category>研</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Week8-1: Clustering]]></title>
    <url>%2F%2Fp%2F31473.html</url>
    <content type="text"><![CDATA[序模仿coursera的结构，把第八周分成两部分。前面七周学习内容，我们经常用到训练这个词，就像妈妈教小孩子说话一样，是属于监督学习，数据集是有标记的，这样我们才能根据 $x$ 来预测 $y$。 对于无监督学习，给出的数据集是无标记的，可以看成只给了 $x$，而没有给出 $y$ 。 K-means 聚类算法在监督学习中，我们学到了分类，相对应的，无监督学习中有聚类的概念。 假设现有两个特征的数据集，把它们画在坐标系中如下图，直观上觉得它们应该可以分成2类数据，右上角部分和左下角部分，K-means算法就是干这个事的，它接受两个参数$ (X, K) $ K表示要将数据分成几类，$X$ 表示数据集。 那么K-means算法是怎么工作的呢？ K-means算法工作流程随机选取K个中心点假设需要将数据分成2类，那么它会先随机的选取2个中心点，然后将数据集按照靠哪个点比较近而分成了两类。 计算新的中心点接着计算每一类的新中心点，也就是点 $\frac 1 m (x^{(1)}+.. + x^{(n)})$ 这样迭代的计算新中心点，直到中心点稳定后，K-means算法就完成工作了 代价函数K-means的代价函数是指每一类里的点到中心点的距离之和 K-means 算法优化从数据集中选取中心点算法初始时需要随机选取中心点，如果要是选了一个偏离数据很远的中心点，那就会造成一次分类后，这个中心点周围没有任何数据，会陷入局部最优。 所以我们可以在初始时，从数据中选取K个点作为初始的中心点。 多次运行算法初始中心点对K-means的影响非常大，如果初始点选的不好，很可能陷入局部最优。 而应对这一问题的方法是多次运行K-means算法，优化它的失真值(Cost Function)。 选择K值首先这是一个模棱两可的问题，没有标准答案或方法可以告诉你到底应该分成几类。 人也许一眼能看出来数据可以分成几类，但是总有一些情况是连人眼的分不出来的。 例如衣服的尺码可以分成S,M,L三类，也可以分成XS,S,M,L,XL五类，而到底分成几类，按照你的业务需求定。 还有一种方式是利用肘算法，也就是尝试不同的K值，每次分类完成后都计算它的Cost Function。 左图容易看出来，选K=3就行，形象点也就是手肘的那个点，而对于有图，似乎就看不出肘在哪了。。。 总结看到目前为止，K-means算法印象最深刻。。也许是因为Ng讲的比较详细吧。。]]></content>
      <categories>
        <category>研</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Week7: Support Vector Machines]]></title>
    <url>%2F%2Fp%2F38722.html</url>
    <content type="text"><![CDATA[序因为时间上的限制，本周必须要把Course的课程看完，之后开始步入实战，所以周二就搞定了第七周的内容。支持向量机（Support Vector Machines）这个词很早就听过，今天总算是学到了这个知识点。 线性可分和非线性可分什么是线性可分？例如对于下图所表示的数据集，我们可以用一条直线将红蓝圆点分开，就叫做这个问题是线性可分问题。（下图均出自知乎，作者简之） 但是当数据变多的时候，也许会出现一些错误分类，例如下图： 线性不可分就是说，对于下面这张二维数据，你无法用一条直线将其分开： 最大间距和升维SVM可用于线性可分，也可以用于线性不可分问题。 对于线性可分问题，SVM的策略是选取一个“间距”最大的划分直线，也就是下图的黄色区域越大越好。 因为这样的话，在数据增多的情况下，仍有比较好的分类效果。 而对于线性不可分问题，SVM的策略是将数据升维，在二维上看起来无法用直线划分解决问题，将其提升到三维用一个平面进行划分。 核函数首先，线性模型的预测函数为： $x_1 … x_n$ 是特征值，将其替换为 $ f_1 … f_n$ 就得到了SVM模型的预测函数： $f_i$ 就是 $x_i$ 的核函数，它通常是非线性的，例如 $f_i = xi x{i+1} (i &lt; n)$ ，可以知道，$f_i$ 其实是一个映射函数。 通过$f_i$ 的映射，可以将低维特征转化为高维特征，所以SVM对非线性也有很好拟合能力。 常用的核函数有： 线性核：$κ(x, x_i) = x·x_i$ 多项式核：$κ(x, x_i) = ((x · x_i) + 1)^d$ 高斯核（RBF）：$κ(x, x_i) = exp(- \frac {||x - x_i||^2} {δ^2}) $ 高斯核假设SVM模型为： 值得注意的是，$f$ 的数量不一定和特征 $x$ 数量一样，例如模型中 $f$ 有3个，而特征 $x$ 只有2个。 我们可以在数据集中取三个点 $l^{(1)}, l^{(2)}, l^{(3)}$，函数 $f_i$ 就是点 $x$ 到点 $l^{(i)}$ 的相似度（similarity），它的计算公式为： $f_i =similarity(x, l^{(i)}) = exp(- \frac {||x - l^{(i)}||^2} {2δ^2}) $ 假设 $l^{(1)} = [1,3], δ = 1$ 那么$f_1 = exp(-(x1- 1)^2 - (x2 - 3)^2)$，它表示的是一个三维曲面。 Cost Function逻辑回归的代价函数为： SVM对 $y = 0$ 和 $y = 1$ 两种情况所对应的曲线用直线代替了，如下图： 这样一来，我们得到了 $cost_1$ 和 $cost_2$ 函数，所以SVM的代价函数变成了： Cost Function 正规化逻辑回归中的 $\lambda$ 是正规化参数，SVM中C参数有着相似的作用，先看逻辑回归的代价函数： 常数项只影响最优值大小，而不会影响取得最优值时的参数，所以可以去掉上图的 $m$。 并且 $\lambda $ 作用于左边的误差上，而不是右边的 $\theta$ 上就得到了： 其中 $C$ 的作用类似于$\frac {1} {\lambda}$ 。 C和δ参数C参数作用于代价函数，它的作用类似于$\frac {1} {\lambda}$ ，所以 C大 → 方差大，偏差小 C小 → 方差小，偏差大 δ参数作用于高斯核函数，可以发现δ越大，函数值的的变化率其实越小，所以随着x远离 $l^{(i)}$ ，函数值降低的比较慢，那么方差就越小，偏差就越大，所以 δ大 → 方差小，偏差大 δ小 → 方差大，偏差小 总结我觉得SVM的核心就在于$f$ 核函数，通过升维来解决低维问题着实厉害。]]></content>
      <categories>
        <category>研</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Week 6:Machine Learning System Design]]></title>
    <url>%2F%2Fp%2F62235.html</url>
    <content type="text"><![CDATA[序接Exercise5的内容，他们都是第六周的内容。这一部分主要讲如何设计一个机器学习系统。 垃圾邮件分类器假设你正要去实现一个垃圾邮件分类器，你会怎么做？这是一个二分类问题，我已经学习了线性回归、逻辑回归、神经网络。最快速、准确的方式应该是先确定特征向量，收集数据，再选择一个模型，快速地进行一次测试，画出学习曲线，根据曲线分析现状是高方差还是高偏差，再进行下一步的选择。 那么特征向量怎么选取呢？如果是英文，例如： 12345From: xxxx@xx.comTo: me@xx.comSubject: Buy now!!Deal of the week! Buy now! 可以考虑建立一个特征向量，选择100个词左右，记录词是否出现过，例如 x = [buy, deal, discount, …, now]，对于上面这封邮件，x的值会是 x = [1, 1, 0, …, 1]。 接着就是模型选择，参考Exercise5的模型选择方法即可，那么训练集怎么获取呢？一个简单的方法是把自己的邮箱暴露给个大广告商，这样的话你可能每天都会受到上百封广告邮件，接着就是对模型进行训练了。 错误分析在验证集中你需要手动对错误分类的邮件进行纠错，并且检查哪一类的邮件容易被分类错误，这件事必须手动进行。例如验证集大小为500，其中有100封邮件是垃圾邮件，但是没有被过滤掉，那么你最好检查这100封邮件，手动进行分类。 现在进行深一步考虑特征向量： 我们都知道英语是有词根的，例如 discount, discounted, discounts 它们是不是应该看成一个词，即discount呢？ 对于Deal和deal 是否也看成一个词呢？（即不区分大小写） 对于错误拼写如discount 和 disc0unt，有些聪明人会采取这种方式来躲过垃圾邮件过滤器 一个好的方法便是，把这些方法都试一遍，看哪一个种的准确率比较高，因为它们比较容易在算法中进行调整。 偏斜类、查准率、召回率偏斜类是指在二分类中，其中某一类的占比很大的情况。例如在癌症预测中，实际上患癌症的概率是很小的，例如只有1%，此时如果你的预测函数直接写成 $ y = 0$ 而不经过机器学习算法来计算，你的准确率也许会更高一些，但是我们都知道这个预测函数肯定是错的，显然准确率并不能衡量一个预测函数的好坏。此时就需要引入两个概念：查准率、召回率。 还是以癌症预测为例，$ y = {0, 1}$ 表示未患癌症和患上癌症，预测函数 $h = {0, 1}$ 表示预测没患癌症和预测患上了癌症。那么根据两两组合关系可以得到下面的关系： y = 1 y = 0 h = 1 真阳性(True positive) 假阳性(False positive) h = 0 假阴性(False negative) 真阴性(True negative) 查准率：预测病人患上了癌症的正确率，P = $\frac {真阳性} {真阳性 + 假阳性}$ 召回率：患癌症的病人能查出来的概率，R = $\frac {真阳性} {真阳性 + 假阴性}$ 一个好的预测函数，这两个率都要高，也就是 $\frac {2PR} {P + R}$ 要大 那么对于之前的那个情况，如果预测函数为 $y = 0$ ，那么它的召回率 $R = 0$，这样的预测函数是很差的。]]></content>
      <categories>
        <category>研</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Exercise 5: Regularized Linear Regression and Bias v.s. Variance]]></title>
    <url>%2F%2Fp%2F42787.html</url>
    <content type="text"><![CDATA[序第六周的内容是机器学习的训练方法。 我们已经学习了线性回归、逻辑回归、神经网络，本质上他们都是通过数据的输入，来习得一个预测函数。那么怎么来评估习得函数的好坏呢？如果一个预测函数很好的预测了训练数据，而对于新数据的预测却不准，那这就不是一个好的预测函数，所以判断预测函数好坏的标准就是它对新数据的预测结果准不准，当出现不准的情况时，通常可以采取下面几个方法来提高准确度： 增加训练集数据 用更少的特征进行训练 用更多的特征进行训练 添加多项式 增加/减少 $\lambda$ 那么现在问题抛出来了，是不是从上到下依次尝试呢？答案必须是否定的，因为每一个尝试都会花费大量的时间，有的也许还要花费财力，所以我们需要根据现有情况来判断到底选择哪一种方法。 训练集、测试集假如你正要基于机器学习开发一款应用，现在你手中只有100组数据，在没有办法获得新数据的情况下，你需要想方设法来提高最后习得函数的泛化效率，那么可以考虑这么做：把数据按7:3划分，70%作为训练集，30%作为测试集。这样你就有了两份数据，一份用于训练，一份是新数据（相对训练集而言），那么习得函数在测试集上的准确率可以近似为泛化效率，我们使用误差函数来评判预测函数的好坏。 线性回归的误差函数可以表示为： 二分类误差函数： 模型选择和验证集线性回归需要预先选择模型，如果特征项（项的幂）太多，会导致过拟合，太少则会导致欠拟合。 使用训练集、测试集，可以取得一个比较好的”$\theta$”，而模型选择，则是需要选择最高次幂的大小，所以引入了验证集来判断。 假如现在要在上面的1~10个模型中选取一个作为你的线性回归模型，你需要针对每一个模型，计算出他们的最低误差$J_{test}(\theta)^{(i)}$ ，再选取误差最小的那个模型作为你的预测模型。 偏差和方差记 $J{train}(Θ)$ 为训练集的误差，$J{cv}(Θ)$ 为验证集的误差 。 $J{train}(Θ)$ 很大，$J{cv}(Θ)$ 也很大，则说明预测函数的偏差高，这表示欠拟合 $J{train}(Θ)$ 很小，$J{cv}(Θ)$ 也很大，则说明预测函数的方差高，这表示过拟合 通过引入$\lambda$ 进行正规化，可以很好的解决方差/偏差过高的问题 学习曲线通过绘制误差函数与训练集大小的函数图像，可以观察到高偏差和高方差的情况。 下图表示的是高偏差，可以看到训练误差和测试误差高于了期望值，而且两者几乎相等 下图表示的是高方差，可以看到训练误差比较小和测试误差很大 回到第一个问题我们说到了，当发现预测函数效果不太好时，可以采取以下几种方式提高效果： 增加训练集数据 用更少的特征进行训练 用更多的特征进行训练 添加多项式 增加/减少 $\lambda$ 那么这些方法，分别对应什么样的情况呢？可以根据欠拟合和过拟合分类，高偏差就是欠拟合，高方差就是过拟合，结论如下： 过拟合，高方差 ：增加训练集数据 过拟合，高方差 ：用更少的特征进行训练 欠拟合，高偏差 ：用更多的特征进行训练 欠拟合，高偏差 ：添加多项式 欠拟合，高偏差 ：减少 $\lambda$ 的值 过拟合，高方差 ：增加 $\lambda$ 的值 如果是神经网络，隐藏层越多则可能过拟合，越少则可能欠拟合，判断逻辑与上述一样。 总结以后尽量少贴代码而采取文字总结的方式，因为我觉得贴代码起不到总结的效果。 以后也尽量按照coursera给出的课程大纲来进行分点总结，起到翻译的效果吧。]]></content>
      <categories>
        <category>研</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Exercise 4: Neural Networks Learning]]></title>
    <url>%2F%2Fp%2F8039.html</url>
    <content type="text"><![CDATA[序一天两发，是因为这两次的练习内容都是神经网络，比较接近。 回顾第四周的作业，最后使用神经网络进行多分类预测的时候，Ng给出了训练好的Θ，本周的主要内容就是学习如何训练一个神经网络，最终得出Θ。 正向传播：Cost Function参考公式： K表示分类的数量，$ y^{(i)}$ 表示第 $i$ 个数据集的预测值，这个预测值的可能取值如下： 那么 $y_k^{(i)} = {0,1}$ ，正向传播的意思就是从左到右一层一层的计算，它的Cost Function就是每一层之间的逻辑回归Cost累加。 反向传播：误差计算和梯度为求得 $ min_Θ J(Θ)$，使用梯度下降算法，则需要计算下面两个值： $J(Θ)$ $ \frac {∂} {∂ Θ_{i, j}^{(l)}} J(Θ)$ 对于$J(Θ)$ ，用正向传播可以求得，而$ \frac {∂} {∂ Θ_{i, j}^{(l)}} J(Θ)$ 可以使用反向传播来求得，反向传播的计算流程如下。 $δ_j^{(l)}$ 表示 $Layer_l$ 的第 $j$ 个单元的误差，所以 $δ_j^{(4)}$ = $a_j^{(4)} - y_i$ 而 $δ_j^{(3)}$ 、$δ_j^{(2)}$ 的推导过程比较复杂，这里直接给出计算公式。 并且 $ \frac {∂} {∂ Θ_{i, j}^{(l)}} J(Θ) = a_j^{(l)}δ_i^{(l+1)} $ ，这个证明过程也很繁琐。 梯度检测在计算 $ \frac {∂} {∂ Θ_{i, j}^{(l)}} J(Θ) $ 时，代码上可能会出现一些bug，所以为了检测代码是否写对了，在测试时可以对其计算的梯度进行检测，原理是： 我们取 $\epsilon$ 是一个很小的值，就能近似的计算偏导数，检查两者的差值就能得到我们的偏导数算法是否写对了。 作业1：实现正向传播由于分类数量是K，所以对于每一组训练数据，都需要计算Cost值 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061function [J grad] = nnCostFunction(nn_params, ... input_layer_size, ... hidden_layer_size, ... num_labels, ... X, y, lambda)%NNCOSTFUNCTION Implements the neural network cost function for a two layer%neural network which performs classification% [J grad] = NNCOSTFUNCTON(nn_params, hidden_layer_size, num_labels, ...% X, y, lambda) computes the cost and gradient of the neural network. The% parameters for the neural network are "unrolled" into the vector% nn_params and need to be converted back into the weight matrices. % % The returned parameter grad should be a "unrolled" vector of the% partial derivatives of the neural network.%% Reshape nn_params back into the parameters Theta1 and Theta2, the weight matrices% for our 2 layer neural networkTheta1 = reshape(nn_params(1:hidden_layer_size * (input_layer_size + 1)), ... hidden_layer_size, (input_layer_size + 1));Theta2 = reshape(nn_params((1 + (hidden_layer_size * (input_layer_size + 1))):end), ... num_labels, (hidden_layer_size + 1));% Setup some useful variablesm = size(X, 1); % You need to return the following variables correctly J = 0;Theta1_grad = zeros(size(Theta1));Theta2_grad = zeros(size(Theta2));% ====================== YOUR CODE HERE ======================X = [ones(m,1) X];a1 = X;a2 = sigmoid(X * Theta1');a2 = [ones(size(a2, 1), 1) a2];a3 = sigmoid(a2 * Theta2');for i = 1:m yi = zeros(num_labels, 1); yi(y(i),1) = 1; a3i = a3(i,:)'; J = J + sum(-yi .* log(a3i) - (1 - yi) .* log(1 - a3i));endJ = 1/m * J;rTheta1 = Theta1(:,2:size(Theta1,2));rTheta2 = Theta2(:,2:size(Theta2,2));J = J + lambda/(2*m) * (sum(sum(rTheta1 .^ 2)) + sum(sum(rTheta2 .^ 2)));% -------------------------------------------------------------% =========================================================================% Unroll gradientsgrad = [Theta1_grad(:) ; Theta2_grad(:)];end 作业2：sigmoid的导数其实就是对sigmoid函数求导 作业3：反向传播其实和正向传播是写在同一个地方的，因为本质上实在计算偏导数。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071function [J grad] = nnCostFunction(nn_params, ... input_layer_size, ... hidden_layer_size, ... num_labels, ... X, y, lambda)%NNCOSTFUNCTION Implements the neural network cost function for a two layer%neural network which performs classification% [J grad] = NNCOSTFUNCTON(nn_params, hidden_layer_size, num_labels, ...% X, y, lambda) computes the cost and gradient of the neural network. The% parameters for the neural network are "unrolled" into the vector% nn_params and need to be converted back into the weight matrices. % % The returned parameter grad should be a "unrolled" vector of the% partial derivatives of the neural network.%% Reshape nn_params back into the parameters Theta1 and Theta2, the weight matrices% for our 2 layer neural networkTheta1 = reshape(nn_params(1:hidden_layer_size * (input_layer_size + 1)), ... hidden_layer_size, (input_layer_size + 1));Theta2 = reshape(nn_params((1 + (hidden_layer_size * (input_layer_size + 1))):end), ... num_labels, (hidden_layer_size + 1));% Setup some useful variablesm = size(X, 1); % You need to return the following variables correctly J = 0;Theta1_grad = zeros(size(Theta1));Theta2_grad = zeros(size(Theta2));% ====================== YOUR CODE HERE ======================X = [ones(m,1) X];a1 = X;a2 = sigmoid(X * Theta1');a2 = [ones(size(a2, 1), 1) a2];a3 = sigmoid(a2 * Theta2');bdelta_2 = 0;bdelta_1 = 0;for i = 1:m yi = zeros(num_labels, 1); yi(y(i),1) = 1; a3i = a3(i,:)'; a2i = a2(i,:)'; a1i = a1(i,:)'; delta_3 = (a3i - yi); delta_2 = Theta2' * delta_3; delta_2 = delta_2(2:size(delta_2,1)); delta_2 = delta_2 .* sigmoidGradient(Theta1 * a1i); bdelta_2 = bdelta_2 + delta_3*(a2i)'; bdelta_1 = bdelta_1 + delta_2*(a1i)';endTheta1_grad = 1/m * bdelta_1 + [zeros(size(Theta1, 1),1) lambda/m*rTheta1];Theta2_grad = 1/m * bdelta_2 + [zeros(size(Theta2, 1),1) lambda/m*rTheta2];% -------------------------------------------------------------% =========================================================================% Unroll gradientsgrad = [Theta1_grad(:) ; Theta2_grad(:)];end 总结感觉机器学习理解起来可能不难，但是写代码上却需要非常小心，尤其是对偏置单元的处理，做矩阵乘法时要检查size是否匹配，这些都很重要。]]></content>
      <categories>
        <category>研</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Exercise 3:Multi-class Classification and Neural Networks]]></title>
    <url>%2F%2Fp%2F18619.html</url>
    <content type="text"><![CDATA[序其实上周就做完啦，只是一直没写而已，对于这个Ng的ML课程，我希望能一直记录下去。 直到第三周，已经学习了线性回归和逻辑回归，它们可以用来实现二分类器，而且对于一些非线性分类也比较有效，例如： 但是对于非线性分类，他的Cost Function通常需要引入高阶方程： 所以选取一个预测模型非常重要，但选取一个合适的预测模型是非常困难的，因为通常会出现拟合问题，那么就诞生了”神经网络“，它其实早在上世纪40年代就有了，在沉寂了一段时间后的今天火了起来，原因是大数据和云计算的发展。 神经网络下图是一个最简单的神经网络，它有一个输入层和一个输出层： 如果取$ h_θ(x) = \frac {1} {1 + e^{-x}} $ ，这就是一个逻辑回归呀！其中 $\theta$ 可以看做是输入层（第一层）的参数，如果在输入、输出层之间再加一层： 这是不是就是我们经常在网上看到的图呢，哈哈。他的计算过程是这样的，可以看到 $a_1^{(2)}$ 是由 $x_1, x_2, x_3$ 共同计算出来的，也就是： $Θ^{(1)}$ 是第一层的参数，它是一个 $3 * 4 $ 的矩阵。 那么这么做有啥好处呢？我们可以发现，输入的变量只有$ x1, x2, x3$ ，通过从左到右的一层一层的计算，就能得到最后的结果。那么这期间起到作用的另外一个变量就是$Θ^{(1)}$，而它是可以通过训练这个网络（逻辑回归中的梯度下降可以理解为在训练数据集来得到一个合适的θ）来获得，这就不需要你事先选择一个精确的Cost Function模型了，你只需要考在输入、输出层之间加多少个隐藏层以及每一层的隐藏单元数量就行。 神经网络与多分类器线性回归、逻辑回归可以实现二分类器，我们可以实现多个二分类器来作为一个多分类器，而神经网络就可以干这个事儿。举个例子，一个四分类器，他的$ y^{(i)}$ 取值可能为 它可以通过神经网络来计算，计算过程如下： 那么计算出来的结果（预测值）为： 作业1：Cost Fcuntion这个作业先要求我们用二分类器来实现多分类器，那么每一个二分类器的Cost Function就是： 12345678910111213141516171819202122232425function [J, grad] = lrCostFunction(theta, X, y, lambda)%LRCOSTFUNCTION Compute cost and gradient for logistic regression with %regularization% J = LRCOSTFUNCTION(theta, X, y, lambda) computes the cost of using% theta as the parameter for regularized logistic regression and the% gradient of the cost w.r.t. to the parameters. % Initialize some useful valuesm = length(y); % number of training examples% You need to return the following variables correctly J = 0;grad = zeros(size(theta));J = 1/m * sum(-y .* log(sigmoid(X * theta)) - (1 - y) .* log(1 - sigmoid(X * theta)));J = J + lambda/(2*m) * sum(theta(2:size(theta)) .^ 2);grad = 1/m * (X' * (sigmoid(X * theta) - y)) + lambda/m * theta;grad(1) = grad(1) - lambda/m * theta(1);% =============================================================grad = grad(:);end 作业2：one vs All对于每一个分类器，它需要只识别自己负责的部分，所以用(y==k)来训练他的θ值 12345678910111213141516171819202122232425262728293031function [all_theta] = oneVsAll(X, y, num_labels, lambda)%ONEVSALL trains multiple logistic regression classifiers and returns all%the classifiers in a matrix all_theta, where the i-th row of all_theta %corresponds to the classifier for label i% [all_theta] = ONEVSALL(X, y, num_labels, lambda) trains num_labels% logistic regression classifiers and returns each of these classifiers% in a matrix all_theta, where the i-th row of all_theta corresponds % to the classifier for label i% Some useful variablesm = size(X, 1);n = size(X, 2);% You need to return the following variables correctly all_theta = zeros(num_labels, n + 1);% Add ones to the X data matrixX = [ones(m, 1) X];% ====================== YOUR CODE HERE ======================for k = 1:num_labels initial_theta = all_theta(k:k,1:n+1)'; options = optimset('GradObj', 'on', 'MaxIter', 50); [thetak] = fmincg (@(t)(lrCostFunction(t, X, (y == k), lambda)), ... initial_theta, options); all_theta(k:k,1:n+1) = thetak';end% =========================================================================end 作业3：使用多分类器来预测使用作业2训练完成的θ来预测，只需要做矩阵乘即可，这里需要注意的是，每一组数据对应的预测值是一个矩阵。例如一组数据的预测值是[0,0,0,0,1,0,0,0,0,0]，那么这组数据很有可能表示的是数字5，所以我们需要取预测矩阵中最大值的索引值。 1234567891011121314151617181920212223242526function p = predictOneVsAll(all_theta, X)%PREDICT Predict the label for a trained one-vs-all classifier. The labels %are in the range 1..K, where K = size(all_theta, 1). % p = PREDICTONEVSALL(all_theta, X) will return a vector of predictions% for each example in the matrix X. Note that X contains the examples in% rows. all_theta is a matrix where the i-th row is a trained logistic% regression theta vector for the i-th class. You should set p to a vector% of values from 1..K (e.g., p = [1; 3; 1; 2] predicts classes 1, 3, 1, 2% for 4 examples) m = size(X, 1);num_labels = size(all_theta, 1);% You need to return the following variables correctly p = zeros(size(X, 1), 1);% Add ones to the X data matrixX = [ones(m, 1) X];% ====================== YOUR CODE HERE ====================== p = X * all_theta';[v, p] = max(p, [], 2);% =========================================================================end 作业4：使用神经网络进行多分类这里比较花俏的是，Ng提供了已经训练好的Θ，我们只需要实现正向传播即可，也就是从左往右一层一层计算过去。 12345678910111213141516171819202122232425function p = predict(Theta1, Theta2, X)%PREDICT Predict the label of an input given a trained neural network% p = PREDICT(Theta1, Theta2, X) outputs the predicted label of X given the% trained weights of a neural network (Theta1, Theta2)% Useful valuesm = size(X, 1);num_labels = size(Theta2, 1);% You need to return the following variables correctly p = zeros(size(X, 1), 1);% ====================== YOUR CODE HERE ======================X = [ones(size(X, 1), 1) X];a2 = sigmoid(X * Theta1');a2 = [ones(size(a2, 1), 1) a2];a3 = sigmoid(a2 * Theta2');[v, p] = max(a3, [], 2);% =========================================================================end 总结第一次接触神经网络，其实也没有想象中的这么神秘，但是不得不感叹前辈们的智慧，这么几行代码就能进行图像识别了。]]></content>
      <categories>
        <category>研</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[初学者如何查阅自然语言处理（NLP）领域学术资料]]></title>
    <url>%2F%2Fp%2F49382.html</url>
    <content type="text"><![CDATA[原文：初学者如何查阅自然语言处理（NLP）领域学术资料 昨天实验室一位刚进组的同学发邮件来问我如何查找学术论文，这让我想起自己刚读研究生时茫然四顾的情形：看着学长们高谈阔论领域动态，却不知如何入门。经过研究生几年的耳濡目染，现在终于能自信地知道去哪儿了解最新科研动态了。我想这可能是初学者们共通的困惑，与其只告诉一个人知道，不如将这些Folk Knowledge写下来，来减少更多人的麻烦吧。当然，这个总结不过是一家之谈，只盼有人能从中获得一点点益处，受个人认知所限，难免挂一漏万，还望大家海涵指正。 国际学术组织、学术会议与学术论文 自然语言处理（natural language processing，NLP）在很大程度上与计算语言学（computational linguistics，CL）重合。与其他计算机学科类似，NLP/CL有一个属于自己的最权威的国际专业学会，叫做The Association for Computational Linguistics（ACL），这个协会主办了NLP/CL领域最权威的国际会议，即ACL年会，ACL学会还会在北美和欧洲召开分年会，分别称为NAACL和EACL。除此之外，ACL学会下设多个特殊兴趣小组（special interest groups，SIGs），聚集了NLP/CL不同子领域的学者，性质类似一个大学校园的兴趣社团。其中比较有名的诸如SIGDAT（Linguistic data and corpus-based approaches to NLP）、SIGNLL（Natural Language Learning）等。这些SIGs也会召开一些国际学术会议，其中比较有名的就是SIGDAT组织的EMNLP（Conference on Empirical Methods on Natural Language Processing）和SIGNLL组织的CoNLL（Conference on Natural Language Learning）。此外还有一个International Committee on Computational Linguistics的老牌NLP/CL学术组织，它每两年组织一个称为International Conferenceon Computational Linguistics (COLING)的国际会议，也是NLP/CL的重要学术会议。NLP/CL的主要学术论文就分布在这些会议上。 作为NLP/CL领域的学者最大的幸福在于，ACL学会网站建立了称作ACL Anthology的页面，支持该领域绝大部分国际学术会议论文的免费下载，甚至包含了其他组织主办的学术会议，例如COLING、IJCNLP等，并支持基于Google的全文检索功能，可谓一站在手，NLP论文我有。由于这个论文集合非常庞大，并且可以开放获取，很多学者也基于它开展研究，提供了更丰富的检索支持，具体入口可以参考ACL Anthology页面上方搜索框右侧的不同检索按钮。 与大部分计算机学科类似，由于技术发展迅速，NLP/CL领域更重视发表学术会议论文，原因是发表周期短，并可以通过会议进行交流。当然NLP/CL也有自己的旗舰学术期刊，发表过很多经典学术论文，那就是Computational Linguistics。该期刊每期只有几篇文章，平均质量高于会议论文，时间允许的话值得及时追踪。此外，ACL学会为了提高学术影响力，也刚刚创办了Transactions of ACL（TACL），值得关注。值得一提的是这两份期刊也都是开放获取的。此外也有一些与NLP/CL有关的期刊，如ACM Transactions on Speech and Language Processing，ACM Transactions on Asian Language Information Processing，Journal of Quantitative Linguistics等等。 根据Google Scholar Metrics 2013年对NLP/CL学术期刊和会议的评价，ACL、EMNLP、NAACL、COLING、LREC、Computational Linguistics位于前5位，基本反映了本领域学者的关注程度。 NLP/CL作为交叉学科，其相关领域也值得关注。主要包括以下几个方面：（1）信息检索和数据挖掘领域。相关学术会议主要由美国计算机学会（ACM）主办，包括SIGIR、WWW、WSDM等；（2）人工智能领域。相关学术会议主要包括AAAI和IJCAI等，相关学术期刊主要包括Artificial Intelligence和Journal of AI Research；（3）机器学习领域，相关学术会议主要包括ICML，NIPS，AISTATS，UAI等，相关学术期刊主要包括Journal of Machine Learning Research（JMLR）和Machine Learning（ML）等。例如最近兴起的knowledge graph研究论文，就有相当一部分发表在人工智能和信息检索领域的会议和期刊上。实际上国内计算机学会（CCF）制定了“中国计算机学会推荐国际学术会议和期刊目录”，通过这个列表，可以迅速了解每个领域的主要期刊与学术会议。 最后，值得一提的是，美国Hal Daumé III维护了一个natural language processing的博客，经常评论最新学术动态，值得关注。我经常看他关于ACL、NAACL等学术会议的参会感想和对论文的点评，很有启发。另外，ACL学会维护了一个Wiki页面，包含了大量NLP/CL的相关信息，如著名研究机构、历届会议录用率，等等，都是居家必备之良品，值得深挖。 国内学术组织、学术会议与学术论文 与国际上相似，国内也有一个与NLP/CL相关的学会，叫做中国中文信息学会。通过学会的理事名单基本可以了解国内从事NLP/CL的主要单位和学者。学会每年组织很多学术会议，例如全国计算语言学学术会议（CCL）、全国青年计算语言学研讨会（YCCL）、全国信息检索学术会议（CCIR）、全国机器翻译研讨会（CWMT），等等，是国内NLP/CL学者进行学术交流的重要平台。尤其值得一提的是，全国青年计算语言学研讨会是专门面向国内NLP/CL研究生的学术会议，从组织到审稿都由该领域研究生担任，非常有特色，也是NLP/CL同学们学术交流、快速成长的好去处。值得一提的是，2010年在北京召开的COLING以及2015年即将在北京召开的ACL，学会都是主要承办者，这也一定程度上反映了学会在国内NLP/CL领域的重要地位。此外，计算机学会中文信息技术专委会组织的自然语言处理与中文计算会议（NLP&amp;CC）也是最近崛起的重要学术会议。中文信息学会主编了一份历史悠久的《中文信息学报》，是国内该领域的重要学术期刊，发表过很多篇重量级论文。此外，国内著名的《计算机学报》、《软件学报》等期刊上也经常有NLP/CL论文发表，值得关注。 过去几年，在水木社区BBS上开设的AI、NLP版面曾经是国内NLP/CL领域在线交流讨论的重要平台。这几年随着社会媒体的发展，越来越多学者转战新浪微博，有浓厚的交流氛围。如何找到这些学者呢，一个简单的方法就是在新浪微博搜索的“找人”功能中检索“自然语言处理”、 “计算语言学”、“信息检索”、“机器学习”等字样，马上就能跟过去只在论文中看到名字的老师同学们近距离交流了。还有一种办法，清华大学梁斌开发的“微博寻人”系统可以检索每个领域的有影响力人士，因此也可以用来寻找NLP/CL领域的重要学者。值得一提的是，很多在国外任教的老师和求学的同学也活跃在新浪微博上，例如王威廉、李沐等，经常爆料业内新闻，值得关注。还有，国内NLP/CL的著名博客是52nlp，影响力比较大。总之，学术研究既需要苦练内功，也需要与人交流。所谓言者无意、听者有心，也许其他人的一句话就能点醒你苦思良久的问题。无疑，博客微博等提供了很好的交流平台，当然也注意不要沉迷哦。 如何快速了解某个领域研究进展 最后简单说一下快速了解某领域研究进展的经验。你会发现，搜索引擎是查阅文献的重要工具，尤其是谷歌提供的Google Scholar，由于其庞大的索引量，将是我们披荆斩棘的利器。 当需要了解某个领域，如果能找到一篇该领域的最新研究综述，就省劲多了。最方便的方法还是在Google Scholar中搜索“领域名称 + survey / review / tutorial / 综述”来查找。也有一些出版社专门出版各领域的综述文章，例如NOW Publisher出版的Foundations and Trends系列，Morgan &amp; Claypool Publisher出版的Synthesis Lectures on Human Language Technologies系列等。它们发表了很多热门方向的综述，如文档摘要、情感分析和意见挖掘、学习排序、语言模型等。 如果方向太新还没有相关综述，一般还可以查找该方向发表的最新论文，阅读它们的“相关工作”章节，顺着列出的参考文献，就基本能够了解相关研究脉络了。当然，还有很多其他办法，例如去videolectures.net上看著名学者在各大学术会议或暑期学校上做的tutorial报告，去直接咨询这个领域的研究者，等等。]]></content>
      <categories>
        <category>转载</category>
      </categories>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode买股票系列2]]></title>
    <url>%2F%2Fp%2F31115.html</url>
    <content type="text"><![CDATA[上一个系列是对交易次数进行限制，本系列是无限次交易但是有一些其他限制。 问题1：卖出后休息一天原题：Best Time to Buy and Sell Stock with Cooldown 题意：你可以进行任意次交易，但完成一次交易后，接下来的一天不能进行买入/卖出操作。 我们先回顾一下上一期 买股票系列 的问题2，它和这个问题就差了一个条件，当时我们用了贪心算法，策略是低买高卖，且经过思考后得到的下述的状态叠加公式 $ profit += max(0, prices[i + 1] - prices[i])$ 直观上理解为，只要今天的股票比昨天高，我就可以挣一个净增值。例如对于序列 $[1,2,3,4]​$ 第2天比第1天多1，我们就在第1天买入，第2天卖出，这样能挣1元 第3天比第2天多1，我们就在第2天买入，第3天卖出，这样能挣1元 …… 细看发现，其实这个状态是连续叠加的，所以如果一次交易后必须休息一天，那么这个状态就不连续了。 现在再看上述的过程，发现 profit += 这一句代码其实隐含着一个意思，就是局部最优，当前状态下的最优值是从上一个状态的最优值加上本次状态的最佳选择得到的，到这一步其实就已经得到本题的结果了，也就是公式 今天的最优值 = 上一个状态的最优值 + 今天的最佳选择 ，上述题目中的上一个状态的最优值 其实就是昨天的最优值，而本题确是上一次交易的最优值 123456789101112131415class Solution(object): def maxProfit(self, prices): """ :type prices: List[int] :rtype: int """ if len(prices) &lt; 2: return 0 sell, buy, prev_sell, prev_buy = 0, -2147483648, 0, 0 for price in prices: prev_buy = buy buy = max(prev_sell - price, prev_buy) prev_sell = sell sell = max(prev_buy + price, prev_sell) return sell 把买和卖拆开看，buy表示今天买入后最大价值，sell表示今天卖出后最大价值，prev_buy表示今天前买入的最大价值，prev_sell表示今天前卖出的最大价值。 问题2：交易收费原题：Best Time to Buy and Sell Stock with Transaction Fee 题意：每次交易收费 还是今天的最优值 = 上一个状态的最优值 + 今天的最佳选择 ，把买和卖拆开看，但是卖的时候要收手续费。 12345678910111213class Solution(object): def maxProfit(self, prices, fee): """ :type prices: List[int] :type fee: int :rtype: int """ buy, sell, prev_sell = -2147483648, 0, 0 for price in prices: prev_sell = sell sell = max(prev_sell, buy + price - fee) buy = max(buy, prev_sell - price) return sell]]></content>
      <categories>
        <category>编程题</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode买股票系列]]></title>
    <url>%2F%2Fp%2F52113.html</url>
    <content type="text"><![CDATA[问题1：一次交易原题：Best Time to Buy and Sell Stock 题意：给定一个数组，第 $i$ 个元素表示第 $i$ 天股票的价值，你只能进行一次交易（买入+卖出= 一次交易），请告诉我你获得的最大价值是多少？ Example 1: 1234Input: [7, 1, 5, 3, 6, 4]Output: 51买进，6卖出，获利5 Example 2: 1234Input: [7, 6, 4, 3, 1]Output: 0什么时候买都会亏，所以获利0 作为该系列的开门见山题，难度口算，下面说2个可能想到的错误思路： 也许你会觉得，找出最大值和最小值，两者差值就是答案 也许你会觉得，找出最大值和最小值，并且最小值索引小于最大值的索引就行了 对于第一个思路，Example 2: 已经告诉你是错的。 对于第二个思路，你怎么确保最小值索引是小于最大值索引的，万一不是呢，那是不是要进行搜索了呢？那搜索策略又是什么呢？顿时觉得前途一片昏暗 DP从下至上的计算，设 总天数为n $a[i]$ 是第 $i$ 天的股票价值 $ i = 1,2,…,n$ $d[i]$ 是第 $i$ 天买入股票所能达到的最大利益 $ i = 1,2,…,n$ 那么 $d[i] = max(a[i], a[i+1], .. , a[n]) - a[i] ， i = 1,2,…,n$ 那么最大价值就是 $max ( d[1], d[2], .., d[n])， i = 1,2,…,n$ 相信聪明的你已经知道代码怎么写了。 123456789101112class Solution(object): def maxProfit(self, prices): """ :type prices: List[int] :rtype: int """ max_after = 0 max_profit = 0 for i in range(len(prices) - 1, -1 , -1): max_profit = max(max_profit, max(0, max_after - prices[i])) max_after = max(max_after, prices[i]) return max_profit 贪心从上至下计算。假设你现在就是一个股民，用变量money表示你现在有多少钱，所以初始你是0元， 买入一个a元的股票，$money = -a$（欠钱） 今天股票价值是b，如果今天卖掉，则 $money = money + b$ 要达到两个最大即可，一是买入欠钱最少，所以-a越大越好，二是money越大越好 123456789101112class Solution(object): def maxProfit(self, prices): """ :type prices: List[int] :rtype: int """ debt = -2147483648 money = 0 for price in prices: money = max(money, price + debt) debt = max(debt, -price) return money 问题2：任意次交易原题：Best Time to Buy and Sell Stock II 题意：在问题1基础上，交易次数变为任意多次 一瞬间没啥思路，感觉问题很复杂？其实，如果你炒股你就知道，其实很简单，一句话：低买高卖 啥意思？借一张leecode某个答主的图，股票的波动一般像下图一样 低买高卖 是人类的直觉，即在valley（低谷）买入，在peak（顶峰）卖出，这样的收益会比较大。 我们可以分2类情况来讨论，一只股票其实就是下面两种情况的循环往复 股票一直跌，跌倒谷底了，马上要开始涨了这时候赶紧买啊！因为这个时候买肯定赚，对应的是低谷 如果在valley右侧买，肯定没有valley的收益大 如果在valley左侧买，也没有valley的收益大 如果在顶峰之后再买，那就白白错过了这次稳赚的机会，最后总收益肯定不是最大的 所以结论是必须在低谷买 股票一直涨，涨到顶峰了，马上要开始降了这时候赶紧抛啊！！因为这一点的收益是最大的，往后的收益肯定没现在这个大，对应的是顶峰 如果在peak左边卖，收益没有peak大 如果在peak右边卖，收益也没有peak大 如果在peak右边的低谷之后再卖，又白白错过了稳赚的机会，最后总收益肯定不是最大 所以结论是必须在顶峰卖 两个顶峰，分开买第一张图很好的诠释了 $ A + B &gt;= C$ 这一不等式，一看就懂 实现找一个低谷点买入，在遇到下一个顶峰点的时候立马卖出，这样你就能拿到最大的收益。 不过在代码上要注意最后一天的处理 1234567891011121314151617class Solution(object): def maxProfit(self, prices): """ :type prices: List[int] :rtype: int """ buy = -1; profit = 0 for i in range(len(prices)): if i + 1 &gt;= len(prices): profit += prices[i] - buy if buy != -1 else 0 elif prices[i + 1] &gt; prices[i]: buy = prices[i] if buy == -1 else buy elif prices[i + 1] &lt; prices[i]: profit += prices[i] - buy if buy != -1 else 0 buy = -1 return profit 优化其实仔细看刚刚写的代码你会发现，只要是“上升期”的股票我们都可以买，所以代码可以优化为 12345678910class Solution(object): def maxProfit(self, prices): """ :type prices: List[int] :rtype: int """ max_profit = 0 for i in range(len(prices) - 1): max_profit += max(0, prices[i + 1] - prices[i]) return max_profit 问题3：最多两次交易原题：Best Time to Buy and Sell Stock III 题意：在问题1的基础上，交易次数变为最多两次 这和问题1非常类似，意思是在问题1交易完成后，还能再交易1次。站在问题1的基础上，我们把解法扩展一下。 问题1的的贪心解法的成功运用，表示问题1其实是具有局部最优性质的，意思是这个算法可以用于任何一个局部，当这个局部扩大到全局的时候，就得到的原问题的最优解。那如果把交易次数增加到2次，这种局部最优性质是否还有呢，答案是有的。 我们可以设两对个变量，一对记录第一次交易状态，一对记录第二次交易状态，代码如下： 12345678910111213class Solution(object): def maxProfit(self, prices): """ :type prices: List[int] :rtype: int """ debt1, money1, debt2, money2 = -2147483648, 0, -2147483648, 0 for price in prices: money2 = max(money2, debt2 + price) debt2 = max(debt2 , money1 - price) money1 = max(money1, debt1 + price) debt1 = max(debt1 , -price) return money2 问题4：最多K次交易原题：Best Time to Buy and Sell Stock IV 题意：在问题1的基础上，交易次数变为最多K次，K是程序的输入 注意到K是可变的，所以： K = 1时，退化成问题1 K = 2时，退化成问题3 K &gt;= 总天数/2 时，退化成问题2 对于第3种情况，我们直接调用问题2的解法即可，以免超时 1234567891011121314151617181920212223class Solution(object): def maxProfit(self, k, prices): """ :type k: int :type prices: List[int] :rtype: int """ if k &gt;= len(prices) / 2: return maxProfitAny(prices) debt, money, max_money = [-2147483648 for i in range(k)], [0 for i in range(k)], 0 for price in prices: for t in range(k - 1, - 1, -1): money[t] = max(money[t], debt[t] + price) debt[t] = max(debt[t], (money[t - 1] if t &gt; 0 else 0) - price) max_money = max(max_money, money[t]) return max_money def maxProfitAny(prices): max_profit = 0 for i in range(len(prices) - 1): max_profit += max(0, prices[i + 1] - prices[i]) return max_profit]]></content>
      <categories>
        <category>编程题</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[提高你的记忆力：记忆宫殿法]]></title>
    <url>%2F%2Fp%2F61075.html</url>
    <content type="text"><![CDATA[原文：知乎 - 世界上真的存在记忆宫殿吗？常人能掌握吗？ 世界上确实是存在记忆宫殿的，而且并不是一些人想象中那样玄妙和难以掌握，许多答主分享了自己建立记忆宫殿的经验和心得，就在这里给大家整理一下建立记忆宫殿的简单步骤吧。 建造记忆宫殿的原理我们的大脑擅长记忆图像信息与空间信息，而不擅长于记忆文字类信息（比如单词）。于是，人们试着将自己不擅长记忆的文字信息，转变为图像和空间信息来进行记忆，比如为自己造一座“记忆宫殿（memory palace）”。“记忆宫殿”被证明能有效地提升人们对文字的记忆力（Foer, 2011）。 建造属于自己的记忆宫殿只需要五步第一步：寻找一个合适的宫殿建议选择一个你熟悉的地方作为你的记忆宫殿，比如你的家，或是你平时习惯行走的街道。越是熟悉的场景，我们越是能在脑海中清晰地再现场景中的细节（比如摆设和建筑），也有利于我们接下来将需要记忆的内容与这些细节联系起来。 第二步：规划你在宫殿中行走的路线想象你走过平时上班会路过的街道，你会从哪里开始？你在哪里会拐弯？你选择在哪里结束？试着闭上眼、在脑海中多走几遍，直到我们确定这条路径已经牢牢映在我们的脑海中。要注意，每次走过的路线一定得是一样的。如果你选择把自己的家作为记忆宫殿，那么需要想清楚：一般从哪个房间开始？会经过哪些房间？在哪个房间停下，等等。 第三步：选择一些有特征的物品这一步要求你在宫殿中设置一些特征物。比如说街边的建筑、或是房间里的大型家具（床、柜子等）。建议选一些大的物体作为特征物，因为它们能更好地吸引我们的注意力。 现在，想象我们自己重新沿着第二步的路径行走，并在过程中注意那些特征物：你在街边看到的第一个建筑是什么？它是什么样的？第二个引起你注意的物体是什么？每条街上最好有5~10个不同的特征物。如果你觉得很难同时想象特征物与路径的话，你可以画下平面图，来帮助自己更好地进行空间想象。 第四步：把特征物和和记忆内容建立联系在这一步，把你想要记住的内容与你的记忆宫殿联系在一起。这些联系越是形象和荒诞，越是能加深我们的记忆。举个例子，比如你想记住一句英文，开头第一个单词是“mania（躁狂）”，它的第一个音节读起来很像“妈”，于是你可以在你的记忆宫殿的第一个特征物上印上一个“疯狂妈妈”的形象（比如，你走上街，发现自己看到的第一个建筑上印着一个大笑的、手舞足蹈的妈妈形象）。 接下来，按照顺序，用类似的方法将句子中剩下单词与宫殿中的特征物一一联系起来。这样，你就可以通过在宫殿中行走、浏览特征物，来复述句子了。 第五步：重复地参观你的宫殿这一步主要是巩固我们的记忆。在这一步中，我们从之前开始的地方、遵循先前的路线，重复地参观我们的宫殿。注意我们经过的特征物，当我们经过它们时，那些与特征物有关的记忆就会逐渐浮现。如果你觉得一开始有些困难，你还是可以用平面图、或者大声说出记忆内容的方法，来帮助你进行空间想象。而经过几次练习后，我们的空间想象能力会随之提高。 其他方法除了记忆宫殿之外，还有一些其他帮助记忆的技术可以帮助我们更长久、更准确的记忆。比如： 使用“闪存卡（flash card）”技术闪存卡技术是一项很有效的、帮助人们牢记知识点的方法（Boser, 2017)）。方法很简单，首先，将你需要记住的知识点，用提问和回答的方式，写在一张卡片的正反面（比如正面写“记忆的定义是？”，反面写上答案）；通过这样的方法，你可能会制作出一叠闪存卡。 随后，浏览这些卡片，将你能回答出的部分挑出放在一边，称为“已答卡”；而余下的卡片放在另一边，称为“未答卡”。接下来，反复地抽取和回答“未答卡”上的问题，直到你能将所有卡片上的问题都回答正确。你可以将一些闪存卡带在身边，在通勤时抽卡进行复习。 捏压力球研究发现，捏压力球有助于提升人的记忆能力。在一项实验中，右撇子参与者在开始记信息之前，用右手捏压力球45秒；在背完信息后，再用左手捏压力球45秒（左撇子参与者是先左手后右手）。结果显示，比起没有捏压力球的人，捏了压力球的参与者对信息记得更多、更牢。这可能是因为，用惯用手捏压力球，可以刺激大脑中负责解码、信息的区域，帮助我们更好地感知和分析需要记忆的信息（Szalavitz, 2013）。]]></content>
      <categories>
        <category>转载</category>
      </categories>
      <tags>
        <tag>知乎</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Exercise2: Logistic Regression]]></title>
    <url>%2F%2Fp%2F48722.html</url>
    <content type="text"><![CDATA[序一个月快过去了，总算是学完Coursera上的第三课了，其实我的进度比预期满了一周。这是第二次做Coursera的编程作业，对比大学课程的作业，Coursera的难度应该在中等甚至偏下，对我来说唯一的难度在于适应“新”，有以下几点： 新语言 新工具 新的思考方式 对于一个Software Engineer来说以上都是最基础的技能，即保持学习态度，及时学习新技术、新知识。 第三周的课程内容主要讲分类、逻辑回归，与线性回归相比，也就是把Hypothesis函数用Sigmoid函数来表示，也就是：$设 g(z) = \frac{1} {1 + e ^{-z}}$，$令 z = θ^T x$，$则 h_θ(x) = g(θ^Tx)$ 为什么要 $设 g(z) = \frac{1} {1 + e ^{-z}}$ 呢，因为它的值域为 $(0, 1)$，定义域为(-∞, +∞) 的单调连续增函数，这意味着可以用换元法把所有函数的值域压缩到$(0, 1)$，它的函数图像如下。 这非常适合作为二分类（01分类）函数，我们可以取 $ hθ(x) &gt;= 0.5$ 时 $y = 1$ ，$ hθ(x) &lt; 0.5$ 时 $y = 0$ 。 同时Cost函数和Gradient Descent方程也有些许改变，这种改变是换元法带来的（公式识别失败，用图代替了） 它们对应的矩阵形式是 ① $J(θ) = \frac {1} {m} (- y ^T log(h) - (1 - y)^T log(1 - h))，h = g(Xθ)$ ② $θ = θ - \frac {α} {m} X^T (g(Xθ) - y)$ 注意其中$X、y、θ$ 都是矩阵 作业内容标*的是需要做的，没标的是作业中已经实现 数据可视化 * Sigdmoid函数12345678910111213141516function g = sigmoid(z)%SIGMOID Compute sigmoid function% g = SIGMOID(z) computes the sigmoid of z.% You need to return the following variables correctly g = zeros(size(z));% ====================== YOUR CODE HERE ======================% Instructions: Compute the sigmoid of each value of z (z can be a matrix,% vector or scalar). g = 1 ./ (1 + exp(-z));% =============================================================end 传入的参数是一个矩阵，所以需要算出每一个元素的sigmoid函数值，这里用到的./操作符来对每一个元素进行取倒数操作。 * Cost函数和对应的Gradient Descent方程123456789101112131415161718192021222324252627282930function [J, grad] = costFunction(theta, X, y)%COSTFUNCTION Compute cost and gradient for logistic regression% J = COSTFUNCTION(theta, X, y) computes the cost of using theta as the% parameter for logistic regression and the gradient of the cost% w.r.t. to the parameters.% Initialize some useful valuesm = length(y); % number of training examples% You need to return the following variables correctly J = 0;grad = zeros(size(theta));% ====================== YOUR CODE HERE ======================% Instructions: Compute the cost of a particular choice of theta.% You should set J to the cost.% Compute the partial derivatives and set grad to the partial% derivatives of the cost w.r.t. each parameter in theta%% Note: grad should have the same dimensions as theta%J = 1/m * sum(-y .* log(sigmoid(X * theta)) - (1 - y) .* log(1 - sigmoid(X * theta)));grad = 1/m * (X' * (sigmoid(X * theta) - y));% =============================================================end 在这个函数中需要同时计算代价和梯度，通过公式①、②不难得到上述两行代码 * Hypothesis函数1234567891011121314151617181920212223function p = predict(theta, X)%PREDICT Predict whether the label is 0 or 1 using learned logistic %regression parameters theta% p = PREDICT(theta, X) computes the predictions for X using a % threshold at 0.5 (i.e., if sigmoid(theta'*x) &gt;= 0.5, predict 1)m = size(X, 1); % Number of training examples% You need to return the following variables correctlyp = zeros(m, 1);% ====================== YOUR CODE HERE ======================% Instructions: Complete the following code to make predictions using% your learned logistic regression parameters. % You should set p to a vector of 0's and 1's%s = sigmoid(X * theta);p = floor(s + 0.5);% =========================================================================end 预测函数就比较简单的，由$则 h_θ(x) = g(θ^Tx)$，$g$ 是Sigmoid函数，可以得到上述两行代码，因为当函数值大于0.5时$y = 1$ ，又得到的 $s$ 其实是一个矩阵，所以使用floor(s + 0.5)来进行批量操作。 完成上述三个函数后，运行ex2函数可以得到一条预测线，如图。 * 正规化正规化是为了防止过拟合，核心是保持 $ x$ 值不变， 减小 (惩罚) $θ$的值。 当然还有一种方式防止过拟合的方法是建立一个模型来去除用处不大的特征，保留作用相对较大的特征。 上图表示的预测函数，从左到右依次是欠拟合、拟合 也叫刚好拟合、过拟合，欠拟合一般是因为多项式的最高次太小，过拟合一般是因为多项式的最高次幂太大。 正规化后的代价函数如下，注意尾巴上对 $θ_j$ 的求和是从1开始，也就意味着跳过了 $θ_0$ 值得注意的是梯度方程不需要对 $θ_0$ 进行正规化，所以方程变成了两部分： 12345678910111213141516171819202122232425262728function [J, grad] = costFunctionReg(theta, X, y, lambda)%COSTFUNCTIONREG Compute cost and gradient for logistic regression with regularization% J = COSTFUNCTIONREG(theta, X, y, lambda) computes the cost of using% theta as the parameter for regularized logistic regression and the% gradient of the cost w.r.t. to the parameters. % Initialize some useful valuesm = length(y); % number of training examples% You need to return the following variables correctly J = 0;grad = zeros(size(theta));% ====================== YOUR CODE HERE ======================% Instructions: Compute the cost of a particular choice of theta.% You should set J to the cost.% Compute the partial derivatives and set grad to the partial% derivatives of the cost w.r.t. each parameter in thetaJ = 1/m * sum(-y .* log(sigmoid(X * theta)) - (1 - y) .* log(1 - sigmoid(X * theta)));J = J + lambda/(2*m) * sum(theta(2:size(theta)) .^ 2);grad = 1/m * (X' * (sigmoid(X * theta) - y)) + lambda/m * theta;grad(1) = grad(1) - lambda/m * theta(1);% =============================================================end 根据上述的三张图不难写出上述四行代码 调整λ参数的大小λ = 1 λ = 10 λ = 0.0001 可以看出 $λ$ 越小，对训练样本的拟合程度越好，从而导致了整个曲线看起来比较扭曲，没有平滑感；而 $λ$ 越大，对训练样本的拟合程度越差，曲线越光滑。]]></content>
      <categories>
        <category>研</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Centos7修改docker的Data Space Total大小]]></title>
    <url>%2F%2Fp%2F55543.html</url>
    <content type="text"><![CDATA[记得备份你的容器此处就不多做介绍了 –storage-opts 参数介绍devicemapper文档：https://github.com/moby/moby/tree/master/daemon/graphdriver/devmapperdocker官方文档：https://docs.docker.com/engine/reference/commandline/dockerd/#options-per-storage-driver 修改–storage-opts参数停止docker，修改配置，重新加载配置12sudo systemctl stop dockersudo vi /lib/systemd/system/docker.service 找到ExecStart=/usr/bin/dockerd在这一行后面加上--storage-opt dm.loopdatasize=8G --storage-opt dm.loopmetadatasize=4G --storage-opt dm.basesize=8G 意思是为，设置devicemapper的data为8G，metadata为4G，镜像的大小不能大于8G 记得还要抹去现有的空间，请确保你已经完成了第一步1234sudo rm -rf /var/lib/dockersudo mkdir -p /var/lib/docker/devicemapper/devicemapper/sudo dd if=/dev/zero of=/var/lib/docker/devicemapper/devicemapper/data bs=1M count=0 seek=8192sudo dd if=/dev/zero of=/var/lib/docker/devicemapper/devicemapper/metadata bs=1M count=0 seek=4096 完成后运行123sudo systemctl daemon-reloadsudo systemctl start dockerdocker info 查看是否设置正确，貌似比预设的大了一点点。]]></content>
      <categories>
        <category>软件技术</category>
      </categories>
      <tags>
        <tag>devopps</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mac下使用命令行或脚本删除文件到废纸篓]]></title>
    <url>%2F%2Fp%2F35267.html</url>
    <content type="text"><![CDATA[mac删除文件有两种方式： 使用Finder的移到废纸篓功能 使用rm命令第二种方式删除的文件，不能在废纸篓中找到，也就是所谓的彻底删除但是我们在使用terminal的时候，一般都会使用rm删除文件，那要是删错了不就完了？而且我们的mac上本身又自带了废纸篓，为什么不能将两者结合起来呢？于是我做了一些探究 Mac废纸篓的真实面目1$ ls ~/.Trash 你会发现，~/.Trash目录就是废纸篓但是它只是一个普通的目录，只是Finder将删除的文件移动到了这个目录而已，不相信？接着往下看 ~/.Trash的本质只是文件夹123cd ~echo &quot;666666&quot; &gt;&gt; wantodel.txtmv wantodel.txt ~/.Trash 此时打开你的废纸篓一看，里面有一个wantodel.txt文件，但是你无法将它放回原处，而如果你使用Finder的移到废纸篓功能删除的文件却可以放回原处 Finder是如何实现放回原处的呢？最有可能是将操作记录到了数据库中。 不大可能写在log日志中，因为我grep了整个磁盘都没找到相关的文件。 最好的帮手是Finder既然~/.Trash只是普通文件夹，那我们单纯使用linux命令是无法达到目的的了。我们现在已知Finder可以达到预期目的，如果我们能调用或是告诉Finder的我们要做什么，并且它也愿意做，不就可以达到目的了么？在window中有一个消息的概念，意思是一个应用程序A可以对另一个应用程序B发送消息以操作B来完成某项指定的任务，那Mac种是否也有这种或是类似这种的机制呢？答案是AppleScript，是一种脚本语言，可以用来控制Mac上的应用程序。最后附上使用shell通知Finder程序移动文件废纸篓的样例代码1234567#!/bin/bashfp=/absolute/path/to/fileosascript &lt;&lt; EOFtell application "Finder" posix path of ((delete posix file "$&#123;fp&#125;") as unicode text)end tellEOF]]></content>
      <categories>
        <category>软件技术</category>
      </categories>
      <tags>
        <tag>mac</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git托管现有代码]]></title>
    <url>%2F%2Fp%2F49319.html</url>
    <content type="text"><![CDATA[场景：将现有的文件托管至新的git repo 用到的命令： git clone 复制网络仓库到本地 git add [file] 加入一个文件/目录到版本控制 git commit 提交修改到仓库(本地) git push 推送对仓库的修改到网络仓库 操作步骤如果对一个非空目录执行clone命令是会失败的，因为git不允许这样做。 进入项目目录（非空），运行下面命令： 1234567cd /path/to/dirgit clone &#123;repo_url&#125; tmpmv tmp/.git .rmdir tmpgit add .git commit -m &apos;current files&apos;git push 解释.git是一个隐藏的git用于管理本地代码的目录 git clone {repo_url} tmp 其实是把远程repo加载到tmp目录 将tmp/.git 直接移动到项目的根目录后运行git add . 其实相当于是把项目的文件放到了本地的git repo中 此时运行git commit 就提交了现有项目文件到本地，git push 就把操作推向了服务器 操作完成后，你的另外一个同事就可以使用git clone {repo_url} 来加载你的代码]]></content>
      <categories>
        <category>软件技术</category>
      </categories>
      <tags>
        <tag>无标签</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[打开FTP服务器上的文件夹时发生错误,请检查是否有权限访问该文件夹]]></title>
    <url>%2F%2Fp%2F11354.html</url>
    <content type="text"><![CDATA[环境：Wndows2012 R2， 已完成操作：FTP服务器搭建、防火墙规则允许(或关闭防火墙)、权限已给(Administrator) 客户端：FileZilla（或ie浏览器、windows资源管理器） 场景： 使用windows资源管理器连接ftp报错：“打开FTP服务器上的文件夹时发生错误,请检查是否有权限访问该文件夹” 使用FileZilla链接时超时 最后解决方法： 在FileZilla中将FTP模式改为PORT(主动模式)即可]]></content>
      <categories>
        <category>软件技术</category>
      </categories>
      <tags>
        <tag>无标签</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[负数的除2和右移1位]]></title>
    <url>%2F%2Fp%2F14496.html</url>
    <content type="text"><![CDATA[令人费解的输出且看如下代码，试问输出是什么。 1234int F, G, X = -5;F = X / 2;G = X &gt;&gt; 1;Console.WriteLine(&quot;F = &#123;0&#125;, G= &#123;1&#125;&quot;, F, G); 老师说过“乘2是二进制左移1位”，那除2理所当然应该是右移1位，所以两者结果是一样的。 然而，输出却是F = -2, G = -3 ，在VS2013中，换成在codeblocks中，结果是一样的。 反汇编在VS2013中对上述代码反汇编得到下图 第一句：F = X / 2 1234500DF39F7 mov eax,dword ptr [ebp-58h] ;将X的值移到寄存器eax 00DF39FA mov ecx,2 ;将值2移到ecx 00DF39FF cdq ;将eax高位扩展到edx 00DF3A00 idiv eax,ecx ;做除法运算 00DF3A02 mov dword ptr [ebp-50h],eax ;移动到内存 idiv指令是带符号的二进制除法 第二句：G = X &gt;&gt; 1 逻辑右移，最低位被舍弃 结论除法运算，结果都向0取整；位运算结果向下取整]]></content>
      <categories>
        <category>软件技术</category>
      </categories>
      <tags>
        <tag>汇编</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[汇编实现回文判断]]></title>
    <url>%2F%2Fp%2F9879.html</url>
    <content type="text"><![CDATA[不多说，直接上代码 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061DATA SEGMENT Prompt DB &apos;Please enter a string:$&apos; YesStr DB 13, 10, &apos;This string is palindrome$&apos; NoStr DB 13, 10, &apos;This string is not palindrome$&apos; Input DB 128, ?, 128 DUP(0) DATA ENDS STACK SEGMENT DB 128 DUP(?) STACK ENDS CODE SEGMENT ASSUME CS:CODE, DS:DATA, SS:STACK MAIN: MOV AX, DATA MOV DS, AX MOV DX, offset Prompt MOV AH, 09H INT 21H ; MOV DX, OFFSET Input MOV AH, 0AH INT 21H ; MOV DI, OFFSET Input MOV SI, OFFSET Input XOR CX, CX MOV CL, [DI+1] ADD DI, 2 ADD SI, 2 DEC CX ADD DI, CX ;force DI point to the last word of Input AGAIN: CMP SI, DI JA TESSKIP MOV AH, byte ptr [SI] MOV AL, byte ptr [DI] CMP AH, AL JNE NOSKIP INC SI DEC DI JMP AGAIN TESSKIP: MOV DX, OFFSET YesStr MOV AH, 09H INT 21H JMP ENDSKPI ; NOSKIP: MOV DX, OFFSET NoStr MOV AH, 09H INT 21H ; ENDSKPI: MOV AH, 4CH INT 21H CODE ENDS END MAIN]]></content>
      <categories>
        <category>软件技术</category>
      </categories>
      <tags>
        <tag>汇编</tag>
      </tags>
  </entry>
</search>
