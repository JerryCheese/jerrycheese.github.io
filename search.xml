<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[快速幂（取模）算法]]></title>
    <url>%2F%2Fp%2F18313.html</url>
    <content type="text"><![CDATA[问题计算取模运算的值 $ a^n \% b$，其中a、b、n均为int类型的变量。 分析因为 $a^n$ 可能很大，所以不能先计算 $a^n$ 再将其 $ \%b$ 。 现有取模公式，写作： $ (a*b) \% c = [ (a \% c) \times (b \% c) ] \% c $ 那么： 幂为偶数，则有 $ a^n \% b = (a^{\frac n 2} \times a^{\frac n 2}) \% b = [(a^{\frac n 2} \% b) \times (a^{\frac n 2}) \% b ] \% b​$ 幂为奇数，则有 $ (a \times a^{n-1}) \% b = [(a \% b) \times (a^{n-1} \% b)] \%b $，其中 $ a^{n-1} \% b$ 对应到幂为偶数的情况 代码递归递归写法（会溢出） 1234567891011121314int fastPower(int a, int b, int n) &#123; if (n == 0) &#123; return 1 % b; &#125; else if (n == 1) &#123; return a % b; &#125; if (n % 2 == 1) &#123; return ((a % b) * fastPower(a, b, n - 1)) % b; &#125; else &#123; int res = fastPower(a, b, n / 2); return (res * res) % b; &#125;&#125; 溢出点： 奇数幂时，在计算 res * res 时可能超出int范围。 偶数幂时，在计算(a % b) * fastPower(a, b, n - 1) 时可能超出范围 改正后： 1234567891011121314int fastPower(int a, int b, int n) &#123; if (n == 0) &#123; return 1 % b; &#125; else if (n == 1) &#123; return a % b; &#125; if (n % 2 == 1) &#123; return ((a % b) * (long long)fastPower(a, b, n - 1)) % b; &#125; else &#123; long long res = fastPower(a, b, n / 2) % b; return (res * res) % b; &#125;&#125; 迭代所有的递归函数都能写成迭代的形式。 123456789int fastPower(int a, int b, int n) &#123; long long r = 1, aa=a; while(n) &#123; if (n &amp; 1 == 1) r = (r * aa) % b; n &gt;&gt;= 1; aa = (aa * aa) % b; &#125; return r % b;&#125;]]></content>
      <categories>
        <category>编程题</category>
      </categories>
      <tags>
        <tag>剑指Offer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NLTK之词性标注]]></title>
    <url>%2F%2Fp%2F63432.html</url>
    <content type="text"><![CDATA[词性标注重要性回想学英语的时候，老师就开始讲词性，通过分析句子中某个单词的词性，我们可以推测这个词的意思，猜测这个词在句子中的作用，这对理解句子意思有极大的帮助。小弟也还是初学，以后若发现词性有更多作用时会继续补充~ 标注语料库NLTK(3.2.5)中提供了一些已经标注好词性的文本，通过下面代码可以查看： 12345import nltknltk.corpus.brown.tagged_words()outputs:[(u'The', u'AT'), (u'Fulton', u'NP-TL'), ...] 这表示The 被标注为AT词性，Fulton 被标注为NP-TL 词性，似乎看不太懂？ 下面可以把它们转成统一词性名称： 1234nltk.corpus.brown.tagged_words(tagset='universal')outputs:[(u'The', u'DET'), (u'Fulton', u'NOUN'), ...] DET 是限定词，NOUN 是名词。 这是因为标注器本身所使用的符号与统一符号 不一样的原因，通过制定tagset 可以转化为统一符号，而标注转换可以在~/nltk_data/taggers/universal_tagset 中找到对应的文件。 我调查源代码发现，上述代码所使用的是en-brown.map文件，打开查看可以发现： 12345....36 AT DET....294 NP-TL NOUN... 其他标注语料库如 1234nltk.corpus.sinica_treebank.tagged_words()nltk.corpus.indian.tagged_words()nltk.corpus.mac_morpho.tagged_words()... 拿indian为例： 1234nltk.corpus.indian.tagged_words()outputs:[(u'\u09ae\u09b9\u09bf\u09b7\u09c7\u09b0', u'NN'), (u'\u09b8\u09a8\u09cd\u09a4\u09be\u09a8', u'NN'), ...] 似乎输出了unicode_escape编码，怎么办呢？下面可以解决： 1234print ', '.join([word + '/ + tag for (word, tag) in nltk.corpus.indian.tagged_words()][:100])outputs:মহিষের/NN, সন্তান/NN, :/SYM, তোড়া/NNP, উপজাতি/NN, ৷/SYM, বাসস্থান-ঘরগৃহস্থালি/NN, তোড়া/NNP, ভাষায়/NN, গ্রামকেও/NN, বলে/VM, `/SYM, মোদ/NN, '/SYM, ৷/SYM, মোদের/NN, আয়তন/NN, খুব/INTF, বড়ো/JJ, নয়/VM, ৷/SYM, প্রতি/QF, মোদে/NN, আছে/VM, কিছু/QF, কুঁড়েঘর/NN, ,/SYM, সাধারণ/JJ, মহিষশালা/NN, ৷/SYM, আর/CC, গ্রামের/NN, বাইরে/NST, থাকে/VM, ডেয়ারি-মন্দির/NN, ৷/SYM, আয়তনের/NN, তারতম্য/NN, অনুসারে/PSP, গ্রামগুলি/NN, দু/QC, রকমের/NN, :/SYM, এতূডমোদ/NNP, (/SYM, বড়ো/JJ, গ্রাম/NN, )/SYM, ওকিনমোদ/NNP, (/SYM, ছোট/JJ, গ্রাম/NN, )/SYM, ৷/SYM, কোন/DEM, কোন/RDP, গ্রামের/NN, আবার/CC, ধর্মীয়/JJ, বা_Cমহিষের/NN, সন্তান/NN, :/SYM, তোড়া/NNP, উপজাতি/NN, ৷/SYM, িকে/PRP, বলে/VM, `/SYM, সোতি-মোদ/NNP, '/SYM, ৷/SYM, কুঁড়েঘরগুলির/NN, আকার/NN, বাংলার/NNP, বা/CC, ভারতের/NNP, অন্য/JJ, অঞ্চলের/NN, প্রচলিত/JJ, কুঁড়ে/NN, ঘর/NN, নয়/VM, ৷/SYM, এগুলি/PRP, দেখতে/NN, শোয়ানো/JJ, পিপের/NN, মতো/PSP, ৷/SYM, এক/QC, দিকের/PSP, বাঁশের/NN, কাঠামো/NN, খিলানের/NN, মতো/PSP, বেঁকে/JJ, গিয়ে/VM, অন্যদিকের/NN, মাটিতে/NN, মিশেছে/VM 标注器使用标注器NLTK提供了现成的标注器，你可以直接使用： 12345text = nltk.word_tokenize('This beautiful future is just his imagination so far')nltk.pos_tag(text, tagset='universal')outputs:[('This', u'DET'), ('beautiful', u'ADJ'), ('future', u'NOUN'), ('is', u'VERB'), ('just', u'ADV'), ('his', u'PRON'), ('imagination', u'NOUN'), ('so', u'ADV'), ('far', u'ADV')] 你觉得这个标注器的准确率怎么样呢？ 似乎完成的还不错，那么我们试试另外一个句子： 12345text = nltk.word_tokenize('They refuse to permit us to obtain the refuse permit')nltk.pos_tag(text, tagset='universal')outputs:[('They', u'PRON'), ('refuse', u'VERB'), ('to', u'PRT'), ('permit', u'VERB'), ('us', u'PRON'), ('to', u'PRT'), ('obtain', u'VERB'), ('the', u'DET'), ('refuse', u'NOUN'), ('permit', u'NOUN')] 对于文中的两个refuse，前者被标为动词，后者被标为名词，完成的还不错。 这有什么意义呢？拿第二个句子来说，其实两个refuse的读音不一样，第一个读作refUSE，第二个读作REFuse，所以语音系统为了正确的发音，需要先做词性标注才行。 自动标注器为了更好的理解标注器的原理，我们慢慢来自建构建一个词性标注器，先载入数据： 12345from nltk.corpus import brownbrown_tagged_sents = brown.tagged_sents(categories='news')brown_sents = brown.sents(categories='news')brown_tagged_words = brown.tagged_words(categories='news')brown_words = brown.words(categories='news') 默认标注器这是最简单的标注器了，它给所有的标识符都分配同样的词性标记，我们先看来来哪个标记是最有可能的： 12345tags = [tag for (word, tag) in brown_tagged_words]nltk.FreqDist(tags).max()outputs:u'NN' 说明名词是最多的，那么我们就生成一个标注器，它将所有词都标注为名词： 12345default_tagger = nltk.DefaultTagger('NN')default_tagger.tag(nltk.word_tokenize('This beautiful future is just his imagination so far'))outputs:[('This', 'NN'), ('beautiful', 'NN'), ('future', 'NN'), ('is', 'NN'), ('just', 'NN'), ('his', 'NN'), ('imagination', 'NN'), ('so', 'NN'), ('far', 'NN')] 可以看到，已经全部标注为NN了，下面评估一下我们这个标注器： 1234default_tagger.evaluate(brown_tagged_sents)outputs:0.13089484257215028 哈哈，说明这个标注器太差了，它的标注正确率只有13.1%。 虽然如此，但碰巧的是在处理大量文本的时候，大部分新词都是名词，这意味着默认标注器可以帮助我们提高语言处理系统的稳定性。 正则表达式标注器在英语单词中，我们可以通过后缀ness、ing、ed等来推测一个单词的词性，那么这样做是否也有效呢？试试就知道啦~ 12345678910patterns = [ (r'.*ing$', 'VBG'), (r'.*ed$', 'VBD'), (r'.*es$', 'VBZ'), (r'.*ould$', 'MD'), (r'.*\'s$', 'NN$'), (r'.*s$', 'NNS'), (r'^-?[0-9]+(.[0-9]+)?$', 'CD'), (r'.*', 'NN')] 按照顺序匹配，当全部都不匹配时，最后会被标注为NN词性。 12345regexp_tagger = nltk.RegexpTagger(patterns)regexp_tagger.tag(brown_sents[3])outputs:[(u'``', 'NN'), (u'Only', 'NN'), (u'a', 'NN'), (u'relative', 'NN'), (u'handful', 'NN'), (u'of', 'NN'), (u'such', 'NN'), (u'reports', 'NNS'), (u'was', 'NNS'), (u'received', 'VBD'), (u"''", 'NN'), (u',', 'NN'), (u'the', 'NN'), (u'jury', 'NN'), (u'said', 'NN'), (u',', 'NN'), (u'``', 'NN'), (u'considering', 'VBG'), (u'the', 'NN'), (u'widespread', 'NN'), (u'interest', 'NN'), (u'in', 'NN'), (u'the', 'NN'), (u'election', 'NN'), (u',', 'NN'), (u'the', 'NN'), (u'number', 'NN'), (u'of', 'NN'), (u'voters', 'NNS'), (u'and', 'NN'), (u'the', 'NN'), (u'size', 'NN'), (u'of', 'NN'), (u'this', 'NNS'), (u'city', 'NN'), (u"''", 'NN'), (u'.', 'NN')] 评估一下： 1234regexp_tagger.evaluate(brown_tagged_sents)outputs:0.20326391789486245 比默认标注器要好点，哈哈 查询标注器可以发现，名词虽然出现的频率最高，但出现频率最高的词未必都是名词，所以我们可以试试取频率最大的前100个词，用他们最有可能的词性来进行标注。 123456789fd = nltk.FreqDist(brown_words)cfd = nltk.ConditionalFreqDist(brown_tagged_words)most_freq_words = fd.most_common()[:100]likely_tags = dict((word, cfd[word].max()) for (word, freq) in most_freq_words)baseline_tagger = nltk.UnigramTagger(model=likely_tags)baseline_tagger.evaluate(brown_tagged_sents)outputs:0.45578495136941344 可见，就算只取前100个词，效率也已经比之前高很多了。 我们实地看看它的工作结果： 1234baseline_tagger.tag(brown_sents[3])outputs:[(u'``', u'``'), (u'Only', None), (u'a', u'AT'), (u'relative', None), (u'handful', None), (u'of', u'IN'), (u'such', None), (u'reports', None), (u'was', u'BEDZ'), (u'received', None), (u"''", u"''"), (u',', u','), (u'the', u'AT'), (u'jury', None), (u'said', u'VBD'), (u',', u','), (u'``', u'``'), (u'considering', None), (u'the', u'AT'), (u'widespread', None), (u'interest', None), (u'in', u'IN'), (u'the', u'AT'), (u'election', None), (u',', u','), (u'the', u'AT'), (u'number', None), (u'of', u'IN'), (u'voters', None), (u'and', u'CC'), (u'the', u'AT'), (u'size', None), (u'of', u'IN'), (u'this', u'DT'), (u'city', None), (u"''", u"''"), (u'.', u'.')] 可以看到有很多是None，说明它没有出现在前100个词中，这时候我们可以把它们交给默认标注器处理，也就是标记为NN，这个转移工作叫做回退。 12345baseline_tagger = nltk.UnigramTagger(model=likely_tags, backoff=nltk.DefaultTagger('NN'))baseline_tagger.evaluate(brown_tagged_sents)outputs:0.5817769556656125 准确率瞬间提高了10%+ 有木有！！ 如果取更多的词呢？下面给出数据： 高频词数量 准确率 200 0.5060962269029576 800 0.6335401873620145 1600 0.7067247449131809 3200 0.7813513137219802 说明随着数量增加，准确率还会提升~ N-gram标注一元标注它使用简单的统计算法，给每一个词分配一个最可能的标记，不会关联上下文。 12345unigram_tagger = nltk.UnigramTagger(brown_tagged_sents)unigram_tagger.evaluate(brown_tagged_sents)outputs:0.9349006503968017 准确率还蛮高的，学过的同学知道，这其实是过拟合啦，不信？我们试试其他十个语料库： 12345678910111213print "\n".join([cate + "\t" + str(unigram_tagger.evaluate(brown.tagged_sents(categories=cate))) for cate in brown.categories()[:10]])outputs:adventure 0.787891898128belles_lettres 0.798707075842editorial 0.813940653204fiction 0.799147295877government 0.807778427485hobbies 0.771327949481humor 0.793178151648learned 0.772942690007lore 0.798085204762mystery 0.80790288443 准确率表现在80%左右，下降了10%多，影响还是蛮大的啦。 一般的N-gram的标注它是根据上下文来推断词性的。比如一段句子是 $w{n-2} w{n-1} w{n} w{n+1}$，对应的词性是$t{n-2} t{n-1} t{n} t{n+1}$，三元标准器（n=3）就是考虑当前词$w{n}$ 的前两个词的标记$t{n-2} t{n-1}$ ，我们来推断$t{n}$ 的词性。 下面是一个二元标注器（即只考虑前一个词） 12345bigram_tagger = nltk.BigramTagger(brown_tagged_sents)bigram_tagger.evaluate(brown_tagged_sents)outputs:0.7860751437038805 三元标注器： 12345trigram_tagger = nltk.TrigramTagger(brown_tagged_sents)trigram_tagger.evaluate(brown_tagged_sents)outputs:0.8223641028700996 准确率要高点。我们来试试它对其他文本的准确率怎么样 12345678910111213print "\n".join([cate + "\t" + str(trigram_tagger.evaluate(brown.tagged_sents(categories=cate))) for cate in brown.categories()[:10]])outputs:adventure 0.0947189293646belles_lettres 0.0632885797477editorial 0.0675930134407fiction 0.0889498890317government 0.0531682758817hobbies 0.0558503855729humor 0.0754551740032learned 0.0566172589726lore 0.0623759054933mystery 0.096993125645 好吧，我怀疑我用了假标注器！ 我们组合一下所建的标注器： 1234567t0 = nltk.DefaultTagger('NN')t1 = nltk.UnigramTagger(brown_tagged_sents, backoff=t0)t2 = nltk.BigramTagger(brown_tagged_sents, backoff=t1)t2.evaluate(brown_tagged_sents)outputs:0.9730592517453309 嗯。。。准确率还不错，下面试试： 12345678910111213print "\n".join([cate + "\t" + str(t2.evaluate(brown.tagged_sents(categories=cate))) for cate in brown.categories()[:10]])outputs:adventure 0.835626315941belles_lettres 0.840522022462editorial 0.849977274203fiction 0.84151968228government 0.844089165252hobbies 0.825283866659humor 0.839041253745learned 0.836756685433lore 0.844033037471mystery 0.85128303801 em………还不错吧，下降也有10%，但准确率还有85%左右]]></content>
      <categories>
        <category>研</category>
      </categories>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS使用ss]]></title>
    <url>%2F%2Fp%2F64326.html</url>
    <content type="text"><![CDATA[ss客户端必要工具： python pip pip的安装方法： 1234wget https://bootstrap.pypa.io/get-pip.pysudo python get-pip.pyrm get-pip.pypip --version 我的的pip版本是 9.0.1 的 下面开始安装ss： 1sudo pip install shadowsocks 下面配置ss： 1sudo vi /etc/shadowsocks.json 内容如下 1234567891011&#123; "server": "0.0.0.0", "server_port": 0, "local_address": "127.0.0.1", "local_port": 10010, "password": "", "method": "aes-256-cfb", "timeout": 500, "fastopen": false, "workers": 1&#125; 除了server、server_port、password、method 四个主要参数外，还需特别添加local_address 和 local_port 两个配置，用户对内提供代理服务。 下面启动sslocal，也就是ss的linux客户端 1nohup sslocal -c /etc/shadowsocks.json /dev/null 2&gt;&amp;1 &amp; 如果你想开机自启动，则： 1echo "nohup sslocal -c /etc/shadowsocks.json /dev/null 2&gt;&amp;1 &amp;" /etc/rc.local 怎么查看是否启动成功呢？运行： 1sudo ps aux| grep sslocal 查看是否在运行即可，也可以运行： 1sudo netstat -l|grep 10010 查看10010端口是否已经被监听 注意，此时的10010端口提供的是socks5 的代理服务，不是http的，所以在/etc/profile文件中配置http_proxy=127.0.0.1:10010是不行的， 在这一层中需要有一个软件将socks5代理转化成http代理，privoxy可以做到这一点。 proxychains你是否觉得git clone 速度太慢？是的，因为git repo在国外，国内也没有比较完美的镜像源。proxychains可以将git clone 用上ss 代理，起到加速作用。 安装proxychains： 123456git clone https://github.com/rofl0r/proxychains-ng.gitcd proxychains-ngsudo ./configuresudo make &amp;&amp; sudo make installcp ./src/proxychains.conf /etc/proxychians.confcd .. &amp;&amp; rm -rf proxychains-ng 配置proxychains，让他使用本地sslocal起的代理服务： 1vim /etc/proxychains.conf 找到ProxyList 配置项（应该在文件末尾），添加 1socks5 127.0.0.1 10010 测试： 注意，命令是proxychains4 ，这里不能使用ping 命令测试，因为ping命令使用的是ICMP报文，不会走代理的。 1proxychains4 curl www.google.com privoxy你是否觉得每次运行命令都需要加proxychains4 觉得很烦？要是能让服务器的http链接直接上代理就好啦，所以privoxy就是做这个事的，它可以让你的wget、curl 等命令在http请求中使用配置好的代理。 如果你配置好了阿里云的yum源，那你赚大发了（所以我赚大发了），直接运行： 1sudo yum install privoxy 编辑配置文件： 1sudo vim /etc/privoxy/config 找到listen-address 127.0.0.1:8118 将注释去掉就行（默认应该没有注释啦~） 找到forward-socks5t / 去掉注释就行（改改端口啦） 运行： 1sudo systemctl start privoxy 开机自启动： 1sudo systemctl enable privoxy 此时privoxy会在8118 端口处提供http代理服务，包括https，之后只需配置/etc/profile即可 1sudo vim /etc/profile 添加： 123export http_proxy=http://127.0.0.1:8118export https_proxy=http://127.0.0.1:8118#export ftp_proxy=http://127.0.0.1:8118 最后一个ftp代理视自己需要打开吧，之后运行即可完成配置。 1source /etc/profile 接着使用curl www.google.com 来看看是否配置成功了吧~ 总结个人比较喜欢proxychains的方式，按需使用，毕竟ss流量还是比较损哈哈~~ 如果你在使用privoxy，那就要注意流量啦！]]></content>
      <categories>
        <category>软件技术</category>
      </categories>
      <tags>
        <tag>运维</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[摩尔投票法和大多数]]></title>
    <url>%2F%2Fp%2F45987.html</url>
    <content type="text"><![CDATA[摩尔投票算法假设有这样一个场景：票选村长，每人可投一票，我们将候选村长从1开始编号，村民们在票上写上候选村长的编号即可完成投票。那么最后统计的票可形成一个整型数组。那么谁是村长呢？票数过半的那个人。 摩尔投票算法可以快速的计算出一个数组中出现次数过半的数即大多数（majority），算法核心思想是同加，异减。我们举个例子。 假设数组是：[1,2,1,1,2,1]。算法步骤如下： 1。当前大多数是1，得分置1 2。与当前大多数不同，得分 - 1，得分为0，当前大多数 = 1 1。与当前大多数不同，得分为0，所以设置当前大多数 1 -&gt; 1，得分置1 1。与当前大多数相同，得分 + 1，得分为2，当前大多数 = 1 2。与当前大多数不同，得分 - 1 ，得分为1，当前大多数 = 1 1。与当前大多数相同，得分 + 1，得分为2，当前大多数 = 1 这意味着1是这个数组中出现次数过半的数。 可以感受得到，算法会保存一个当前大多数，和得分，当遇到一个数不是当前大多数时，得分会减一，当减到0时，大多数会发生改变，并且重置得分为1。 这里需要区分的是，摩尔算法不能用来得到众数（mode），例如数组：[1,1,1,2,2,3,3,4,4]，摩尔算法得出最后的结果应该是4，但4并不是众数，可是显然4也不是大多数，那是因为，大多数是指出现次数过半的数，而这个数组中没有这样的数，所以摩尔算法是是失效的，对于这种情况采取需要重新投票。 出现次数超过一半的数LeetCode原题：169. Majority Element 这里要求出现次数大于一半，所以直接套用摩尔投票算法即可得到答案。 12345678910111213class Solution(object): def majorityElement(self, nums): """ :type nums: List[int] :rtype: int """ a, ca = None, 0 for n in nums: if a == n : ca += 1 elif ca == 0: a, ca = n, 1 else : ca -= 1 return a 出现次数超过数组1/3长LeetCode原题：229. Majority Element II 还能用摩尔投票法吗？答案当然是要，但是需要变通一下。 需要注意的是出现次数超过1/3数组长的数，也许会有多个，例子如下： [1,1,1,2,2,2,3,3]，数组1/3长=2（向下取整），所以1和2都是符合条件的。 但最多只能是2个，证明如下： 证明假设数组有且只有3个数a,b,c，它出现的次数分别为ca, cb ,cc ，不妨假设 $ ca = cb= ⌊\frac n 3⌋ + 1$，数组长度为$n$ 设 $n = 3a + b$，其中$a、b$均为非负整数，其实这就是余数公式，其中 $b$ 是余数，$ a = ⌊\frac n 3⌋$，那么 $ \frac n 3 = a + \frac b 3$，即 $ \frac n 3 - \frac b 3 = ⌊\frac n 3⌋$ 所以： ① $ca + cb + cc = n$ ② $ca + cb = 2 * (\frac n 3 - \frac b 3) + 2 = \frac {2n} 3 - \frac {2b} 3 + 2$ 由①、②整理得 $ cc = \frac {n + 2b - 6} 3 $ ，做差： ③ $ cc - ⌊\frac n 3⌋ = \frac {n + 2b - 6} 3 - \frac n 3 - \frac b 3 = \frac {b - 6} 3$ 因为$b = n \% 3$，所以 $b - 6 &lt; 0$ 恒成立，所以 $ cc &lt; ⌊\frac n 3⌋$ 可以知道，如果连 $ ca = cb= ⌊\frac n 3⌋ + 1$ 时都找不到一个 $cc$ 使得它大于 $ ⌊\frac n 3⌋$，那么取ca、cb为其他比$ ⌊\frac n 3⌋$ 大的数，更加不可能找到符合条件的$cc$ 。 综上，一个数组中不可能存在2个以上的数它们出现的次数大于$ ⌊\frac n 3⌋$。 回到题目如果我们在使用摩尔算法时，同时记录两个大多数，会怎么样呢？直觉告诉我，这会得到一个大多数，和一个出现次数仅次于大多数的数，但是这两个数不一定会比数组长的1/3大 所以我们得到它们后，还需要检查它们出现的次数是否符合条件。 AC代码： 1234567891011121314151617181920212223class Solution(object): def majorityElement(self, nums): """ :type nums: List[int] :rtype: int """ a, b, ca, cb, ans = None, None, 0, 0, [] for n in nums: if n == a: ca += 1 elif n == b: cb += 1 elif ca == 0: a, ca = n, 1 elif cb == 0: b, cb = n, 1 else: ca, cb = ca - 1, cb - 1 ca, cb = 0, 0 for n in nums: if n == a: ca += 1 elif n == b: cb += 1 if ca &gt; len(nums)/3: ans.append(a) if cb &gt; len(nums)/3: ans.append(b) return ans]]></content>
      <categories>
        <category>编程题</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[House Robber系列]]></title>
    <url>%2F%2Fp%2F26677.html</url>
    <content type="text"><![CDATA[House Robber链接：198. House Robber 题意： ​ 一个专业小偷打算去偷某条街上的房子，每个房子藏有一定量的金钱，不过相邻的房子间装有报警系统，所以如果你偷了两个相邻的房子，报警系统就会被激活，小偷就会被警察带走。现在要求你给小偷安排一个方案，让他能够偷到最多的钱，同时不被警察带走。 ​ 其实就是：给定一个非负数整数数组，求一个子序列a，要求序列中的元素在原数组互不相邻，求使得sum(a)最大的子序列a。 分析： 这个问题具有最优子结构性质，所以我们从头到尾遍历一次数组即可，注意： 每个房子有偷和不偷两个方案 如果偷了房子i，则房子i+1和房子i-1都不能偷 所以转移方程为，dp[i] 表示 房子 0~i 所能偷到的最大价值： dp[i] = max(dp[i - 1] , dp[i - 2] + x[i]) 边界是i - 1 &lt; 0 和 i - 2 &lt; 0 ，此时的dp 值为0 AC代码： 123456789101112class Solution(object): def rob(self, nums): """ :type nums: List[int] :rtype: int """ if len(nums) == 0: return 0 dp = [0] * len(nums) for (i, v) in enumerate(nums): dp[i] = max(dp[i - 2] + v if i - 2 &gt;= 0 else 0, dp[i - 1] if i - 1 &gt;= 0 else 0, v) return max(dp) House Robber II链接：213. House Robber II 题意： ​ 在第一题的基础上，第一间房子和最后一件房子也是连着的，也就是说，房子构成了一个环。 分析： ​ 构成环后，第一个房子和最后一个房子同时偷的情况是不被允许的，所以我们只能取其中一个。 ​ 假设把第一间房子从环上挖掉，就退化到了问题1，求出此时的价值为$V_1$ ​ 假设把最后一间房子从环上挖掉，就也退化到了问题1，求出此时的价值为$V_2$ ​ 那么答案就是两者中最大的那个。 AC代码： 12345678910111213141516171819202122232425262728293031class Solution(object): def rob(self, nums): """ :type nums: List[int] :rtype: int """ n = len(nums) if n == 0: return 0 elif n == 1: return nums[0] no_0, no_n = self.lineRob(nums, 1, n), self.lineRob(nums, 0, n - 1) return max(no_0, no_n) def lineRob(self, nums, left = 0, right = 0): """ :type nums: List[int] :rtype: int """ n = len(nums) if n== 0: return 0 elif n == 1: return nums[0] dp, left, right = [0] * n, max(left, 0), min(right, n) for i in range(left, right): v = nums[i] dp[i] = max(dp[i - 2] + v if i - 2 &gt;= left else 0, dp[i - 1] if i - 1 &gt;= left else 0, v) return max(dp) House Robber III链接：337. House Robber III 题意： ​ 好了，小偷偷完那条街后，来到了一个新地方，他发现这个地方的房子布局是一个二叉树的结构，此时如果父亲和孩子节点同时被偷，报警系统就会被拉响。 分析： ​ 跟第一题思路类似，每个节点有偷和不偷两种情况，从树底往上计算即可得到根节点的所得的最大值，所以我们需要做后序遍历，先计算两个孩子的 dp 值 AC代码： 每一个节点有两个状态，偷和不偷，这两个状态都对应有一个最大值，我用left[0] 表示左孩子不偷时，左孩子的最大值，left[1] 表示左孩子被偷时的最大值。那么对于父亲节点来讲： 如果父亲被偷，则孩子就不能被偷了，此时价值为 left[0] + right[0] + node.val 如果父亲没被偷，则孩子偷不偷无所谓，只要价值最大就行，所以价值为max(left) + max(right) 1234567891011121314151617181920212223242526# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution(object): def rob(self, root): """ :type root: TreeNode :rtype: int """ res = self.tryRob(root) return max(res) def tryRob(self, node): if not isinstance(node, TreeNode): return [0, 0] ans = [0, 0] # max value of (0 means not rob this node, 1 means rob this node) left = self.tryRob(node.left) right = self.tryRob(node.right) ans[0] = max(left) + max(right) ans[1] = left[0] + right[0] + node.val return ans]]></content>
      <categories>
        <category>编程题</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
        <tag>medium</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NLTK之词频]]></title>
    <url>%2F%2Fp%2F45749.html</url>
    <content type="text"><![CDATA[FreqDist类官方文档：nltk.probability.FreqDist 介绍：词的概率分布类，包含一些概率学的方法。 所在文件：probability.py ，大约在63~427行 创建定义：__init__(self, samples=None) 1234567import nltkfrom nltk.book import *fdist = FreqDist(gutenberg.words(&apos;chesterton-thursday.txt&apos;))print fdistoutputs:&lt;FreqDist with 6807 samples and 69213 outcomes&gt; 上面的69213 outcomes是指有69213个词，而6807 samples是指有6807个不重复的词。 通过下述命令可以确认这一点： 123456len(wds)len(set(wds))outputs:692136807 词数量1234fdist.N()outputs:69213 不重复词的数量(样本数量)1234fdist.B()outputs:6807 生成副本123456789101112fdist2 = fdistfdist3 = fdist.copy() #make copyfdist == fdist2fdist == fdist3fdist is fdist2fdist is fdist3outputs:TrueTrueTrueFalse 词频率定义：freq(sample) sample是词，返回值∈[0, 1] 1234fdist.freq(&apos;the&apos;)outputs:0.04754887087685839 可见，the的词频是0.0475，也就是4.75% 词频度（出现次数）1234fdist[&apos;the&apos;]outputs:3291 词和词频的对12345items = fdist.items()print items[:5]outputs:[(u&apos;yellow&apos;, 16), (u&apos;four&apos;, 18), (u&apos;Does&apos;, 6), (u&apos;conjuring&apos;, 2), (u&apos;marching&apos;, 4)] 只出现一次的词会一次性全部返回，下面只展示前5个。 12345hapaxes = fdist.hapaxes()print hapaxes[:5]outputs:[u&apos;disturb&apos;, u&apos;buttonhole&apos;, u&apos;himselves&apos;, u&apos;woods&apos;, u&apos;wavered&apos;] 出现次数最多的词1234fdist.max()outputs:u&apos;,&apos; u是unicode的标志，说明,出现的最多。 这明显不符合预期，因为,不应该被算作一个词，这会干扰我们分析文本，所以在我们创建FreqDist实例的时候，应该要把词集进行过滤，剔除掉标点符号。 词频分布介绍定义：pformat(maxlen=10) 返回一个可以代表本FreqDist类的字符串，其实就是把高频词和频度列出来了。 maxlen 表示打印多少个高频词 1234fdist.pformat()outputs:u&apos;FreqDist(&#123;u\&apos;,\&apos;: 3488, u\&apos;the\&apos;: 3291, u\&apos;.\&apos;: 2717, u\&apos;a\&apos;: 1713, u\&apos;of\&apos;: 1710, u\&apos;and\&apos;: 1568, u\&apos;&quot;\&apos;: 1336, u\&apos;to\&apos;: 1045, u\&apos;in\&apos;: 888, u\&apos;I\&apos;: 885, ...&#125;)&apos; 频率分布图定义：plot(args, \*kwargs) 可以指定的参数有 title 表示图像的标题 cumulative 是否累计频度 如果第一参数是整数的话，它表示展示词的数量 1fdist.plot(50) 这样看起来是不是别扭呢，因为曲线肯定会下降（频度越高的词越显示在左边） 所以有时候我们需要看这些高频词加起来占了多少，所以： 1fdist.plot(50, cumulative=True) 可以发现，曲线越到后面越平缓。 频率分布表12345fdist.tabulate(10)outputs: , the . a of and3488 3291 2717 1713 1710 1568 累计表： 12345fdist.tabulate(10,cumulative=True)outputs: , the . a of and 3488 6779 9496 11209 12919 14487 FreqDist的应用前面已经介绍了Text，结合本次的FreqDist类，我们已经可以做一些基础的NLP研究了。 理解文本的主题或风格高频词 text1 变量保存的是《白鲸记》Text类实例，高频词是否可以帮助我们理解呢？ 打印出它前50个词的频率分布图可以发现： 12fdist1 = text1.vocab()fdist1.plot(50, cumulative=True) 相比the、of 这类助词，whale的频率相对靠后，而这个词恰恰是《白鲸记》中比较重要的词，可以体现出文本的主题。 低频词既然高频率词没啥帮助，那只出现一次的词呢？ 1234len(fdist1.hapaxes())outputs:9002 emmm，貌似低频词太多了，如果不看上下文，我们也很难借助低频词来理解文本主题。 细粒度的选择词或许长词包含的信息比较多，是否有帮助呢？ 我们考虑以下哪些成都大于15的词： 123456789101112samples = set(text1)print [w for w in samples if len(w) &gt; 15]outputs:[u&apos;hermaphroditical&apos;, u&apos;subterraneousness&apos;, u&apos;uninterpenetratingly&apos;, u&apos;irresistibleness&apos;, u&apos;responsibilities&apos;, u&apos;comprehensiveness&apos;, u&apos;uncompromisedness&apos;, u&apos;superstitiousness&apos;, u&apos;uncomfortableness&apos;, u&apos;supernaturalness&apos;, u&apos;circumnavigating&apos;, u&apos;cannibalistically&apos;, u&apos;circumnavigations&apos;, u&apos;indispensableness&apos;, u&apos;preternaturalness&apos;, u&apos;apprehensiveness&apos;, u&apos;CIRCUMNAVIGATION&apos;, u&apos;simultaneousness&apos;, u&apos;undiscriminating&apos;, u&apos;characteristically&apos;, u&apos;Physiognomically&apos;, u&apos;physiognomically&apos;, u&apos;circumnavigation&apos;, u&apos;indiscriminately&apos;] 似乎比低频词的数量要少很多了，可以发现的是，长词 大多也是 低频词，所以从这个角度来说我们忽略了短高频词和大部分低频词，也许寻找那些长高频词会比较有帮助。 长高频词下面将选择出那些长度大于7，且出现的频度也大于7的词： 1234print [w for w in samples if len(w) &gt; 7 and fdist1[w] &gt; 7]outputs:[u&apos;uncertain&apos;, u&apos;bringing&apos;, u&apos;substance&apos;, u&apos;cannibal&apos;, u&apos;therefore&apos;, u&apos;violently&apos;, u&apos;whalebone&apos; ...] 其实得到的结果数量还是很多，但我们已经看到了whalebone 这个词，可以大致确定下来《白鲸记》的主题了，所以寻找长高频词可以有效的帮助我们理解文本的主题。 搭配词文中出现的搭配词能非常明确的体现文本的类型。 下面寻找搭配词： 1234567text1.collocations()outputs:Sperm Whale; Moby Dick; White Whale; old man; Captain Ahab; spermwhale; Right Whale; Captain Peleg; New Bedford; Cape Horn; cried Ahab;years ago; lower jaw; never mind; Father Mapple; cried Stubb; chiefmate; white whale; ivory leg; one hand 可以看到第一个词就是Sperm Whale 。 计算其他东西词长分布也就是计算每个长度的词出现的次数 123456lens = [len(w) for w in text1]fdistw = FreqDist(lens)fdistw.keys()outputs:[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 20] 可以看到text1中的词，最短长度是1，最长长度是20 1fdistw.plot() 可以发现长度大于8和之后的的词出现的频度已经很小了 1234fdistw.max()outputs:3 可以发现，最频繁出现的词长度是3。 ConditionalFreqDist类官方文档：nltk.probability.ConditionalFreqDist 介绍：条件频率分布类 所在文件：probablity.py，大约在1734~2003 创建定义：__init__(cond_samples=None): 123456789import nltkfrom nltk.book import *from nltk.probability import ConditionalFreqDistcond_samples = [(len(w), w) for w in text1]cfdist = ConditionalFreqDist(cond_samples)print cfdistoutputs:&lt;ConditionalFreqDist with 19 conditions&gt; cond_samples参数是元组集合，元组是(条件, 词)，上述代码的条件是词长，下面看看效果你就懂了： 1234cfdist[3]outputs:FreqDist(&#123;u&apos;the&apos;: 13721, u&apos;and&apos;: 6024, u&apos;his&apos;: 2459, u&apos;was&apos;: 1632, u&apos;all&apos;: 1462, u&apos;for&apos;: 1414, u&apos;but&apos;: 1113, u&apos;not&apos;: 1103, u&apos;him&apos;: 1058, u&apos;one&apos;: 889, ...&#125;) 这列出了所有长度为3的词，这意味着元组的第一个元素条件被作为了cfdist的键，而它值是一个FreqDist对象。 查看所有条件1234cfdist.conditions()outputs:[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 20] 条件频率1234cfdist[3].freq(&apos;but&apos;)outputs:0.02216116122095454 在长度为3的条件下，but出现的频率是0.02216 条件频率分布图定义：plot(args, \*kwargs) 可以指定的参数有 samples list 要显示的样本集合 title str 标题 conditions list 要显示的条件，默认是全部 为了便于理解，下面把cfdist换成以文本为条件： 1234567891011from nltk.corpus import browncond_samples = [ (genre, word) for genre in [&apos;news&apos;, &apos;romance&apos;] for word in brown.words(categories=genre)]cfdist = ConditionalFreqDist(cond_samples)print cfdistoutputs:&lt;ConditionalFreqDist with 2 conditions&gt; 这里我们取了两个文体：news 和 romance，看看两者中情态动词的频率分布吧： 1cfdist.plot(samples=[&apos;would&apos;, &apos;will&apos;, &apos;could&apos;, &apos;can&apos;, &apos;might&apos;, &apos;may&apos;, &apos;should&apos;]) 哈哈，大致符合预期呢，新闻中的will频率异常的高。 条件频率分布表定义：tabulate(args, \*kwargs) 可以指定的参数有 samples list 要显示的样本集合 title str 标题 conditions list 要显示的条件，默认是全部 123456cfdist.tabulate(samples=[&apos;would&apos;, &apos;will&apos;, &apos;could&apos;, &apos;can&apos;, &apos;might&apos;, &apos;may&apos;, &apos;should&apos;])outputs: would will could can might may should news 244 389 86 93 38 66 59romance 244 43 193 74 51 11 32 ConditionalFreqDist的应用用词分析之前对比了news、romance两个文体中，情态动词的频率分布，发现了news 的will词用的频率很高。 生成随机文本如果知道了一个词如living，也知道了living这个词的条件频率分布，那么可以找到最有可能出现在living后面的那个词，这样迭代的进行计算，可以获得一个随机文本。根据上述方法，可以定义一个函数用来生成随机文本： 1234def random_text(cfdist, word, num=15): for i in range(num): print word, word = cfdist[word].max() 我们导入《创世纪》文本： 12from nltk.corpus import genesistext = genesis.words(&apos;english-kjv.txt&apos;) 为创建条件概率分布类，我们需要cond_samples，所以我们可以构造一些双连词，bigrams 可以从数组 生成 双连词： 1234567from nltk import bigramscond_samples = bigrams(text)cfdist = ConditionalFreqDist(cond_samples)random_text(cfdist, &apos;living&apos;)outputs:living creature that he said , and the land of the land of the land 可以发现。。效果不怎么好，因为后面无限循环了。。 我们试试其他词： 1234random_text(cfdist, &apos;I&apos;)outputs:I will not be a son , and the land of the land of the 哈哈，最后还是无限循环了，不过挺有趣的！]]></content>
      <categories>
        <category>研</category>
      </categories>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NLTK常用操作和语料库]]></title>
    <url>%2F%2Fp%2F22281.html</url>
    <content type="text"><![CDATA[安装NLTK按照官方步骤：Installing NLTK 在安装完nltk后，通过下述命令可查看nltk版本： 12import nltkprint nltk.__doc__ 输出： 123456789The Natural Language Toolkit (NLTK) is an open source Python libraryfor Natural Language Processing. A free online book is available.(If you use the library for academic research, please cite the book.)Steven Bird, Ewan Klein, and Edward Loper (2009).Natural Language Processing with Python. O&apos;Reilly Media Inc.http://nltk.org/book@version: 3.2.5 所以，我的版本是 3.2.5 获取图书集合为便于学习，nltk提供了图书集合，通过下述命令下载该集合： 1nltk.download() 在出现的界面中选择 book 进行下载，文件大概有100M，这也是《Python自然语言处理》这本书中推荐的图书集合。 下载完成后，回到python解析器，导入刚刚下载的图书集合： 1from nltk.book import * 运行 texts() 可以查看书籍列表，sents() 可以查看句子列表。 查看 book.py 文件可知，该文件导入了下面一些模块 nltk.corpus 中的一些语料库 nltk.text 中的Text类 nltk.probability 中的 FreqDist 类 nltk.util 中的 bigrams 模块 同时通过语料库创建9个 Text 实例，变量为text1~text9。 而 sent1~sent9 是硬编码的9个句子。 语料库介绍在book.py 文件中，我们可以看到： 12from nltk.corpus import (gutenberg, genesis, inaugural, nps_chat, webtext, treebank, wordnet) 所以我们已经引入了语料库了，通过打印可知： 1234print gutenbergoutputs:&lt;PlaintextCorpusReader in u&apos;/Users/jerry/nltk_data/corpora/gutenberg&apos;&gt; 可见 gutenberg 是一个 PlaintextCorpusReader 类。 进入到指定的目录可见gutenberg 的所有文本，可见都是txt文档 1234567891011cd /Users/jerry/nltk_data/corpora/gutenberglsoutputs:README burgess-busterbrown.txt milton-paradise.txtausten-emma.txt carroll-alice.txt shakespeare-caesar.txtausten-persuasion.txt chesterton-ball.txt shakespeare-hamlet.txtausten-sense.txt chesterton-brown.txt shakespeare-macbeth.txtbible-kjv.txt chesterton-thursday.txt whitman-leaves.txtblake-poems.txt edgeworth-parents.txtbryant-stories.txt melville-moby_dick.txt 古腾堡语料库古腾堡项目 大约有36000本免费电子图书，NLTK中只包含了其中的一小部分，通过下述命令可以查看语料库中的文件标识符（文件名）： 12345from nltk.corpus import gutenberggutenberg.fileids()outputs:[u&apos;austen-emma.txt&apos;, u&apos;austen-persuasion.txt&apos;, u&apos;austen-sense.txt&apos;, u&apos;bible-kjv.txt&apos;, u&apos;blake-poems.txt&apos;, u&apos;bryant-stories.txt&apos;, u&apos;burgess-busterbrown.txt&apos;, u&apos;carroll-alice.txt&apos;, u&apos;chesterton-ball.txt&apos;, u&apos;chesterton-brown.txt&apos;, u&apos;chesterton-thursday.txt&apos;, u&apos;edgeworth-parents.txt&apos;, u&apos;melville-moby_dick.txt&apos;, u&apos;milton-paradise.txt&apos;, u&apos;shakespeare-caesar.txt&apos;, u&apos;shakespeare-hamlet.txt&apos;, u&apos;shakespeare-macbeth.txt&apos;, u&apos;whitman-leaves.txt&apos;] 网络和聊天文本古腾堡项目代表的既定文学，考虑到非正式的文本也很重要，所以NLTK中也包含网络小说和论坛、《加勒比海盗》的电影剧本等内容。 12345from nltk.corpus import webtextwebtext.fileids()outputs:[u&apos;firefox.txt&apos;, u&apos;grail.txt&apos;, u&apos;overheard.txt&apos;, u&apos;pirates.txt&apos;, u&apos;singles.txt&apos;, u&apos;wine.txt&apos;] 即时聊天会话语料库，是美国海军研究生院的研究生收集的。 12345from nltk.corpus import nps_chatnps_chat.fileids()outputs:[u&apos;10-19-20s_706posts.xml&apos;, u&apos;10-19-30s_705posts.xml&apos;, u&apos;10-19-40s_686posts.xml&apos;, u&apos;10-19-adults_706posts.xml&apos;, u&apos;10-24-40s_706posts.xml&apos;, u&apos;10-26-teens_706posts.xml&apos;, u&apos;11-06-adults_706posts.xml&apos;, u&apos;11-08-20s_705posts.xml&apos;, u&apos;11-08-40s_706posts.xml&apos;, u&apos;11-08-adults_705posts.xml&apos;, u&apos;11-08-teens_706posts.xml&apos;, u&apos;11-09-20s_706posts.xml&apos;, u&apos;11-09-40s_706posts.xml&apos;, u&apos;11-09-adults_706posts.xml&apos;, u&apos;11-09-teens_706posts.xml&apos;] 布朗语料库布朗语料库是第一个百万词级别的英语电子语料库，这个语料库包含500个不同来源的文本，按文体分类有新闻、社论等，完整列表。 12345from nltk.corpus import brownbrown.categories()outputs:[u&apos;adventure&apos;, u&apos;belles_lettres&apos;, u&apos;editorial&apos;, u&apos;fiction&apos;, u&apos;government&apos;, u&apos;hobbies&apos;, u&apos;humor&apos;, u&apos;learned&apos;, u&apos;lore&apos;, u&apos;mystery&apos;, u&apos;news&apos;, u&apos;religion&apos;, u&apos;reviews&apos;, u&apos;romance&apos;, u&apos;science_fiction&apos;] 这里注意的是它是按文体分类的，例如要取得news类别的文本： 1234print brown.words(categories=&apos;news&apos;)outputs:[u&apos;The&apos;, u&apos;Fulton&apos;, u&apos;County&apos;, u&apos;Grand&apos;, u&apos;Jury&apos;, ...] 当前也可以用fileids 方法列出所有文件标识，再用文件标识来获取文本，但最好还是用类别来取得文本。 自定义语料库例如我们在家目录建立一个texts 目录和一个文件： 123456cd ~ &amp;&amp; mkdir texts &amp;&amp; cd textsecho &quot;This is a love story&quot; &gt;&gt; lovestory.txtpwdoutputs:/Users/jerry/texts 将上面的output放在下面的dir变量中，在python解析器中输入： 1234567from nltk.corpus import PlaintextCorpusReaderdir = &apos;/Users/jerry/texts&apos;my_corpus = PlaintextCorpusReader(dir, &apos;.*\.txt&apos;)print my_corpus.fileids()outputs:[&apos;lovestory.txt&apos;] 断词： 1234print my_corpus.words(&apos;lovestory.txt&apos;)outputs:[u&apos;This&apos;, u&apos;is&apos;, u&apos;a&apos;, u&apos;love&apos;, u&apos;story&apos;] PlaintextCorpusReader类官方文档：nltk.corpus.reader.plaintext.PlaintextCorpusReader 介绍：基于文件系统，用来加载纯文本的类 所在文件：corpus/reader/plaintext.py，大约在23~138行 断词定义：words(fileids=None) 我们可以使用words函数将这些文件分割成tokens(词，此处也叫words)： 12345words = gutenberg.words(&apos;shakespeare-macbeth.txt&apos;)print wordsoutputs:[u&apos;[&apos;, u&apos;The&apos;, u&apos;Tragedie&apos;, u&apos;of&apos;, u&apos;Macbeth&apos;, u&apos;by&apos;, ...] 标点符号也会被单独分出来 断句定义：sents(fileids=None) 12345sents = gutenberg.sents(&apos;shakespeare-macbeth.txt&apos;)print sentsoutputs:[[u&apos;[&apos;, u&apos;The&apos;, u&apos;Tragedie&apos;, u&apos;of&apos;, u&apos;Macbeth&apos;, u&apos;by&apos;, u&apos;William&apos;, u&apos;Shakespeare&apos;, u&apos;1603&apos;, u&apos;]&apos;], [u&apos;Actus&apos;, u&apos;Primus&apos;, u&apos;.&apos;], ...] 可以发现每一句都是一个字符串数组，连起来就是一个句子： 1234print &apos; &apos;.join(sents[0])outputs:[ The Tragedie of Macbeth by William Shakespeare 1603 ] 分段定义：paras(fileids=None) 段是指自然段，一个空白行表示自然段之间的分隔符 12345paras = gutenberg.paras(&apos;shakespeare-macbeth.txt&apos;)print &apos; &apos;.join(paras[0][0]) #first para, first sentenceoutputs:[ The Tragedie of Macbeth by William Shakespeare 1603 ] 加载原始文本定义：raw(fileids=None) 12345678910str = gutenberg.raw(&apos;shakespeare-macbeth.txt&apos;)print str[:100]outputs:[The Tragedie of Macbeth by William Shakespeare 1603]Actus Primus. Scoena Prima.Thunder and Lig Text类官方文档：nltk.text.Text 介绍：token(词)序列的容器，同时提供一些常用的文本分析函数。 所在文件：text.py，大约在264行~395行 创建定义：__init__(self, tokens, name=None) text1 变量其实是通过下述语句创建的： 12345text1 = Text(gutenberg.words(&apos;melville-moby_dick.txt&apos;))print text1outputs:&lt;Text: Moby Dick by Herman Melville 1851&gt; 通过打印words方法的结果可知： 1234print gutenberg.words(&apos;melville-moby_dick.txt&apos;)outputs:[u&apos;[&apos;, u&apos;Moby&apos;, u&apos;Dick&apos;, u&apos;by&apos;, u&apos;Herman&apos;, u&apos;Melville&apos;, ...] 发现gutenberg 的 words 方法返回的其实是一个字符串数组 。 查找单词定义：concordance(word, width=79, lines=25) 参阅： ConcordanceIndex 类 123456789text1.concordance(&apos;monstrous&apos;, 30, 5)outputs:Displaying 5 of 11 matches:of a most monstrous size . ..hing that monstrous bulk of t array of monstrous clubs andered what monstrous cannibalod ; most monstrous and most 这里我指定了只显示5行，每一行宽度是30。 细心的你可能发现每行只有29个字符，详细原因看text.py文件的print_concordance 方法即可。 搭配词定义：collocations(num=20, window_size=2) 1234567text1.collocations()outputs:Sperm Whale; Moby Dick; White Whale; old man; Captain Ahab; spermwhale; Right Whale; Captain Peleg; New Bedford; Cape Horn; cried Ahab;years ago; lower jaw; never mind; Father Mapple; cried Stubb; chiefmate; white whale; ivory leg; one hand 可见，该函数是打印出文中由多个词组成的搭配词，如上文的old man、years ago。 相似词定义：similar(word, num=20) 参阅：ContextIndex.similar_word 方法 1234text1.similar(&apos;monstrous&apos;, 5)outputs:imperial subtly impalpable pitiable curious 该方法根据单词在上下文中的用法，返回与给定词相似的词，相似度越高的词排名越靠前。 num 参数表示查找数量，例如上面就只显示了5个。 相同上下文定义：common_contexts(words, num=20) 参阅：ContextIndex.common_contexts 方法 1234text2.common_contexts([&apos;monstrous&apos;, &apos;very&apos;])outputs:a_pretty is_pretty a_lucky am_glad be_glad 给出两个词的共同的上下文。这意味着下划线部分可用monstrous 或 very进行填空，而且a_pretty 这个公共上下文出现的频率最大。 词分布定义：dispersion_plot(words) 1text4.dispersion_plot([&apos;citizens&apos;, &apos;democracy&apos;, &apos;freedom&apos;, &apos;duties&apos;, &apos;America&apos;]) text4 是美国总统就职演说的文本，上述图像打印出了这五个词在演说过程中的分布情况。 这可以用来研究随时间推移，语言使用上的变化。 正则表达式匹配单词定义：findall(regexp) 1234text5.findall(&quot;&lt;.*&gt;&lt;.*&gt;&lt;bro&gt;&quot;)outputs:you rule bro; telling you bro; u twizted bro 词出现次数定义：count(word) 1234text1.count(&apos;I&apos;)outputs:2124 词索引定义：index(word) 1234text1.index(&apos;I&apos;)outputs:37 文本总长度1234len(text1)outputs:260819 生成FreqDist类12345dist = text1.vocab()print distoutputs:&lt;FreqDist with 19317 samples and 260819 outcomes&gt; 打印频率分布图定义：plot(*args) 参阅：nltk.prob.FreqDist.plot 方法 从定义可以看出，其实就是调用了FreqDist的plot方法 123456def plot(self, *args): &quot;&quot;&quot; See documentation for FreqDist.plot() :seealso: nltk.prob.FreqDist.plot() &quot;&quot;&quot; self.vocab().plot(*args)]]></content>
      <categories>
        <category>研</category>
      </categories>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker模拟多机环境]]></title>
    <url>%2F%2Fp%2F47694.html</url>
    <content type="text"><![CDATA[Docker基础这里不多说，借助docker官方文档可以学习到大部分内容，而且很详细。 在线阅读：GitBook 或 Github。 离线阅读。 pdf 版本 下载 epub 版本 下载 本次配置中用到的镜像： nginx:1.12-alpine，Dockerfile php:7.1-fpm-alpine，Dockerfile mysql/mysql-server:5.7.20，Dockerfile 并且使用docker-compose来管理容器，它在pdf文档中有介绍，这里不再赘述。 多机结构为了实现一台真机模拟多台机，引入逻辑主机的概念。例如现在只有一台云主机，但是我们又要把测试环境(dev) 和 正式环境(online)分开。 以下是项目完成后的目录结构，每一个目录对应一个逻辑主机 和独立数据卷 1234567891011.├── dev│ ├── docker-compose.yml│ └── php│ └── Dockerfile├── online│ ├── docker-compose.yml│ └── php│ └── Dockerfile└── _self └── docker-compose.yml 这里需要做以下说明： 测试环境(dev) 和 正式环境(online) 除了数据卷 不一样，其他配置都一样 _self 目录表示宿主主机，上面有nginx dev 配置dev 环境下有php和mysql，两个app都在独立的容器中，两者通过 sock文件 或网络进行通信，以下是dev环境的docker-compose.yml 文件内容： 12345678910111213141516171819202122232425php1: #build: ./php #image: php:7.1-fpm-alpine image: php-fpm-composer:1.0 container_name: dev_php1 expose: - "9000" volumes: - "/data-dev/data1/:/data1/" - "/data-dev/data2/:/data2/" - "/data-dev/data2/php/conf/:/usr/local/etc/" links: - mysql1:server_mysqlmysql1: image: mysql/mysql-server:5.7.20 container_name: dev_mysql1 expose: - "11308" volumes: - "/data-dev/data1/:/data1/" - "/data-dev/data2/:/data2/" - "/data-dev/data2/mysql/conf/:/etc/mysql/" environment: - MYSQL_ROOT_PASSWORD=123456789 volumes先看数据卷。以 dev_php1 容器为例，我们可以看到 宿主 的/data-dev/data1 和 /data-dev/data2 被映射到了php容器的/data1 和 /data2 目录。是的，宿主的/data-dev 目录就是专门为dev 环境划分的逻辑目录。 再看dev_mysql1 容器，道理和上述一样。 端口配置先看dev_php1，根据 php-fpm 配置文件中监听了9000 端口，所以开放容器的9000端口。 mysql 配置文件中设置的是11308 端口，所以我们开放容器的11308 端口。 其实在实际使用中，两者也都支持 sock文件方式进行连接。 自定义的php镜像由于官方的php缺少一些扩展，所以我基于该镜像添加了扩展和compose，Dockerfile如下： 12345678FROM php:7.1-fpm-alpineRUN apk add --no-cache \ libpng-dev \ &amp;&amp; docker-php-ext-install pdo_mysql gd zip \ &amp;&amp; php -r "copy('https://getcomposer.org/installer', 'composer-setup.php');" \ &amp;&amp; php -r "if (hash_file('SHA384', 'composer-setup.php') === '544e09ee996cdf60ece3804abc52599c22b1f40f4323403c44d44fdfdd586475ca9813a858088ffbc1f233e9b180f061') &#123; echo 'Installer verified'; &#125; else &#123; echo 'Installer corrupt'; unlink('composer-setup.php'); &#125; echo PHP_EOL;" \ &amp;&amp; php composer-setup.php --install-dir=/usr/local/bin --filename=composer \ &amp;&amp; php -r "unlink('composer-setup.php');" 为了避免构建多层镜像又保证命令的整洁性，使用 \ (表示换行)把要执行的命令分开。 可以看到我这里添加了pdo_mysql 、gd 、zip 扩展，其中zip 镜像是安装compose必要的，且为了安装gd 扩展，系统必须要先安装libpng-dev 库。 后面四行命令是安装composer的，安装后放在了/usr/local/bin 目录下，文件名是composer。 根据这个Dockerfile构建了名为php-fpm-composer:1.0 的镜像。 其他配置dev_mysql1 的 MYSQL_ROOT_PASSWORD 表示数据库初始密码，如果数据库初始化过了（数据卷中已有数据）则没效了。 online配置online 环境的docker-compose.yml 文件内容如下： 12345678910111213141516171819202122232425php1: #build: ./php #image: php:7.1-fpm-alpine image: php-fpm-composer:1.0 container_name: online_php1 expose: - "9000" volumes: - "/data-online/data1/:/data1/" - "/data-online/data2/:/data2/" - "/data-online/data2/php/conf/:/usr/local/etc/" links: - mysql1:server_mysqlmysql1: image: mysql/mysql-server:5.7.20 container_name: online_mysql1 expose: - "11308" volumes: - "/data-online/data1/:/data1/" - "/data-online/data2/:/data2/" - "/data-online/data2/mysql/conf/:/etc/mysql/" environment: - MYSQL_ROOT_PASSWORD=123456789 部署步骤可以看到online 环境除了逻辑目录 是/data-onlie 以外，其他的配置都和dev 一样，当然容器名是不一样的。 如果我们已经成功部署了dev 环境，那么只需把/data-dev copy一份为 /data-online 即可完成数据卷的部署，保证online的数据和dev是一样的（主要是php、mysql等app的配置一样）。 nginx的配置本项目采取的方式是nginx 单例，所以nginx必须放在宿主主机上，稍后讨论多例nginx的配置。 目前已经有了dev 和 online 两个环境，且分别运行了php和m ysql，单例nginx模式的架构如下： 圆圈 表示逻辑主机， 矩形框 内是英文的表示容器，英文表示容器名称。 宿主环境 （nginx）的docker-compose.yml 文件内容如下： 12345678910111213141516nginx1: image: nginx:1.12-alpine container_name: nginx1 ports: - &quot;80:80&quot; - &quot;443:443&quot; expose: - &apos;80&apos; - &apos;443&apos; volumes: - &quot;/data/:/data/&quot; - &quot;/data1/:/data1/&quot; - &quot;/data2/:/data2/&quot; - &quot;/data2/nginx/conf/:/etc/nginx/&quot; - &quot;/data-dev/:/data-dev/&quot; - &quot;/data-online/:/data-online/&quot; 可以看到，为了dev、online 提供服务，需要将两个逻辑主机的数据目录 映射到nginx容器中去，从而达到nginx与宿主主机的文件系统是一样的。 文件路径在进行nginx配置的时候，需要非常注意的是文件路径。 在dev 逻辑主机中，网站根目录是/data1/sites/html，这个目录对应于宿主主机的/data-dev/data1/sites/html ，那么在nginx中配置的时候，就需要特别注意。 下面是nginx 为dev 环境提供服务所写的配置信息： 123456789101112131415161718... listen 80; server_name dev.domain.com; root /data-dev/data1/sites/html/; index index.html index.htm index.php; access_log /data2/nginx/logs/dev/html_access.log main; error_log /data2/nginx/logs/dev/html_error.log warn; # use php-fpm.sock location ~ \.php$ &#123; fastcgi_pass unix:/data-dev/data2/socks/php-fpm.sock; fastcgi_index index.php; fastcgi_param SCRIPT_FILENAME /data1/sites/html$fastcgi_script_name; include fastcgi_params; &#125;... 由于nginx运行在宿主主机，所以使用的是宿主主机的文件系统：（下面的/data2 是宿主主机上的） 123root /data-dev/data1/sites/html/;access_log /data2/nginx/logs/dev/html_access.log main;error_log /data2/nginx/logs/dev/html_error.log warn; 根据web请求流程和CGI原理，所以fastcgi在寻找文件时使用的是 dev 主机的文件系统： 1fastcgi_param SCRIPT_FILENAME /data1/sites/html$fastcgi_script_name; 注意：不能写成\$document_root\$fastcgi_script_name，因为$document_root是宿主主机的文件系统 静态文件测试根据上述配置文件，配置好root 路径即可进行静态文件测试。 现有两个文件，他们在宿主主机 文件系统中的路径如下： 12/data-dev/data1/sites/html/index.html/data-online/data1/sites/html/index.html 可以看到，一个在 dev 中，一个在 online 环境中 那么在dev 的文件系统中，则是： 1/data1/sites/html/index.html 文件内容是： 1This is dev 在online 的文件系统中，则是： 1/data1/sites/html/index.html 文件内容是： 1This is online 可以发现，在两个逻辑主机中，他们并不知道宿主主机的存在，也就是宿主主机对于他们来说是透明的。 /etc/hosts配置如下，同步修改nginx的server_name： 12127.0.0.1 dev.domain.com127.0.0.1 online.domain.com 进行测试： 12curl dev.domain.comcurl online.domain.com 输出： 12This is devThis is online php测试php的测试主要看nginx是否正确使用了对应环境的php-fpm.sock，如果你是用9000端口通信的，那就看ip是否填对了，方式与静态页面类似，不再赘述。 总结详情：https://github.com/JerryCheese/docker-architecture]]></content>
      <categories>
        <category>软件技术</category>
      </categories>
      <tags>
        <tag>devopps</tag>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Week11:Photo OCR & Conclusion]]></title>
    <url>%2F%2Fp%2F14526.html</url>
    <content type="text"><![CDATA[OCR问题通俗一点讲就是图像识别中的文字识别。比方说给一张图，请识别图中的文字。 滑动窗口探测法该方法用于识别出图中哪些区域还有所指定的目标物体。 它选一个特定的尺寸的图片进行二分训练，判断这个尺寸的图片中是否含有要找的目标。 例如行人探测 问题，需要找出图片中哪有人，那么可以先选取一个 $82 \times 36$ 大小的小图片作为训练集进行训练。 得到训练模型后，使用一个滑动窗口 在原图上进行搜寻，每次移动的距离叫做step ，step 越小，效果越好。 第一步：文字检测要识别图像中的字，得先知道哪些地方可能是字，所以文字检测是用来探测图像中的哪一部分是含有字的。 与行人检测一样，我们需要先建立起训练集 在原图上我们运行滑动窗口算法，得到左下方的图。白点 越亮则表示该处有文字的概率越大。根据文字的特征，一般文字都是横着的，而且是连着的，所以我们可以通过白点扩散得到右下方的结果，这样就更加接近真实的结果了。 第二步：断字例如现在已经截取到了含有字的那一块图片，那么就要对其进行断句（分词）例如： 这个问题用滑动窗口法也可，对于每一小块，我们取图片的对称轴即可以找到字母的间隔，就可以把文字断开了。 第三步：识字这一步需要识别每一个独立的语言符号，对英语来说，就是识别出每一个字母： 这是一个典型的多分类问题。 合成数据获得真实数据是一件很麻烦的事情，需要消耗人力物力，而文字识别这件事儿其实人工合成的数据也能达到训练效果，例如下面两幅图，人也很难区分哪一个是合成的。 获取大量数据：扭曲、加噪扭曲是模拟真实数据的一个好方法，例如现在有一个很标准的数字A。 同时在语音数据上，你或许已经有一个比较标准的数据，声音很清晰，那你可以试着对这断语音加入一些白噪声，看机器学习模型是否还能正确的识别它。 流水线学习从上述OCR问题的解决方案来看，解决OCR问题可以分成三个步骤，而这三个步骤都分别对应一个机器学习问题。 优化分析那么有什么指标可以衡量一个OCR系统的好坏呢？答案是准确率，那么要是准确率很低，又该怎么优化呢。 因为OCR可以被拆成三个机器学习问题，所以我们可以把它们看出三个组件，通过调整每一个组件的准确率，我们可以提升系统整体的准确率。那么我们应该调整三个组件中的哪一个组件呢？ 我们可以画出以下表格，第一行表示当前系统整体的准确率是72%，假设现在我们要来优化这个系统。 我们可以人工的把第一个组件文字检测的准确率提高到100%，我的意思就是人工纠正那些被模型错误分类的数据，之后我们再次运行系统，得到的系统准确率是89%。 接着我们把断字 组件的准确率也提高到100%，得到的系统准确率是90%。 再把识字 组件准确率也提高到100%，那么最后的系统准确率肯定就是100%了。 好了，现在可以看到，通过提升文字检测组件，我们可以把系统的整体准确率提高17%，，断字、识字 可以提高1%和10%，可以知道，文字检测的贡献率是三个组件中最大的，所以我们可以把大多数优化的精力用在文字检测这个组件上。 总结很感谢Coursera提供的这样一个平台，让百万莘莘学子可以接触到这么多优质的资源，同时也很感谢Ng的这套机器学习课程，帮助大家很快速地踏入了机器学习的大门，教育无疆。 课程虽然结束，而我的言机器学习之旅也才刚刚开始，接下来的两个月我将花费大量时间研究NLP领域，同时也希望自己能够坚持自己的选择，能够披荆斩棘走向梦想的彼岸。]]></content>
      <categories>
        <category>研</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Week10:Learning with large datasets]]></title>
    <url>%2F%2Fp%2F16629.html</url>
    <content type="text"><![CDATA[大规模机器学习大规模是指训练集数据很多，比如有1亿组数据。 大数据有一句话叫做“最后胜利的人不是因为他有好的算法，而是他用有大量数据”，因为随着数据量的增加，机器学习算法的准确度都会变得很高。 批量梯度下降先看梯度下降方程，以线性回归为例： 这种方式称之为批量梯度下降，一个 $\theta_j$ 都需要遍历所有数据，如果m = 100,000,000 ，梯度下降会变得很慢。 ### 随机梯度下降随机不是指随机的选取一些数据来进行梯度下降。 随机下降的Cost Function，只计算一组数据，而不是全部数据： 而在进行梯度下降时，遍历所有数据 $x^{(i)}$ 计算 $\theta$： 幸运的是，通常并不需要遍历全部数据或是遍历一次即可得到比较理想的 $\theta$ 值。 其实这种方式是在逐渐适应遇见的$x^{(i)}$，所以并不能保证$\theta$ 是一直下降的，如果把每次计算的cost画在坐标系中，它会是摆动的： 而且最后会在最优值附加徘徊，但得不到最优值。 最小批量梯度下降获随即下降的启发，我们可以梯度下降时取 $b$ 组数据而不是一个数据，这样可以获得比随机下降更加快的速度，它的过程如下： 在线学习在线学习是指使用源源不断获得的数据进行学习。比如你正在经营一个网上商店，那么你可以持续获得用户操作记录，所以训练集的大小是不固定的，而是每天增长的。随机梯度下降很适合这个场景。 映射约减和并行计算映射约减又叫Map-reduce。假设你现在有5台机器，有4亿的数据量，那么可以把数据拆成四等分，分给4台机器运算，计算结果交个第5台机器，由它来进行汇总。 这个道理也适用于多核cpu： 现在这种方法集成到了一些框架中，例如Hadoop]]></content>
      <categories>
        <category>研</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Week9:Anomaly Detection & Recommender Systems]]></title>
    <url>%2F%2Fp%2F30591.html</url>
    <content type="text"><![CDATA[序Coursera的课程还有2周就要结束了，我这周基本上是每天学一周的内容，而下周就要开始正式接触NLP了。 身边也有一起学习Ng这套课程的人，但是学到一半没到就放弃了，我在学习过程中也问过自己，Ng这套11年的课程放在现在还有价值么？这是多虑的表现。到了Week9我慢慢感觉到了这套课程的魅力所在，首先它可以在很短时间内让你了解机器学习这门学科，我大概也就用了一个月吧；其次它总能用一些简单的例子来让你快速理解问题所在，我觉得机器学习是偏应用。 异常检测这是一个偏斜问题，传统行业像零件生产商，他们在出货前都会做这个工作，也许10000个成品里只有40个是不合格的。假如一家引擎生产商，正在做异常检测的工作，他们规定了两个重要指标（两个特征）来衡量产品的好坏，画在坐标系中可以很清楚的发现所有的点都集中在一个区域，只有一少部分数据会落在边缘地带。 对数学笔记敏感的同学也许很快反应过来，这有点像正态分布，是的。 高斯分布又名正态分布，它的方程长这个样子： 如果变量x服从正态分布，则可以记作 exp就是e为底的对数 $\mu$ 是平均数，又叫做位置参数 $\sigma $ 是标准差，所以 $\sigma^2$ 是方差 画成图就是这样： $\mu$ 和 $\sigma$ 对图像的影响是： $\mu$ 是对称轴 $\sigma$ 大则矮胖；小则廋高 图像的面积为概率，例如$ 8 &lt;= x &lt;= 10$ 的概率，就是正态函数在$x ∈ [8, 10]$区间上的积分（面积） 。 所以可以认为，如果数据x越靠近中心，那么它为正品的概率$p(x)$越高，反之如果x落在了边缘，那么它很可能是异常数据，为了更加精确的描述异常数据，我们可以取一个比较小的值 $\epsilon$ ，当 $p(x) &lt; \epsilon$ 的时候，我们就认为该 $x$ 是异常数据，否则就是正常数据。 异常检测算法既然高斯分布可以用来检测异常，那太好了，我们得到了一个有力的工具。我们假设每一个特征是服从正态分布的，那么$x_1, x_2, …, x_n$ 服从各自的正态分布，他们有各自的 $\mu_i, \sigma_i^2$ 。那么给定的数据集 { $x^{(1)},x^{(2)},…x^{(n)}$ } 中，数据$x^{(i)}$ 的概率 $ p(x^{(i)}) = p(x_1^{(i)})p(x_2^{(i)}) … p(x_n^{(i)})$ ，这个值如果小于 $\epsilon$ ，则可以认为数据是异常数据。那么现在要做的就是计算出每一个特征的$\mu_i, \sigma_i^2$ 值，而其实就是计算特征的平均数和方差。 那么如何衡量一个异常检测算法的好坏呢？当然还是使用查准率和召回率，而不是单纯的使用它的预测准确度，因为异常检测通常是偏斜问题。 异常检测和监督学习它们之间的一些区别 异常检测(0 异常 1正常) 监督学习 (0 反例 1正例) 极少异常，大部分是正常 正、反的比例相当 异常情况复杂，很难从正常数据中学习到异常的特征 足够的正例数据来进行学习 异常数据可能是我们从未出现过的数据 它们不同的应用场景 异常检测 监督学习 诈骗检测 垃圾邮件分类 制造业（如零件生产商） 天气预报 服务器故障监控 癌症预测 推荐系统评分是一个收集用户兴趣的好方法，我们假设已经观察到下面一些用户的评分行为： 电影名 用户1 用户2 用户3 用户4 何以笙箫默 5 5 0 0 栀子花开 5 0 激战 0 5 叶问2 0 4 战狼 0 0 5 5 评分范围是0-5的整数，留空表示用户没有对其评分，可能是没看过。 基于内容的推荐如果独立的看一个用户i，把电影看成是输入值 $X$，评分看成输出 $Y$，这就是一个线性回归问题，通过学习用户1已经对电影的评分，我们可以预测出用户1对“激战”这部电影的评分。 我们可以把电影分成两类：爱情片、动作片，所以特征数量是2，那么预测函数为： $h(X) = \theta_0 + \theta_1x_1 + \theta_2x_2$ ，其中 $x_1$ 理解为表示是这部电影是爱情片的程度，$\theta_1$ 是用户对于爱情片的喜爱程度，可以记用户i的$\theta$ 值为 $\theta^{(i)}$ 。 所以一个用户评价的越多，他的$\theta$就会越精确，我们就可以更加精确的为其推荐内容。 协同过滤随着电影数量越来越多，我们不好好确定一部电影的“爱情片程度”。协同过滤算法的一个特点是，它可以自动的学习出电影的特征值。怎么学的？通过询问用户，来确定他的 $\theta$ 值，然后再根据用户对电影的评分来求得这部电影的特征值。 而回想发现，$\theta$ 应该才是要被优化的参数啊！！所以协同过滤算法的另一个特点是它在递归下降时同时优化$x$ 和 $\theta$ 。 它的Cost Function： 日中 $r(i, j) = 1$ 表示用户 $j$ 对电影 $i$ 评过分（从表格看来，电影的特征是行，用户是列） 分别对 $x$ 、$\theta$ 进行梯度下降，他们的偏导数为：]]></content>
      <categories>
        <category>研</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Week8-2:Dimensionality Reduction]]></title>
    <url>%2F%2Fp%2F26479.html</url>
    <content type="text"><![CDATA[维数约简假如你负责一项机器学习项目，最开始你需要收集数据集，也许把收集任务下发给各部门后，收集上来的数据特征有上千个，你是全部用呢？还是只用一部分呢？如果只用一部分，应该如何选择呢？ 首先我们肯定是不能全用的，因为特征太多会拖慢训练速度。 我们需要选择其中的一部分特征，但是这些特征又要能保证一些特性，使他能够较好的代表所有特征。 二维转一维假设现在有2个特征 $x_1, x_2$，我们需要将其转成一维数据 $y$ ，我们将所有的点 $(x_1, x_2)$ 投影到一条直线上去，那么最后会得到一条数轴上的一些点集，也就是把二维转成一维了。 三维转二维我们将所有的三维上的点投影到一个平面上，这个平面用两个向量$z_1, z_2$来表示，那么最后我们将得到一个平面上的点集，这样也就将三维转成二维了。 主要成分分析又叫PCA，它寻找一条低维的面，使得所有点到该面的距离的平方和最小。如下图中的这条直线，所有点到该直线的距离的平方和最短。 那么其实不用找一条直线，可以发现，只要方向与直线一样，投影的结果是一样的，所以只需要找到一个向量即可。 处理过程预处理如果给定m个数据 $x^{(1)}, x^{(2)}, .. , x^{(m)}$ ，我们首先对数据进行缩放，像第二周讲的那样，我们先求得每一个特征的平均数$\mu_j = \frac 1 m (x_j^{(1)} + x_j^{(2)} … +x_j^{(m)})$ ，用 $x_j^{(i)} - \mu_j$ 替换 $x_j^{(i)}$ 。 计算协方差矩阵计算出每一个数据的协方差矩阵 计算特征向量再计算协方差矩阵的特征向量，在matlab中，可以使用[U, S, V] = svd(Sigma)（Sigma是协方差矩阵）来计算特征向量U，那么这时我们会得到一个 $n \times n$ 的特征向量。 取前k列作为$U_{reduce}$ 值，用来做降维映射。 降维令向量$z^{(i)} = U_{reduce}^T * x^{(i)}$ 就得到了n维的 $x^{(i)}$ 降成k维后的$z^{(i)}$ k的选取现在你有1000个特征，那么到底降成多少个比较合适呢？也许可以是500个，也有可能100个就够了。 这里有一个方法可以检测降维后，新数据对原始数据特性的保留程度，计算下面公式，你将得到新数据与原始数据之间的差异度。假设计算出来的结果是0.05，那么新数据保留了原始数据95%的特性，一般来说，我们只要保持计算结果小于0.01即可。 所以最好的方式是，尝试不同的k值，找到一个比较小的k值，同时新数据保留原始数据99%~90%的特性均可。 PCA的应用PCA常用于： 压缩数据，节约空间 加速机器学习过程 数据可视化（如取k=2、k=3） 一种不好的用法是，用PCA来避免过拟合，我们前几周知道，过拟合问题可以通过用正规化参数 $\lambda$ 来处理，虽然PCA可以减少特征数量，但其实它保持了原数据的特性，也就是方差，所以效果不尽人意。 同时在进行一次机器学习训练过程中，我们也许根本用不到PCA算法，最好的方式还是先不用PCA降维，得到训练结果后，再使用PCA降维重新训练一次，看哪种方式带来的结果比较合乎人意。]]></content>
      <categories>
        <category>研</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Week8-1: Clustering]]></title>
    <url>%2F%2Fp%2F31473.html</url>
    <content type="text"><![CDATA[序模仿coursera的结构，把第八周分成两部分。前面七周学习内容，我们经常用到训练这个词，就像妈妈教小孩子说话一样，是属于监督学习，数据集是有标记的，这样我们才能根据 $x$ 来预测 $y$。 对于无监督学习，给出的数据集是无标记的，可以看成只给了 $x$，而没有给出 $y$ 。 K-means 聚类算法在监督学习中，我们学到了分类，相对应的，无监督学习中有聚类的概念。 假设现有两个特征的数据集，把它们画在坐标系中如下图，直观上觉得它们应该可以分成2类数据，右上角部分和左下角部分，K-means算法就是干这个事的，它接受两个参数$ (X, K) $ K表示要将数据分成几类，$X$ 表示数据集。 那么K-means算法是怎么工作的呢？ K-means算法工作流程随机选取K个中心点假设需要将数据分成2类，那么它会先随机的选取2个中心点，然后将数据集按照靠哪个点比较近而分成了两类。 计算新的中心点接着计算每一类的新中心点，也就是点 $\frac 1 m (x^{(1)}+.. + x^{(n)})$ 这样迭代的计算新中心点，直到中心点稳定后，K-means算法就完成工作了 代价函数K-means的代价函数是指每一类里的点到中心点的距离之和 K-means 算法优化从数据集中选取中心点算法初始时需要随机选取中心点，如果要是选了一个偏离数据很远的中心点，那就会造成一次分类后，这个中心点周围没有任何数据，会陷入局部最优。 所以我们可以在初始时，从数据中选取K个点作为初始的中心点。 多次运行算法初始中心点对K-means的影响非常大，如果初始点选的不好，很可能陷入局部最优。 而应对这一问题的方法是多次运行K-means算法，优化它的失真值(Cost Function)。 选择K值首先这是一个模棱两可的问题，没有标准答案或方法可以告诉你到底应该分成几类。 人也许一眼能看出来数据可以分成几类，但是总有一些情况是连人眼的分不出来的。 例如衣服的尺码可以分成S,M,L三类，也可以分成XS,S,M,L,XL五类，而到底分成几类，按照你的业务需求定。 还有一种方式是利用肘算法，也就是尝试不同的K值，每次分类完成后都计算它的Cost Function。 左图容易看出来，选K=3就行，形象点也就是手肘的那个点，而对于有图，似乎就看不出肘在哪了。。。 总结看到目前为止，K-means算法印象最深刻。。也许是因为Ng讲的比较详细吧。。]]></content>
      <categories>
        <category>研</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Week7: Support Vector Machines]]></title>
    <url>%2F%2Fp%2F38722.html</url>
    <content type="text"><![CDATA[序因为时间上的限制，本周必须要把Course的课程看完，之后开始步入实战，所以周二就搞定了第七周的内容。支持向量机（Support Vector Machines）这个词很早就听过，今天总算是学到了这个知识点。 线性可分和非线性可分什么是线性可分？例如对于下图所表示的数据集，我们可以用一条直线将红蓝圆点分开，就叫做这个问题是线性可分问题。（下图均出自知乎，作者简之） 但是当数据变多的时候，也许会出现一些错误分类，例如下图： 线性不可分就是说，对于下面这张二维数据，你无法用一条直线将其分开： 最大间距和升维SVM可用于线性可分，也可以用于线性不可分问题。 对于线性可分问题，SVM的策略是选取一个“间距”最大的划分直线，也就是下图的黄色区域越大越好。 因为这样的话，在数据增多的情况下，仍有比较好的分类效果。 而对于线性不可分问题，SVM的策略是将数据升维，在二维上看起来无法用直线划分解决问题，将其提升到三维用一个平面进行划分。 核函数首先，线性模型的预测函数为： $x_1 … x_n$ 是特征值，将其替换为 $ f_1 … f_n$ 就得到了SVM模型的预测函数： $f_i$ 就是 $x_i$ 的核函数，它通常是非线性的，例如 $f_i = xi x{i+1} (i &lt; n)$ ，可以知道，$f_i$ 其实是一个映射函数。 通过$f_i$ 的映射，可以将低维特征转化为高维特征，所以SVM对非线性也有很好拟合能力。 常用的核函数有： 线性核：$κ(x, x_i) = x·x_i$ 多项式核：$κ(x, x_i) = ((x · x_i) + 1)^d$ 高斯核（RBF）：$κ(x, x_i) = exp(- \frac {||x - x_i||^2} {δ^2}) $ 高斯核假设SVM模型为： 值得注意的是，$f$ 的数量不一定和特征 $x$ 数量一样，例如模型中 $f$ 有3个，而特征 $x$ 只有2个。 我们可以在数据集中取三个点 $l^{(1)}, l^{(2)}, l^{(3)}$，函数 $f_i$ 就是点 $x$ 到点 $l^{(i)}$ 的相似度（similarity），它的计算公式为： $f_i =similarity(x, l^{(i)}) = exp(- \frac {||x - l^{(i)}||^2} {2δ^2}) $ 假设 $l^{(1)} = [1,3], δ = 1$ 那么$f_1 = exp(-(x1- 1)^2 - (x2 - 3)^2)$，它表示的是一个三维曲面。 Cost Function逻辑回归的代价函数为： SVM对 $y = 0$ 和 $y = 1$ 两种情况所对应的曲线用直线代替了，如下图： 这样一来，我们得到了 $cost_1$ 和 $cost_2$ 函数，所以SVM的代价函数变成了： Cost Function 正规化逻辑回归中的 $\lambda$ 是正规化参数，SVM中C参数有着相似的作用，先看逻辑回归的代价函数： 常数项只影响最优值大小，而不会影响取得最优值时的参数，所以可以去掉上图的 $m$。 并且 $\lambda $ 作用于左边的误差上，而不是右边的 $\theta$ 上就得到了： 其中 $C$ 的作用类似于$\frac {1} {\lambda}$ 。 C和δ参数C参数作用于代价函数，它的作用类似于$\frac {1} {\lambda}$ ，所以 C大 → 方差大，偏差小 C小 → 方差小，偏差大 δ参数作用于高斯核函数，可以发现δ越大，函数值的的变化率其实越小，所以随着x远离 $l^{(i)}$ ，函数值降低的比较慢，那么方差就越小，偏差就越大，所以 δ大 → 方差小，偏差大 δ小 → 方差大，偏差小 总结我觉得SVM的核心就在于$f$ 核函数，通过升维来解决低维问题着实厉害。]]></content>
      <categories>
        <category>研</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Week 6:Machine Learning System Design]]></title>
    <url>%2F%2Fp%2F62235.html</url>
    <content type="text"><![CDATA[序接Exercise5的内容，他们都是第六周的内容。这一部分主要讲如何设计一个机器学习系统。 垃圾邮件分类器假设你正要去实现一个垃圾邮件分类器，你会怎么做？这是一个二分类问题，我已经学习了线性回归、逻辑回归、神经网络。最快速、准确的方式应该是先确定特征向量，收集数据，再选择一个模型，快速地进行一次测试，画出学习曲线，根据曲线分析现状是高方差还是高偏差，再进行下一步的选择。 那么特征向量怎么选取呢？如果是英文，例如： 12345From: xxxx@xx.comTo: me@xx.comSubject: Buy now!!Deal of the week! Buy now! 可以考虑建立一个特征向量，选择100个词左右，记录词是否出现过，例如 x = [buy, deal, discount, …, now]，对于上面这封邮件，x的值会是 x = [1, 1, 0, …, 1]。 接着就是模型选择，参考Exercise5的模型选择方法即可，那么训练集怎么获取呢？一个简单的方法是把自己的邮箱暴露给个大广告商，这样的话你可能每天都会受到上百封广告邮件，接着就是对模型进行训练了。 错误分析在验证集中你需要手动对错误分类的邮件进行纠错，并且检查哪一类的邮件容易被分类错误，这件事必须手动进行。例如验证集大小为500，其中有100封邮件是垃圾邮件，但是没有被过滤掉，那么你最好检查这100封邮件，手动进行分类。 现在进行深一步考虑特征向量： 我们都知道英语是有词根的，例如 discount, discounted, discounts 它们是不是应该看成一个词，即discount呢？ 对于Deal和deal 是否也看成一个词呢？（即不区分大小写） 对于错误拼写如discount 和 disc0unt，有些聪明人会采取这种方式来躲过垃圾邮件过滤器 一个好的方法便是，把这些方法都试一遍，看哪一个种的准确率比较高，因为它们比较容易在算法中进行调整。 偏斜类、查准率、召回率偏斜类是指在二分类中，其中某一类的占比很大的情况。例如在癌症预测中，实际上患癌症的概率是很小的，例如只有1%，此时如果你的预测函数直接写成 $ y = 0$ 而不经过机器学习算法来计算，你的准确率也许会更高一些，但是我们都知道这个预测函数肯定是错的，显然准确率并不能衡量一个预测函数的好坏。此时就需要引入两个概念：查准率、召回率。 还是以癌症预测为例，$ y = {0, 1}$ 表示未患癌症和患上癌症，预测函数 $h = {0, 1}$ 表示预测没患癌症和预测患上了癌症。那么根据两两组合关系可以得到下面的关系： y = 1 y = 0 h = 1 真阳性(True positive) 假阳性(False positive) h = 0 假阴性(False negative) 真阴性(True negative) 查准率：预测病人患上了癌症的正确率，P = $\frac {真阳性} {真阳性 + 假阳性}$ 召回率：患癌症的病人能查出来的概率，R = $\frac {真阳性} {真阳性 + 假阴性}$ 一个好的预测函数，这两个率都要高，也就是 $\frac {2PR} {P + R}$ 要大 那么对于之前的那个情况，如果预测函数为 $y = 0$ ，那么它的召回率 $R = 0$，这样的预测函数是很差的。]]></content>
      <categories>
        <category>研</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Exercise 5: Regularized Linear Regression and Bias v.s. Variance]]></title>
    <url>%2F%2Fp%2F42787.html</url>
    <content type="text"><![CDATA[序第六周的内容是机器学习的训练方法。 我们已经学习了线性回归、逻辑回归、神经网络，本质上他们都是通过数据的输入，来习得一个预测函数。那么怎么来评估习得函数的好坏呢？如果一个预测函数很好的预测了训练数据，而对于新数据的预测却不准，那这就不是一个好的预测函数，所以判断预测函数好坏的标准就是它对新数据的预测结果准不准，当出现不准的情况时，通常可以采取下面几个方法来提高准确度： 增加训练集数据 用更少的特征进行训练 用更多的特征进行训练 添加多项式 增加/减少 $\lambda$ 那么现在问题抛出来了，是不是从上到下依次尝试呢？答案必须是否定的，因为每一个尝试都会花费大量的时间，有的也许还要花费财力，所以我们需要根据现有情况来判断到底选择哪一种方法。 训练集、测试集假如你正要基于机器学习开发一款应用，现在你手中只有100组数据，在没有办法获得新数据的情况下，你需要想方设法来提高最后习得函数的泛化效率，那么可以考虑这么做：把数据按7:3划分，70%作为训练集，30%作为测试集。这样你就有了两份数据，一份用于训练，一份是新数据（相对训练集而言），那么习得函数在测试集上的准确率可以近似为泛化效率，我们使用误差函数来评判预测函数的好坏。 线性回归的误差函数可以表示为： 二分类误差函数： 模型选择和验证集线性回归需要预先选择模型，如果特征项（项的幂）太多，会导致过拟合，太少则会导致欠拟合。 使用训练集、测试集，可以取得一个比较好的”$\theta$”，而模型选择，则是需要选择最高次幂的大小，所以引入了验证集来判断。 假如现在要在上面的1~10个模型中选取一个作为你的线性回归模型，你需要针对每一个模型，计算出他们的最低误差$J_{test}(\theta)^{(i)}$ ，再选取误差最小的那个模型作为你的预测模型。 偏差和方差记 $J{train}(Θ)$ 为训练集的误差，$J{cv}(Θ)$ 为验证集的误差 。 $J{train}(Θ)$ 很大，$J{cv}(Θ)$ 也很大，则说明预测函数的偏差高，这表示欠拟合 $J{train}(Θ)$ 很小，$J{cv}(Θ)$ 也很大，则说明预测函数的方差高，这表示过拟合 通过引入$\lambda$ 进行正规化，可以很好的解决方差/偏差过高的问题 学习曲线通过绘制误差函数与训练集大小的函数图像，可以观察到高偏差和高方差的情况。 下图表示的是高偏差，可以看到训练误差和测试误差高于了期望值，而且两者几乎相等 下图表示的是高方差，可以看到训练误差比较小和测试误差很大 回到第一个问题我们说到了，当发现预测函数效果不太好时，可以采取以下几种方式提高效果： 增加训练集数据 用更少的特征进行训练 用更多的特征进行训练 添加多项式 增加/减少 $\lambda$ 那么这些方法，分别对应什么样的情况呢？可以根据欠拟合和过拟合分类，高偏差就是欠拟合，高方差就是过拟合，结论如下： 过拟合，高方差 ：增加训练集数据 过拟合，高方差 ：用更少的特征进行训练 欠拟合，高偏差 ：用更多的特征进行训练 欠拟合，高偏差 ：添加多项式 欠拟合，高偏差 ：减少 $\lambda$ 的值 过拟合，高方差 ：增加 $\lambda$ 的值 如果是神经网络，隐藏层越多则可能过拟合，越少则可能欠拟合，判断逻辑与上述一样。 总结以后尽量少贴代码而采取文字总结的方式，因为我觉得贴代码起不到总结的效果。 以后也尽量按照coursera给出的课程大纲来进行分点总结，起到翻译的效果吧。]]></content>
      <categories>
        <category>研</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Exercise 4: Neural Networks Learning]]></title>
    <url>%2F%2Fp%2F8039.html</url>
    <content type="text"><![CDATA[序一天两发，是因为这两次的练习内容都是神经网络，比较接近。 回顾第四周的作业，最后使用神经网络进行多分类预测的时候，Ng给出了训练好的Θ，本周的主要内容就是学习如何训练一个神经网络，最终得出Θ。 正向传播：Cost Function参考公式： K表示分类的数量，$ y^{(i)}$ 表示第 $i$ 个数据集的预测值，这个预测值的可能取值如下： 那么 $y_k^{(i)} = {0,1}$ ，正向传播的意思就是从左到右一层一层的计算，它的Cost Function就是每一层之间的逻辑回归Cost累加。 反向传播：误差计算和梯度为求得 $ min_Θ J(Θ)$，使用梯度下降算法，则需要计算下面两个值： $J(Θ)$ $ \frac {∂} {∂ Θ_{i, j}^{(l)}} J(Θ)$ 对于$J(Θ)$ ，用正向传播可以求得，而$ \frac {∂} {∂ Θ_{i, j}^{(l)}} J(Θ)$ 可以使用反向传播来求得，反向传播的计算流程如下。 $δ_j^{(l)}$ 表示 $Layer_l$ 的第 $j$ 个单元的误差，所以 $δ_j^{(4)}$ = $a_j^{(4)} - y_i$ 而 $δ_j^{(3)}$ 、$δ_j^{(2)}$ 的推导过程比较复杂，这里直接给出计算公式。 并且 $ \frac {∂} {∂ Θ_{i, j}^{(l)}} J(Θ) = a_j^{(l)}δ_i^{(l+1)} $ ，这个证明过程也很繁琐。 梯度检测在计算 $ \frac {∂} {∂ Θ_{i, j}^{(l)}} J(Θ) $ 时，代码上可能会出现一些bug，所以为了检测代码是否写对了，在测试时可以对其计算的梯度进行检测，原理是： 我们取 $\epsilon$ 是一个很小的值，就能近似的计算偏导数，检查两者的差值就能得到我们的偏导数算法是否写对了。 作业1：实现正向传播由于分类数量是K，所以对于每一组训练数据，都需要计算Cost值 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061function [J grad] = nnCostFunction(nn_params, ... input_layer_size, ... hidden_layer_size, ... num_labels, ... X, y, lambda)%NNCOSTFUNCTION Implements the neural network cost function for a two layer%neural network which performs classification% [J grad] = NNCOSTFUNCTON(nn_params, hidden_layer_size, num_labels, ...% X, y, lambda) computes the cost and gradient of the neural network. The% parameters for the neural network are "unrolled" into the vector% nn_params and need to be converted back into the weight matrices. % % The returned parameter grad should be a "unrolled" vector of the% partial derivatives of the neural network.%% Reshape nn_params back into the parameters Theta1 and Theta2, the weight matrices% for our 2 layer neural networkTheta1 = reshape(nn_params(1:hidden_layer_size * (input_layer_size + 1)), ... hidden_layer_size, (input_layer_size + 1));Theta2 = reshape(nn_params((1 + (hidden_layer_size * (input_layer_size + 1))):end), ... num_labels, (hidden_layer_size + 1));% Setup some useful variablesm = size(X, 1); % You need to return the following variables correctly J = 0;Theta1_grad = zeros(size(Theta1));Theta2_grad = zeros(size(Theta2));% ====================== YOUR CODE HERE ======================X = [ones(m,1) X];a1 = X;a2 = sigmoid(X * Theta1');a2 = [ones(size(a2, 1), 1) a2];a3 = sigmoid(a2 * Theta2');for i = 1:m yi = zeros(num_labels, 1); yi(y(i),1) = 1; a3i = a3(i,:)'; J = J + sum(-yi .* log(a3i) - (1 - yi) .* log(1 - a3i));endJ = 1/m * J;rTheta1 = Theta1(:,2:size(Theta1,2));rTheta2 = Theta2(:,2:size(Theta2,2));J = J + lambda/(2*m) * (sum(sum(rTheta1 .^ 2)) + sum(sum(rTheta2 .^ 2)));% -------------------------------------------------------------% =========================================================================% Unroll gradientsgrad = [Theta1_grad(:) ; Theta2_grad(:)];end 作业2：sigmoid的导数其实就是对sigmoid函数求导 作业3：反向传播其实和正向传播是写在同一个地方的，因为本质上实在计算偏导数。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071function [J grad] = nnCostFunction(nn_params, ... input_layer_size, ... hidden_layer_size, ... num_labels, ... X, y, lambda)%NNCOSTFUNCTION Implements the neural network cost function for a two layer%neural network which performs classification% [J grad] = NNCOSTFUNCTON(nn_params, hidden_layer_size, num_labels, ...% X, y, lambda) computes the cost and gradient of the neural network. The% parameters for the neural network are "unrolled" into the vector% nn_params and need to be converted back into the weight matrices. % % The returned parameter grad should be a "unrolled" vector of the% partial derivatives of the neural network.%% Reshape nn_params back into the parameters Theta1 and Theta2, the weight matrices% for our 2 layer neural networkTheta1 = reshape(nn_params(1:hidden_layer_size * (input_layer_size + 1)), ... hidden_layer_size, (input_layer_size + 1));Theta2 = reshape(nn_params((1 + (hidden_layer_size * (input_layer_size + 1))):end), ... num_labels, (hidden_layer_size + 1));% Setup some useful variablesm = size(X, 1); % You need to return the following variables correctly J = 0;Theta1_grad = zeros(size(Theta1));Theta2_grad = zeros(size(Theta2));% ====================== YOUR CODE HERE ======================X = [ones(m,1) X];a1 = X;a2 = sigmoid(X * Theta1');a2 = [ones(size(a2, 1), 1) a2];a3 = sigmoid(a2 * Theta2');bdelta_2 = 0;bdelta_1 = 0;for i = 1:m yi = zeros(num_labels, 1); yi(y(i),1) = 1; a3i = a3(i,:)'; a2i = a2(i,:)'; a1i = a1(i,:)'; delta_3 = (a3i - yi); delta_2 = Theta2' * delta_3; delta_2 = delta_2(2:size(delta_2,1)); delta_2 = delta_2 .* sigmoidGradient(Theta1 * a1i); bdelta_2 = bdelta_2 + delta_3*(a2i)'; bdelta_1 = bdelta_1 + delta_2*(a1i)';endTheta1_grad = 1/m * bdelta_1 + [zeros(size(Theta1, 1),1) lambda/m*rTheta1];Theta2_grad = 1/m * bdelta_2 + [zeros(size(Theta2, 1),1) lambda/m*rTheta2];% -------------------------------------------------------------% =========================================================================% Unroll gradientsgrad = [Theta1_grad(:) ; Theta2_grad(:)];end 总结感觉机器学习理解起来可能不难，但是写代码上却需要非常小心，尤其是对偏置单元的处理，做矩阵乘法时要检查size是否匹配，这些都很重要。]]></content>
      <categories>
        <category>研</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Exercise 3:Multi-class Classification and Neural Networks]]></title>
    <url>%2F%2Fp%2F18619.html</url>
    <content type="text"><![CDATA[序其实上周就做完啦，只是一直没写而已，对于这个Ng的ML课程，我希望能一直记录下去。 直到第三周，已经学习了线性回归和逻辑回归，它们可以用来实现二分类器，而且对于一些非线性分类也比较有效，例如： 但是对于非线性分类，他的Cost Function通常需要引入高阶方程： 所以选取一个预测模型非常重要，但选取一个合适的预测模型是非常困难的，因为通常会出现拟合问题，那么就诞生了”神经网络“，它其实早在上世纪40年代就有了，在沉寂了一段时间后的今天火了起来，原因是大数据和云计算的发展。 神经网络下图是一个最简单的神经网络，它有一个输入层和一个输出层： 如果取$ h_θ(x) = \frac {1} {1 + e^{-x}} $ ，这就是一个逻辑回归呀！其中 $\theta$ 可以看做是输入层（第一层）的参数，如果在输入、输出层之间再加一层： 这是不是就是我们经常在网上看到的图呢，哈哈。他的计算过程是这样的，可以看到 $a_1^{(2)}$ 是由 $x_1, x_2, x_3$ 共同计算出来的，也就是： $Θ^{(1)}$ 是第一层的参数，它是一个 $3 * 4 $ 的矩阵。 那么这么做有啥好处呢？我们可以发现，输入的变量只有$ x1, x2, x3$ ，通过从左到右的一层一层的计算，就能得到最后的结果。那么这期间起到作用的另外一个变量就是$Θ^{(1)}$，而它是可以通过训练这个网络（逻辑回归中的梯度下降可以理解为在训练数据集来得到一个合适的θ）来获得，这就不需要你事先选择一个精确的Cost Function模型了，你只需要考在输入、输出层之间加多少个隐藏层以及每一层的隐藏单元数量就行。 神经网络与多分类器线性回归、逻辑回归可以实现二分类器，我们可以实现多个二分类器来作为一个多分类器，而神经网络就可以干这个事儿。举个例子，一个四分类器，他的$ y^{(i)}$ 取值可能为 它可以通过神经网络来计算，计算过程如下： 那么计算出来的结果（预测值）为： 作业1：Cost Fcuntion这个作业先要求我们用二分类器来实现多分类器，那么每一个二分类器的Cost Function就是： 12345678910111213141516171819202122232425function [J, grad] = lrCostFunction(theta, X, y, lambda)%LRCOSTFUNCTION Compute cost and gradient for logistic regression with %regularization% J = LRCOSTFUNCTION(theta, X, y, lambda) computes the cost of using% theta as the parameter for regularized logistic regression and the% gradient of the cost w.r.t. to the parameters. % Initialize some useful valuesm = length(y); % number of training examples% You need to return the following variables correctly J = 0;grad = zeros(size(theta));J = 1/m * sum(-y .* log(sigmoid(X * theta)) - (1 - y) .* log(1 - sigmoid(X * theta)));J = J + lambda/(2*m) * sum(theta(2:size(theta)) .^ 2);grad = 1/m * (X' * (sigmoid(X * theta) - y)) + lambda/m * theta;grad(1) = grad(1) - lambda/m * theta(1);% =============================================================grad = grad(:);end 作业2：one vs All对于每一个分类器，它需要只识别自己负责的部分，所以用(y==k)来训练他的θ值 12345678910111213141516171819202122232425262728293031function [all_theta] = oneVsAll(X, y, num_labels, lambda)%ONEVSALL trains multiple logistic regression classifiers and returns all%the classifiers in a matrix all_theta, where the i-th row of all_theta %corresponds to the classifier for label i% [all_theta] = ONEVSALL(X, y, num_labels, lambda) trains num_labels% logistic regression classifiers and returns each of these classifiers% in a matrix all_theta, where the i-th row of all_theta corresponds % to the classifier for label i% Some useful variablesm = size(X, 1);n = size(X, 2);% You need to return the following variables correctly all_theta = zeros(num_labels, n + 1);% Add ones to the X data matrixX = [ones(m, 1) X];% ====================== YOUR CODE HERE ======================for k = 1:num_labels initial_theta = all_theta(k:k,1:n+1)'; options = optimset('GradObj', 'on', 'MaxIter', 50); [thetak] = fmincg (@(t)(lrCostFunction(t, X, (y == k), lambda)), ... initial_theta, options); all_theta(k:k,1:n+1) = thetak';end% =========================================================================end 作业3：使用多分类器来预测使用作业2训练完成的θ来预测，只需要做矩阵乘即可，这里需要注意的是，每一组数据对应的预测值是一个矩阵。例如一组数据的预测值是[0,0,0,0,1,0,0,0,0,0]，那么这组数据很有可能表示的是数字5，所以我们需要取预测矩阵中最大值的索引值。 1234567891011121314151617181920212223242526function p = predictOneVsAll(all_theta, X)%PREDICT Predict the label for a trained one-vs-all classifier. The labels %are in the range 1..K, where K = size(all_theta, 1). % p = PREDICTONEVSALL(all_theta, X) will return a vector of predictions% for each example in the matrix X. Note that X contains the examples in% rows. all_theta is a matrix where the i-th row is a trained logistic% regression theta vector for the i-th class. You should set p to a vector% of values from 1..K (e.g., p = [1; 3; 1; 2] predicts classes 1, 3, 1, 2% for 4 examples) m = size(X, 1);num_labels = size(all_theta, 1);% You need to return the following variables correctly p = zeros(size(X, 1), 1);% Add ones to the X data matrixX = [ones(m, 1) X];% ====================== YOUR CODE HERE ====================== p = X * all_theta';[v, p] = max(p, [], 2);% =========================================================================end 作业4：使用神经网络进行多分类这里比较花俏的是，Ng提供了已经训练好的Θ，我们只需要实现正向传播即可，也就是从左往右一层一层计算过去。 12345678910111213141516171819202122232425function p = predict(Theta1, Theta2, X)%PREDICT Predict the label of an input given a trained neural network% p = PREDICT(Theta1, Theta2, X) outputs the predicted label of X given the% trained weights of a neural network (Theta1, Theta2)% Useful valuesm = size(X, 1);num_labels = size(Theta2, 1);% You need to return the following variables correctly p = zeros(size(X, 1), 1);% ====================== YOUR CODE HERE ======================X = [ones(size(X, 1), 1) X];a2 = sigmoid(X * Theta1');a2 = [ones(size(a2, 1), 1) a2];a3 = sigmoid(a2 * Theta2');[v, p] = max(a3, [], 2);% =========================================================================end 总结第一次接触神经网络，其实也没有想象中的这么神秘，但是不得不感叹前辈们的智慧，这么几行代码就能进行图像识别了。]]></content>
      <categories>
        <category>研</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[初学者如何查阅自然语言处理（NLP）领域学术资料]]></title>
    <url>%2F%2Fp%2F49382.html</url>
    <content type="text"><![CDATA[原文：初学者如何查阅自然语言处理（NLP）领域学术资料 昨天实验室一位刚进组的同学发邮件来问我如何查找学术论文，这让我想起自己刚读研究生时茫然四顾的情形：看着学长们高谈阔论领域动态，却不知如何入门。经过研究生几年的耳濡目染，现在终于能自信地知道去哪儿了解最新科研动态了。我想这可能是初学者们共通的困惑，与其只告诉一个人知道，不如将这些Folk Knowledge写下来，来减少更多人的麻烦吧。当然，这个总结不过是一家之谈，只盼有人能从中获得一点点益处，受个人认知所限，难免挂一漏万，还望大家海涵指正。 国际学术组织、学术会议与学术论文 自然语言处理（natural language processing，NLP）在很大程度上与计算语言学（computational linguistics，CL）重合。与其他计算机学科类似，NLP/CL有一个属于自己的最权威的国际专业学会，叫做The Association for Computational Linguistics（ACL），这个协会主办了NLP/CL领域最权威的国际会议，即ACL年会，ACL学会还会在北美和欧洲召开分年会，分别称为NAACL和EACL。除此之外，ACL学会下设多个特殊兴趣小组（special interest groups，SIGs），聚集了NLP/CL不同子领域的学者，性质类似一个大学校园的兴趣社团。其中比较有名的诸如SIGDAT（Linguistic data and corpus-based approaches to NLP）、SIGNLL（Natural Language Learning）等。这些SIGs也会召开一些国际学术会议，其中比较有名的就是SIGDAT组织的EMNLP（Conference on Empirical Methods on Natural Language Processing）和SIGNLL组织的CoNLL（Conference on Natural Language Learning）。此外还有一个International Committee on Computational Linguistics的老牌NLP/CL学术组织，它每两年组织一个称为International Conferenceon Computational Linguistics (COLING)的国际会议，也是NLP/CL的重要学术会议。NLP/CL的主要学术论文就分布在这些会议上。 作为NLP/CL领域的学者最大的幸福在于，ACL学会网站建立了称作ACL Anthology的页面，支持该领域绝大部分国际学术会议论文的免费下载，甚至包含了其他组织主办的学术会议，例如COLING、IJCNLP等，并支持基于Google的全文检索功能，可谓一站在手，NLP论文我有。由于这个论文集合非常庞大，并且可以开放获取，很多学者也基于它开展研究，提供了更丰富的检索支持，具体入口可以参考ACL Anthology页面上方搜索框右侧的不同检索按钮。 与大部分计算机学科类似，由于技术发展迅速，NLP/CL领域更重视发表学术会议论文，原因是发表周期短，并可以通过会议进行交流。当然NLP/CL也有自己的旗舰学术期刊，发表过很多经典学术论文，那就是Computational Linguistics。该期刊每期只有几篇文章，平均质量高于会议论文，时间允许的话值得及时追踪。此外，ACL学会为了提高学术影响力，也刚刚创办了Transactions of ACL（TACL），值得关注。值得一提的是这两份期刊也都是开放获取的。此外也有一些与NLP/CL有关的期刊，如ACM Transactions on Speech and Language Processing，ACM Transactions on Asian Language Information Processing，Journal of Quantitative Linguistics等等。 根据Google Scholar Metrics 2013年对NLP/CL学术期刊和会议的评价，ACL、EMNLP、NAACL、COLING、LREC、Computational Linguistics位于前5位，基本反映了本领域学者的关注程度。 NLP/CL作为交叉学科，其相关领域也值得关注。主要包括以下几个方面：（1）信息检索和数据挖掘领域。相关学术会议主要由美国计算机学会（ACM）主办，包括SIGIR、WWW、WSDM等；（2）人工智能领域。相关学术会议主要包括AAAI和IJCAI等，相关学术期刊主要包括Artificial Intelligence和Journal of AI Research；（3）机器学习领域，相关学术会议主要包括ICML，NIPS，AISTATS，UAI等，相关学术期刊主要包括Journal of Machine Learning Research（JMLR）和Machine Learning（ML）等。例如最近兴起的knowledge graph研究论文，就有相当一部分发表在人工智能和信息检索领域的会议和期刊上。实际上国内计算机学会（CCF）制定了“中国计算机学会推荐国际学术会议和期刊目录”，通过这个列表，可以迅速了解每个领域的主要期刊与学术会议。 最后，值得一提的是，美国Hal Daumé III维护了一个natural language processing的博客，经常评论最新学术动态，值得关注。我经常看他关于ACL、NAACL等学术会议的参会感想和对论文的点评，很有启发。另外，ACL学会维护了一个Wiki页面，包含了大量NLP/CL的相关信息，如著名研究机构、历届会议录用率，等等，都是居家必备之良品，值得深挖。 国内学术组织、学术会议与学术论文 与国际上相似，国内也有一个与NLP/CL相关的学会，叫做中国中文信息学会。通过学会的理事名单基本可以了解国内从事NLP/CL的主要单位和学者。学会每年组织很多学术会议，例如全国计算语言学学术会议（CCL）、全国青年计算语言学研讨会（YCCL）、全国信息检索学术会议（CCIR）、全国机器翻译研讨会（CWMT），等等，是国内NLP/CL学者进行学术交流的重要平台。尤其值得一提的是，全国青年计算语言学研讨会是专门面向国内NLP/CL研究生的学术会议，从组织到审稿都由该领域研究生担任，非常有特色，也是NLP/CL同学们学术交流、快速成长的好去处。值得一提的是，2010年在北京召开的COLING以及2015年即将在北京召开的ACL，学会都是主要承办者，这也一定程度上反映了学会在国内NLP/CL领域的重要地位。此外，计算机学会中文信息技术专委会组织的自然语言处理与中文计算会议（NLP&amp;CC）也是最近崛起的重要学术会议。中文信息学会主编了一份历史悠久的《中文信息学报》，是国内该领域的重要学术期刊，发表过很多篇重量级论文。此外，国内著名的《计算机学报》、《软件学报》等期刊上也经常有NLP/CL论文发表，值得关注。 过去几年，在水木社区BBS上开设的AI、NLP版面曾经是国内NLP/CL领域在线交流讨论的重要平台。这几年随着社会媒体的发展，越来越多学者转战新浪微博，有浓厚的交流氛围。如何找到这些学者呢，一个简单的方法就是在新浪微博搜索的“找人”功能中检索“自然语言处理”、 “计算语言学”、“信息检索”、“机器学习”等字样，马上就能跟过去只在论文中看到名字的老师同学们近距离交流了。还有一种办法，清华大学梁斌开发的“微博寻人”系统可以检索每个领域的有影响力人士，因此也可以用来寻找NLP/CL领域的重要学者。值得一提的是，很多在国外任教的老师和求学的同学也活跃在新浪微博上，例如王威廉、李沐等，经常爆料业内新闻，值得关注。还有，国内NLP/CL的著名博客是52nlp，影响力比较大。总之，学术研究既需要苦练内功，也需要与人交流。所谓言者无意、听者有心，也许其他人的一句话就能点醒你苦思良久的问题。无疑，博客微博等提供了很好的交流平台，当然也注意不要沉迷哦。 如何快速了解某个领域研究进展 最后简单说一下快速了解某领域研究进展的经验。你会发现，搜索引擎是查阅文献的重要工具，尤其是谷歌提供的Google Scholar，由于其庞大的索引量，将是我们披荆斩棘的利器。 当需要了解某个领域，如果能找到一篇该领域的最新研究综述，就省劲多了。最方便的方法还是在Google Scholar中搜索“领域名称 + survey / review / tutorial / 综述”来查找。也有一些出版社专门出版各领域的综述文章，例如NOW Publisher出版的Foundations and Trends系列，Morgan &amp; Claypool Publisher出版的Synthesis Lectures on Human Language Technologies系列等。它们发表了很多热门方向的综述，如文档摘要、情感分析和意见挖掘、学习排序、语言模型等。 如果方向太新还没有相关综述，一般还可以查找该方向发表的最新论文，阅读它们的“相关工作”章节，顺着列出的参考文献，就基本能够了解相关研究脉络了。当然，还有很多其他办法，例如去videolectures.net上看著名学者在各大学术会议或暑期学校上做的tutorial报告，去直接咨询这个领域的研究者，等等。]]></content>
      <categories>
        <category>转载</category>
      </categories>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode买股票系列2]]></title>
    <url>%2F%2Fp%2F31115.html</url>
    <content type="text"><![CDATA[上一个系列是对交易次数进行限制，本系列是无限次交易但是有一些其他限制。 问题1：卖出后休息一天原题：Best Time to Buy and Sell Stock with Cooldown 题意：你可以进行任意次交易，但完成一次交易后，接下来的一天不能进行买入/卖出操作。 我们先回顾一下上一期 买股票系列 的问题2，它和这个问题就差了一个条件，当时我们用了贪心算法，策略是低买高卖，且经过思考后得到的下述的状态叠加公式 $ profit += max(0, prices[i + 1] - prices[i])$ 直观上理解为，只要今天的股票比昨天高，我就可以挣一个净增值。例如对于序列 $[1,2,3,4]​$ 第2天比第1天多1，我们就在第1天买入，第2天卖出，这样能挣1元 第3天比第2天多1，我们就在第2天买入，第3天卖出，这样能挣1元 …… 细看发现，其实这个状态是连续叠加的，所以如果一次交易后必须休息一天，那么这个状态就不连续了。 现在再看上述的过程，发现 profit += 这一句代码其实隐含着一个意思，就是局部最优，当前状态下的最优值是从上一个状态的最优值加上本次状态的最佳选择得到的，到这一步其实就已经得到本题的结果了，也就是公式 今天的最优值 = 上一个状态的最优值 + 今天的最佳选择 ，上述题目中的上一个状态的最优值 其实就是昨天的最优值，而本题确是上一次交易的最优值 123456789101112131415class Solution(object): def maxProfit(self, prices): """ :type prices: List[int] :rtype: int """ if len(prices) &lt; 2: return 0 sell, buy, prev_sell, prev_buy = 0, -2147483648, 0, 0 for price in prices: prev_buy = buy buy = max(prev_sell - price, prev_buy) prev_sell = sell sell = max(prev_buy + price, prev_sell) return sell 把买和卖拆开看，buy表示今天买入后最大价值，sell表示今天卖出后最大价值，prev_buy表示今天前买入的最大价值，prev_sell表示今天前卖出的最大价值。 问题2：交易收费原题：Best Time to Buy and Sell Stock with Transaction Fee 题意：每次交易收费 还是今天的最优值 = 上一个状态的最优值 + 今天的最佳选择 ，把买和卖拆开看，但是卖的时候要收手续费。 12345678910111213class Solution(object): def maxProfit(self, prices, fee): """ :type prices: List[int] :type fee: int :rtype: int """ buy, sell, prev_sell = -2147483648, 0, 0 for price in prices: prev_sell = sell sell = max(prev_sell, buy + price - fee) buy = max(buy, prev_sell - price) return sell]]></content>
      <categories>
        <category>编程题</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode买股票系列]]></title>
    <url>%2F%2Fp%2F52113.html</url>
    <content type="text"><![CDATA[问题1：一次交易原题：Best Time to Buy and Sell Stock 题意：给定一个数组，第 $i$ 个元素表示第 $i$ 天股票的价值，你只能进行一次交易（买入+卖出= 一次交易），请告诉我你获得的最大价值是多少？ Example 1: 1234Input: [7, 1, 5, 3, 6, 4]Output: 51买进，6卖出，获利5 Example 2: 1234Input: [7, 6, 4, 3, 1]Output: 0什么时候买都会亏，所以获利0 作为该系列的开门见山题，难度口算，下面说2个可能想到的错误思路： 也许你会觉得，找出最大值和最小值，两者差值就是答案 也许你会觉得，找出最大值和最小值，并且最小值索引小于最大值的索引就行了 对于第一个思路，Example 2: 已经告诉你是错的。 对于第二个思路，你怎么确保最小值索引是小于最大值索引的，万一不是呢，那是不是要进行搜索了呢？那搜索策略又是什么呢？顿时觉得前途一片昏暗 DP从下至上的计算，设 总天数为n $a[i]$ 是第 $i$ 天的股票价值 $ i = 1,2,…,n$ $d[i]$ 是第 $i$ 天买入股票所能达到的最大利益 $ i = 1,2,…,n$ 那么 $d[i] = max(a[i], a[i+1], .. , a[n]) - a[i] ， i = 1,2,…,n$ 那么最大价值就是 $max ( d[1], d[2], .., d[n])， i = 1,2,…,n$ 相信聪明的你已经知道代码怎么写了。 123456789101112class Solution(object): def maxProfit(self, prices): """ :type prices: List[int] :rtype: int """ max_after = 0 max_profit = 0 for i in range(len(prices) - 1, -1 , -1): max_profit = max(max_profit, max(0, max_after - prices[i])) max_after = max(max_after, prices[i]) return max_profit 贪心从上至下计算。假设你现在就是一个股民，用变量money表示你现在有多少钱，所以初始你是0元， 买入一个a元的股票，$money = -a$（欠钱） 今天股票价值是b，如果今天卖掉，则 $money = money + b$ 要达到两个最大即可，一是买入欠钱最少，所以-a越大越好，二是money越大越好 123456789101112class Solution(object): def maxProfit(self, prices): """ :type prices: List[int] :rtype: int """ debt = -2147483648 money = 0 for price in prices: money = max(money, price + debt) debt = max(debt, -price) return money 问题2：任意次交易原题：Best Time to Buy and Sell Stock II 题意：在问题1基础上，交易次数变为任意多次 一瞬间没啥思路，感觉问题很复杂？其实，如果你炒股你就知道，其实很简单，一句话：低买高卖 啥意思？借一张leecode某个答主的图，股票的波动一般像下图一样 低买高卖 是人类的直觉，即在valley（低谷）买入，在peak（顶峰）卖出，这样的收益会比较大。 我们可以分2类情况来讨论，一只股票其实就是下面两种情况的循环往复 股票一直跌，跌倒谷底了，马上要开始涨了这时候赶紧买啊！因为这个时候买肯定赚，对应的是低谷 如果在valley右侧买，肯定没有valley的收益大 如果在valley左侧买，也没有valley的收益大 如果在顶峰之后再买，那就白白错过了这次稳赚的机会，最后总收益肯定不是最大的 所以结论是必须在低谷买 股票一直涨，涨到顶峰了，马上要开始降了这时候赶紧抛啊！！因为这一点的收益是最大的，往后的收益肯定没现在这个大，对应的是顶峰 如果在peak左边卖，收益没有peak大 如果在peak右边卖，收益也没有peak大 如果在peak右边的低谷之后再卖，又白白错过了稳赚的机会，最后总收益肯定不是最大 所以结论是必须在顶峰卖 两个顶峰，分开买第一张图很好的诠释了 $ A + B &gt;= C$ 这一不等式，一看就懂 实现找一个低谷点买入，在遇到下一个顶峰点的时候立马卖出，这样你就能拿到最大的收益。 不过在代码上要注意最后一天的处理 1234567891011121314151617class Solution(object): def maxProfit(self, prices): """ :type prices: List[int] :rtype: int """ buy = -1; profit = 0 for i in range(len(prices)): if i + 1 &gt;= len(prices): profit += prices[i] - buy if buy != -1 else 0 elif prices[i + 1] &gt; prices[i]: buy = prices[i] if buy == -1 else buy elif prices[i + 1] &lt; prices[i]: profit += prices[i] - buy if buy != -1 else 0 buy = -1 return profit 优化其实仔细看刚刚写的代码你会发现，只要是“上升期”的股票我们都可以买，所以代码可以优化为 12345678910class Solution(object): def maxProfit(self, prices): """ :type prices: List[int] :rtype: int """ max_profit = 0 for i in range(len(prices) - 1): max_profit += max(0, prices[i + 1] - prices[i]) return max_profit 问题3：最多两次交易原题：Best Time to Buy and Sell Stock III 题意：在问题1的基础上，交易次数变为最多两次 这和问题1非常类似，意思是在问题1交易完成后，还能再交易1次。站在问题1的基础上，我们把解法扩展一下。 问题1的的贪心解法的成功运用，表示问题1其实是具有局部最优性质的，意思是这个算法可以用于任何一个局部，当这个局部扩大到全局的时候，就得到的原问题的最优解。那如果把交易次数增加到2次，这种局部最优性质是否还有呢，答案是有的。 我们可以设两对个变量，一对记录第一次交易状态，一对记录第二次交易状态，代码如下： 12345678910111213class Solution(object): def maxProfit(self, prices): """ :type prices: List[int] :rtype: int """ debt1, money1, debt2, money2 = -2147483648, 0, -2147483648, 0 for price in prices: money2 = max(money2, debt2 + price) debt2 = max(debt2 , money1 - price) money1 = max(money1, debt1 + price) debt1 = max(debt1 , -price) return money2 问题4：最多K次交易原题：Best Time to Buy and Sell Stock IV 题意：在问题1的基础上，交易次数变为最多K次，K是程序的输入 注意到K是可变的，所以： K = 1时，退化成问题1 K = 2时，退化成问题3 K &gt;= 总天数/2 时，退化成问题2 对于第3种情况，我们直接调用问题2的解法即可，以免超时 1234567891011121314151617181920212223class Solution(object): def maxProfit(self, k, prices): """ :type k: int :type prices: List[int] :rtype: int """ if k &gt;= len(prices) / 2: return maxProfitAny(prices) debt, money, max_money = [-2147483648 for i in range(k)], [0 for i in range(k)], 0 for price in prices: for t in range(k - 1, - 1, -1): money[t] = max(money[t], debt[t] + price) debt[t] = max(debt[t], (money[t - 1] if t &gt; 0 else 0) - price) max_money = max(max_money, money[t]) return max_money def maxProfitAny(prices): max_profit = 0 for i in range(len(prices) - 1): max_profit += max(0, prices[i + 1] - prices[i]) return max_profit]]></content>
      <categories>
        <category>编程题</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[提高你的记忆力：记忆宫殿法]]></title>
    <url>%2F%2Fp%2F61075.html</url>
    <content type="text"><![CDATA[原文：知乎 - 世界上真的存在记忆宫殿吗？常人能掌握吗？ 世界上确实是存在记忆宫殿的，而且并不是一些人想象中那样玄妙和难以掌握，许多答主分享了自己建立记忆宫殿的经验和心得，就在这里给大家整理一下建立记忆宫殿的简单步骤吧。 建造记忆宫殿的原理我们的大脑擅长记忆图像信息与空间信息，而不擅长于记忆文字类信息（比如单词）。于是，人们试着将自己不擅长记忆的文字信息，转变为图像和空间信息来进行记忆，比如为自己造一座“记忆宫殿（memory palace）”。“记忆宫殿”被证明能有效地提升人们对文字的记忆力（Foer, 2011）。 建造属于自己的记忆宫殿只需要五步第一步：寻找一个合适的宫殿建议选择一个你熟悉的地方作为你的记忆宫殿，比如你的家，或是你平时习惯行走的街道。越是熟悉的场景，我们越是能在脑海中清晰地再现场景中的细节（比如摆设和建筑），也有利于我们接下来将需要记忆的内容与这些细节联系起来。 第二步：规划你在宫殿中行走的路线想象你走过平时上班会路过的街道，你会从哪里开始？你在哪里会拐弯？你选择在哪里结束？试着闭上眼、在脑海中多走几遍，直到我们确定这条路径已经牢牢映在我们的脑海中。要注意，每次走过的路线一定得是一样的。如果你选择把自己的家作为记忆宫殿，那么需要想清楚：一般从哪个房间开始？会经过哪些房间？在哪个房间停下，等等。 第三步：选择一些有特征的物品这一步要求你在宫殿中设置一些特征物。比如说街边的建筑、或是房间里的大型家具（床、柜子等）。建议选一些大的物体作为特征物，因为它们能更好地吸引我们的注意力。 现在，想象我们自己重新沿着第二步的路径行走，并在过程中注意那些特征物：你在街边看到的第一个建筑是什么？它是什么样的？第二个引起你注意的物体是什么？每条街上最好有5~10个不同的特征物。如果你觉得很难同时想象特征物与路径的话，你可以画下平面图，来帮助自己更好地进行空间想象。 第四步：把特征物和和记忆内容建立联系在这一步，把你想要记住的内容与你的记忆宫殿联系在一起。这些联系越是形象和荒诞，越是能加深我们的记忆。举个例子，比如你想记住一句英文，开头第一个单词是“mania（躁狂）”，它的第一个音节读起来很像“妈”，于是你可以在你的记忆宫殿的第一个特征物上印上一个“疯狂妈妈”的形象（比如，你走上街，发现自己看到的第一个建筑上印着一个大笑的、手舞足蹈的妈妈形象）。 接下来，按照顺序，用类似的方法将句子中剩下单词与宫殿中的特征物一一联系起来。这样，你就可以通过在宫殿中行走、浏览特征物，来复述句子了。 第五步：重复地参观你的宫殿这一步主要是巩固我们的记忆。在这一步中，我们从之前开始的地方、遵循先前的路线，重复地参观我们的宫殿。注意我们经过的特征物，当我们经过它们时，那些与特征物有关的记忆就会逐渐浮现。如果你觉得一开始有些困难，你还是可以用平面图、或者大声说出记忆内容的方法，来帮助你进行空间想象。而经过几次练习后，我们的空间想象能力会随之提高。 其他方法除了记忆宫殿之外，还有一些其他帮助记忆的技术可以帮助我们更长久、更准确的记忆。比如： 使用“闪存卡（flash card）”技术闪存卡技术是一项很有效的、帮助人们牢记知识点的方法（Boser, 2017)）。方法很简单，首先，将你需要记住的知识点，用提问和回答的方式，写在一张卡片的正反面（比如正面写“记忆的定义是？”，反面写上答案）；通过这样的方法，你可能会制作出一叠闪存卡。 随后，浏览这些卡片，将你能回答出的部分挑出放在一边，称为“已答卡”；而余下的卡片放在另一边，称为“未答卡”。接下来，反复地抽取和回答“未答卡”上的问题，直到你能将所有卡片上的问题都回答正确。你可以将一些闪存卡带在身边，在通勤时抽卡进行复习。 捏压力球研究发现，捏压力球有助于提升人的记忆能力。在一项实验中，右撇子参与者在开始记信息之前，用右手捏压力球45秒；在背完信息后，再用左手捏压力球45秒（左撇子参与者是先左手后右手）。结果显示，比起没有捏压力球的人，捏了压力球的参与者对信息记得更多、更牢。这可能是因为，用惯用手捏压力球，可以刺激大脑中负责解码、信息的区域，帮助我们更好地感知和分析需要记忆的信息（Szalavitz, 2013）。]]></content>
      <categories>
        <category>转载</category>
      </categories>
      <tags>
        <tag>知乎</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Exercise2: Logistic Regression]]></title>
    <url>%2F%2Fp%2F48722.html</url>
    <content type="text"><![CDATA[序一个月快过去了，总算是学完Coursera上的第三课了，其实我的进度比预期满了一周。这是第二次做Coursera的编程作业，对比大学课程的作业，Coursera的难度应该在中等甚至偏下，对我来说唯一的难度在于适应“新”，有以下几点： 新语言 新工具 新的思考方式 对于一个Software Engineer来说以上都是最基础的技能，即保持学习态度，及时学习新技术、新知识。 第三周的课程内容主要讲分类、逻辑回归，与线性回归相比，也就是把Hypothesis函数用Sigmoid函数来表示，也就是：$设 g(z) = \frac{1} {1 + e ^{-z}}$，$令 z = θ^T x$，$则 h_θ(x) = g(θ^Tx)$ 为什么要 $设 g(z) = \frac{1} {1 + e ^{-z}}$ 呢，因为它的值域为 $(0, 1)$，定义域为(-∞, +∞) 的单调连续增函数，这意味着可以用换元法把所有函数的值域压缩到$(0, 1)$，它的函数图像如下。 这非常适合作为二分类（01分类）函数，我们可以取 $ hθ(x) &gt;= 0.5$ 时 $y = 1$ ，$ hθ(x) &lt; 0.5$ 时 $y = 0$ 。 同时Cost函数和Gradient Descent方程也有些许改变，这种改变是换元法带来的（公式识别失败，用图代替了） 它们对应的矩阵形式是 ① $J(θ) = \frac {1} {m} (- y ^T log(h) - (1 - y)^T log(1 - h))，h = g(Xθ)$ ② $θ = θ - \frac {α} {m} X^T (g(Xθ) - y)$ 注意其中$X、y、θ$ 都是矩阵 作业内容标*的是需要做的，没标的是作业中已经实现 数据可视化 * Sigdmoid函数12345678910111213141516function g = sigmoid(z)%SIGMOID Compute sigmoid function% g = SIGMOID(z) computes the sigmoid of z.% You need to return the following variables correctly g = zeros(size(z));% ====================== YOUR CODE HERE ======================% Instructions: Compute the sigmoid of each value of z (z can be a matrix,% vector or scalar). g = 1 ./ (1 + exp(-z));% =============================================================end 传入的参数是一个矩阵，所以需要算出每一个元素的sigmoid函数值，这里用到的./操作符来对每一个元素进行取倒数操作。 * Cost函数和对应的Gradient Descent方程123456789101112131415161718192021222324252627282930function [J, grad] = costFunction(theta, X, y)%COSTFUNCTION Compute cost and gradient for logistic regression% J = COSTFUNCTION(theta, X, y) computes the cost of using theta as the% parameter for logistic regression and the gradient of the cost% w.r.t. to the parameters.% Initialize some useful valuesm = length(y); % number of training examples% You need to return the following variables correctly J = 0;grad = zeros(size(theta));% ====================== YOUR CODE HERE ======================% Instructions: Compute the cost of a particular choice of theta.% You should set J to the cost.% Compute the partial derivatives and set grad to the partial% derivatives of the cost w.r.t. each parameter in theta%% Note: grad should have the same dimensions as theta%J = 1/m * sum(-y .* log(sigmoid(X * theta)) - (1 - y) .* log(1 - sigmoid(X * theta)));grad = 1/m * (X' * (sigmoid(X * theta) - y));% =============================================================end 在这个函数中需要同时计算代价和梯度，通过公式①、②不难得到上述两行代码 * Hypothesis函数1234567891011121314151617181920212223function p = predict(theta, X)%PREDICT Predict whether the label is 0 or 1 using learned logistic %regression parameters theta% p = PREDICT(theta, X) computes the predictions for X using a % threshold at 0.5 (i.e., if sigmoid(theta'*x) &gt;= 0.5, predict 1)m = size(X, 1); % Number of training examples% You need to return the following variables correctlyp = zeros(m, 1);% ====================== YOUR CODE HERE ======================% Instructions: Complete the following code to make predictions using% your learned logistic regression parameters. % You should set p to a vector of 0's and 1's%s = sigmoid(X * theta);p = floor(s + 0.5);% =========================================================================end 预测函数就比较简单的，由$则 h_θ(x) = g(θ^Tx)$，$g$ 是Sigmoid函数，可以得到上述两行代码，因为当函数值大于0.5时$y = 1$ ，又得到的 $s$ 其实是一个矩阵，所以使用floor(s + 0.5)来进行批量操作。 完成上述三个函数后，运行ex2函数可以得到一条预测线，如图。 * 正规化正规化是为了防止过拟合，核心是保持 $ x$ 值不变， 减小 (惩罚) $θ$的值。 当然还有一种方式防止过拟合的方法是建立一个模型来去除用处不大的特征，保留作用相对较大的特征。 上图表示的预测函数，从左到右依次是欠拟合、拟合 也叫刚好拟合、过拟合，欠拟合一般是因为多项式的最高次太小，过拟合一般是因为多项式的最高次幂太大。 正规化后的代价函数如下，注意尾巴上对 $θ_j$ 的求和是从1开始，也就意味着跳过了 $θ_0$ 值得注意的是梯度方程不需要对 $θ_0$ 进行正规化，所以方程变成了两部分： 12345678910111213141516171819202122232425262728function [J, grad] = costFunctionReg(theta, X, y, lambda)%COSTFUNCTIONREG Compute cost and gradient for logistic regression with regularization% J = COSTFUNCTIONREG(theta, X, y, lambda) computes the cost of using% theta as the parameter for regularized logistic regression and the% gradient of the cost w.r.t. to the parameters. % Initialize some useful valuesm = length(y); % number of training examples% You need to return the following variables correctly J = 0;grad = zeros(size(theta));% ====================== YOUR CODE HERE ======================% Instructions: Compute the cost of a particular choice of theta.% You should set J to the cost.% Compute the partial derivatives and set grad to the partial% derivatives of the cost w.r.t. each parameter in thetaJ = 1/m * sum(-y .* log(sigmoid(X * theta)) - (1 - y) .* log(1 - sigmoid(X * theta)));J = J + lambda/(2*m) * sum(theta(2:size(theta)) .^ 2);grad = 1/m * (X' * (sigmoid(X * theta) - y)) + lambda/m * theta;grad(1) = grad(1) - lambda/m * theta(1);% =============================================================end 根据上述的三张图不难写出上述四行代码 调整λ参数的大小λ = 1 λ = 10 λ = 0.0001 可以看出 $λ$ 越小，对训练样本的拟合程度越好，从而导致了整个曲线看起来比较扭曲，没有平滑感；而 $λ$ 越大，对训练样本的拟合程度越差，曲线越光滑。]]></content>
      <categories>
        <category>研</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Centos7修改docker的Data Space Total大小]]></title>
    <url>%2F%2Fp%2F55543.html</url>
    <content type="text"><![CDATA[记得备份你的容器此处就不多做介绍了 –storage-opts 参数介绍devicemapper文档：https://github.com/moby/moby/tree/master/daemon/graphdriver/devmapperdocker官方文档：https://docs.docker.com/engine/reference/commandline/dockerd/#options-per-storage-driver 修改–storage-opts参数停止docker，修改配置，重新加载配置12sudo systemctl stop dockersudo vi /lib/systemd/system/docker.service 找到ExecStart=/usr/bin/dockerd在这一行后面加上--storage-opt dm.loopdatasize=8G --storage-opt dm.loopmetadatasize=4G --storage-opt dm.basesize=8G 意思是为，设置devicemapper的data为8G，metadata为4G，镜像的大小不能大于8G 记得还要抹去现有的空间，请确保你已经完成了第一步1234sudo rm -rf /var/lib/dockersudo mkdir -p /var/lib/docker/devicemapper/devicemapper/sudo dd if=/dev/zero of=/var/lib/docker/devicemapper/devicemapper/data bs=1M count=0 seek=8192sudo dd if=/dev/zero of=/var/lib/docker/devicemapper/devicemapper/metadata bs=1M count=0 seek=4096 完成后运行123sudo systemctl daemon-reloadsudo systemctl start dockerdocker info 查看是否设置正确，貌似比预设的大了一点点。]]></content>
      <categories>
        <category>软件技术</category>
      </categories>
      <tags>
        <tag>devopps</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mac下使用命令行或脚本删除文件到废纸篓]]></title>
    <url>%2F%2Fp%2F35267.html</url>
    <content type="text"><![CDATA[mac删除文件有两种方式： 使用Finder的移到废纸篓功能 使用rm命令第二种方式删除的文件，不能在废纸篓中找到，也就是所谓的彻底删除但是我们在使用terminal的时候，一般都会使用rm删除文件，那要是删错了不就完了？而且我们的mac上本身又自带了废纸篓，为什么不能将两者结合起来呢？于是我做了一些探究 Mac废纸篓的真实面目1$ ls ~/.Trash 你会发现，~/.Trash目录就是废纸篓但是它只是一个普通的目录，只是Finder将删除的文件移动到了这个目录而已，不相信？接着往下看 ~/.Trash的本质只是文件夹123cd ~echo &quot;666666&quot; &gt;&gt; wantodel.txtmv wantodel.txt ~/.Trash 此时打开你的废纸篓一看，里面有一个wantodel.txt文件，但是你无法将它放回原处，而如果你使用Finder的移到废纸篓功能删除的文件却可以放回原处 Finder是如何实现放回原处的呢？最有可能是将操作记录到了数据库中。 不大可能写在log日志中，因为我grep了整个磁盘都没找到相关的文件。 最好的帮手是Finder既然~/.Trash只是普通文件夹，那我们单纯使用linux命令是无法达到目的的了。我们现在已知Finder可以达到预期目的，如果我们能调用或是告诉Finder的我们要做什么，并且它也愿意做，不就可以达到目的了么？在window中有一个消息的概念，意思是一个应用程序A可以对另一个应用程序B发送消息以操作B来完成某项指定的任务，那Mac种是否也有这种或是类似这种的机制呢？答案是AppleScript，是一种脚本语言，可以用来控制Mac上的应用程序。最后附上使用shell通知Finder程序移动文件废纸篓的样例代码1234567#!/bin/bashfp=/absolute/path/to/fileosascript &lt;&lt; EOFtell application "Finder" posix path of ((delete posix file "$&#123;fp&#125;") as unicode text)end tellEOF]]></content>
      <categories>
        <category>软件技术</category>
      </categories>
      <tags>
        <tag>mac</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git托管现有代码]]></title>
    <url>%2F%2Fp%2F49319.html</url>
    <content type="text"><![CDATA[场景：将现有的文件托管至新的git repo 用到的命令： git clone 复制网络仓库到本地 git add [file] 加入一个文件/目录到版本控制 git commit 提交修改到仓库(本地) git push 推送对仓库的修改到网络仓库 操作步骤如果对一个非空目录执行clone命令是会失败的，因为git不允许这样做。 进入项目目录（非空），运行下面命令： 1234567cd /path/to/dirgit clone &#123;repo_url&#125; tmpmv tmp/.git .rmdir tmpgit add .git commit -m &apos;current files&apos;git push 解释.git是一个隐藏的git用于管理本地代码的目录 git clone {repo_url} tmp 其实是把远程repo加载到tmp目录 将tmp/.git 直接移动到项目的根目录后运行git add . 其实相当于是把项目的文件放到了本地的git repo中 此时运行git commit 就提交了现有项目文件到本地，git push 就把操作推向了服务器 操作完成后，你的另外一个同事就可以使用git clone {repo_url} 来加载你的代码]]></content>
      <categories>
        <category>软件技术</category>
      </categories>
      <tags>
        <tag>无标签</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[打开FTP服务器上的文件夹时发生错误,请检查是否有权限访问该文件夹]]></title>
    <url>%2F%2Fp%2F11354.html</url>
    <content type="text"><![CDATA[环境：Wndows2012 R2， 已完成操作：FTP服务器搭建、防火墙规则允许(或关闭防火墙)、权限已给(Administrator) 客户端：FileZilla（或ie浏览器、windows资源管理器） 场景： 使用windows资源管理器连接ftp报错：“打开FTP服务器上的文件夹时发生错误,请检查是否有权限访问该文件夹” 使用FileZilla链接时超时 最后解决方法： 在FileZilla中将FTP模式改为PORT(主动模式)即可]]></content>
      <categories>
        <category>软件技术</category>
      </categories>
      <tags>
        <tag>无标签</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[负数的除2和右移1位]]></title>
    <url>%2F%2Fp%2F14496.html</url>
    <content type="text"><![CDATA[令人费解的输出且看如下代码，试问输出是什么。 1234int F, G, X = -5;F = X / 2;G = X &gt;&gt; 1;Console.WriteLine(&quot;F = &#123;0&#125;, G= &#123;1&#125;&quot;, F, G); 老师说过“乘2是二进制左移1位”，那除2理所当然应该是右移1位，所以两者结果是一样的。 然而，输出却是F = -2, G = -3 ，在VS2013中，换成在codeblocks中，结果是一样的。 反汇编在VS2013中对上述代码反汇编得到下图 第一句：F = X / 2 1234500DF39F7 mov eax,dword ptr [ebp-58h] ;将X的值移到寄存器eax 00DF39FA mov ecx,2 ;将值2移到ecx 00DF39FF cdq ;将eax高位扩展到edx 00DF3A00 idiv eax,ecx ;做除法运算 00DF3A02 mov dword ptr [ebp-50h],eax ;移动到内存 idiv指令是带符号的二进制除法 第二句：G = X &gt;&gt; 1 逻辑右移，最低位被舍弃 结论除法运算，结果都向0取整；位运算结果向下取整]]></content>
      <categories>
        <category>软件技术</category>
      </categories>
      <tags>
        <tag>汇编</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[汇编实现回文判断]]></title>
    <url>%2F%2Fp%2F9879.html</url>
    <content type="text"><![CDATA[不多说，直接上代码 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061DATA SEGMENT Prompt DB &apos;Please enter a string:$&apos; YesStr DB 13, 10, &apos;This string is palindrome$&apos; NoStr DB 13, 10, &apos;This string is not palindrome$&apos; Input DB 128, ?, 128 DUP(0) DATA ENDS STACK SEGMENT DB 128 DUP(?) STACK ENDS CODE SEGMENT ASSUME CS:CODE, DS:DATA, SS:STACK MAIN: MOV AX, DATA MOV DS, AX MOV DX, offset Prompt MOV AH, 09H INT 21H ; MOV DX, OFFSET Input MOV AH, 0AH INT 21H ; MOV DI, OFFSET Input MOV SI, OFFSET Input XOR CX, CX MOV CL, [DI+1] ADD DI, 2 ADD SI, 2 DEC CX ADD DI, CX ;force DI point to the last word of Input AGAIN: CMP SI, DI JA TESSKIP MOV AH, byte ptr [SI] MOV AL, byte ptr [DI] CMP AH, AL JNE NOSKIP INC SI DEC DI JMP AGAIN TESSKIP: MOV DX, OFFSET YesStr MOV AH, 09H INT 21H JMP ENDSKPI ; NOSKIP: MOV DX, OFFSET NoStr MOV AH, 09H INT 21H ; ENDSKPI: MOV AH, 4CH INT 21H CODE ENDS END MAIN]]></content>
      <categories>
        <category>软件技术</category>
      </categories>
      <tags>
        <tag>汇编</tag>
      </tags>
  </entry>
</search>
